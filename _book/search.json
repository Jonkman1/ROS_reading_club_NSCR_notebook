[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ROS-Notebook",
    "section": "",
    "text": "The Reading Club\n\n\nThis is a Notebook around Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari. The book is available at https://avehtari.github.io/ROS-Examples/. The notesbook is available at https://github.com/Jonkman1/Regression_reading_club_NSCR.\nThis Notebook is part of the NSC-R Workshops. The NSC-R Workshops is a series of one-hour online instructional sessions to support participants in developing their data science skills in R, and to promote open science principles. The NSC-R workshop meetings are organized by a team mostly affiliated with the Netherlands Institute for the Study of Crime and Law Enforcement (NSCR), but they are open to everyone, regardless of affiliation or skill level.\nThere are workshops on specific topics and Tidy Tuesday workshops that cover more basic skills and materials. A collection of these latter workshop materials have been brought together by Harrie Jonkman in a single document that you can find here.\nStarting November 2023 we are transforming the workshops on specific topics into a reading group, where we explore the world of data analysis using the book Regression and Other Stories by Gelman, Hill and Vehtari as a source of inspiration. Click here for more information.\nIn this Notebook you find short summaries of the chapters of the ROS-book and presentations of these chapters by different chairs. The content of the Notebook will grow over time.\n\n\nMaterials\n\nHere you find the book.\nHere you find background information and syntaxes."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Chapter_2.html",
    "href": "Chapter_2.html",
    "title": "2  Chapter 2",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "01-chapter.html",
    "href": "01-chapter.html",
    "title": "1  Overview",
    "section": "",
    "text": "2 Summary\nThis chapter lays out the key challenges of statistical inference in general and regression modeling in particular.\n\n\nInference defined as using mathematical models to make general claims from particular data\nThere are three challenges to statistics, which all can be framed as problems of prediction: - Generalizing from sample to population;\n- Generalizing from treatment to control group;\n- Generalizing from observed measurements to the underlying construct of interest.\nThe key skills you learn in this book are: - Understanding regression models;\n- Constructing regression models;\n- Fitting regression models to data;\n- Displaying and interpreting the results of regression models;\nRegression is a method that allows researchers to summarize how predictions or average values of an outcome vary across individuals defined by a set of predictors. It is used for example to predict, to explore associations, to extrapolate and for causal inference. Exmaples are given.\nThere are four steps in statistical analysis: - Model building (starting);\n- Model fitting;\n- Understanding model fits;\n- Criticism.\nFitting models and making predictions can be down different frameworks. Three concerns are important everytime: - Information used;\n- Assumptions used;\n- Estimating and interpreting (classical or Bayesian framework).\nThey recommend to use the Bayesian framework. If information available you can use it, if not you can use weakly informative default priors. On this way you stable estimates and with the simulations you can express uncertainty.\nThe overall Bayesian regression in R is:\nfit&lt;-stan_glm(y~x,data=mydata)\n::: {.column-margin} Bayes can take longer time. Here you can use\nfit&lt;-stan_glm(y~x,data=mydata,algorithm=\"optimizing\")\nWhere y is the outcome, x is the predictor and mydata is the data frame. But you can do it also in classical framework:\nfit&lt;-lm(y~x,data=mydata)\nUsing Bayesian and simulation approaches can be more important when fitting multilevel or regularized regression models. This will be handled in their next book.\n\n3 Presentation\n\nFirst load libraries.\n\nlibrary(rosdata)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.26.1\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\n\n\nAttaching package: 'rstanarm'\n\n\nThe following objects are masked from 'package:rosdata':\n\n    kidiq, roaches, wells\n\n\nExplore data.\n\ndata(\"hibbs\")\nglimpse(hibbs)\n\nRows: 16\nColumns: 5\n$ year                &lt;int&gt; 1952, 1956, 1960, 1964, 1968, 1972, 1976, 1980, 19…\n$ growth              &lt;dbl&gt; 2.40, 2.89, 0.85, 4.21, 3.02, 3.62, 1.08, -0.39, 3…\n$ vote                &lt;dbl&gt; 44.60, 57.76, 49.91, 61.34, 49.60, 61.79, 48.95, 4…\n$ inc_party_candidate &lt;chr&gt; \"Stevenson\", \"Eisenhower\", \"Nixon\", \"Johnson\", \"Hu…\n$ other_candidate     &lt;chr&gt; \"Eisenhower\", \"Stevenson\", \"Kennedy\", \"Goldwater\",…\n\n\nReplicate the plot.\n\nggplot(data = hibbs,\n       mapping = aes(x = growth, y = vote)) +\n  # geom_label(mapping = aes(label = year), nudge_x = 0.3, fill = NA, size = 3) +\n  geom_point() \n\n\n\n\nRun model.\n\nM1 &lt;- stan_glm(vote ~ growth, data=hibbs)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.005053 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 50.53 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.059 seconds (Warm-up)\nChain 1:                0.048 seconds (Sampling)\nChain 1:                0.107 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.6e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.055 seconds (Warm-up)\nChain 2:                0.049 seconds (Sampling)\nChain 2:                0.104 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.6e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.057 seconds (Warm-up)\nChain 3:                0.055 seconds (Sampling)\nChain 3:                0.112 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.084 seconds (Warm-up)\nChain 4:                0.052 seconds (Sampling)\nChain 4:                0.136 seconds (Total)\nChain 4: \n\n\nModel summary.\n\nM1\n\nstan_glm\n family:       gaussian [identity]\n formula:      vote ~ growth\n observations: 16\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 46.3    1.7  \ngrowth       3.0    0.8  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 3.9    0.8   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nPrint slope.\n\ncoef(M1)\n\n(Intercept)      growth \n   46.29257     3.01333 \n\n\nAdd line to plot.\n\nggplot(data = hibbs,\n       mapping = aes(x = growth, y = vote)) +\n  geom_point() +\n  geom_abline(slope     = coef(M1)[[\"growth\"]],\n              intercept = coef(M1)[[\"(Intercept)\"]])\n\n\n\n\nPeacekeeping data.\n\npeace_df &lt;- read_csv(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/Peacekeeping/minidata.csv\")\n\nNew names:\nRows: 96 Columns: 7\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (6): cfdate, faildate, peacekeepers?, badness, delay, censored?\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nThis one is weird: badness var is ethnicity?!\n\npeace_df &lt;- read_csv(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/Peacekeeping/data/minidata.csv\")\n\nNew names:\nRows: 2031 Columns: 7\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): badness dbl (6): ...1, cfdate, faildate, peacekeepers?, delay, censored?\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nExplore\n\nglimpse(peace_df)\n\nRows: 2,031\nColumns: 7\n$ ...1            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ cfdate          &lt;dbl&gt; 140, 150, 210, 125, 126, 200, 110, 165, 190, 125, 200,…\n$ faildate        &lt;dbl&gt; 66, 64, 74, 66, 64, 65, 63, 68, 63, 64, 62, 73, 72, 72…\n$ `peacekeepers?` &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ badness         &lt;chr&gt; \"white\", \"black\", \"white\", \"white\", \"white\", \"white\", …\n$ delay           &lt;dbl&gt; 1, 1, 3, 5, 1, 1, 6, 1, 1, 4, 2, 1, 1, 1, 2, 1, 4, 4, …\n$ `censored?`     &lt;dbl&gt; 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, …\n\n\nCreate date measure. It’s actually the same as delay.\n\npeace_df &lt;- peace_df %&gt;% \n  mutate(time_diff = (faildate-cfdate)/365)\n\nPlot\n\npeace_df %&gt;% \n  ggplot(data = .) +\n  geom_histogram(mapping = aes(x = delay), bins = 10) +\n  facet_wrap(~`peacekeepers?`)\n\n\n\n\nScatter.\n\nggplot(data = peace_df) +\n  geom_point(mapping = aes(y = delay,\n                           colour = as.factor(`censored?`),\n                           x = badness,\n                           )) +\n  facet_wrap(~`peacekeepers?`)\n\n\n\n\nMeans.\n\npeace_df %&gt;% \n  group_by(`peacekeepers?`, `censored?`) %&gt;% \n  summarise(mean_badness = mean(badness, na.rm = TRUE))\n\nWarning: There were 5 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `mean_badness = mean(badness, na.rm = TRUE)`.\nℹ In group 1: `peacekeepers? = 0`, `censored? = 1`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\n\n`summarise()` has grouped output by 'peacekeepers?'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 5 × 3\n# Groups:   peacekeepers? [2]\n  `peacekeepers?` `censored?` mean_badness\n            &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1               0           1           NA\n2               0           2           NA\n3               1           1           NA\n4               1           2           NA\n5               1          NA           NA\n\n\nSimple causal graph for reproducibility of simulated data\n\nSEED &lt;- 1151\nset.seed(SEED)\nN &lt;- 50\nx &lt;- runif(N, 1, 5)\ny &lt;- rnorm(N, 10 + 3*x, 3)\nx_binary &lt;- ifelse(x&lt;3, 0, 1)\ncausal_df &lt;- data.frame(N, x, y, x_binary)\n\nPlot\n\nggplot(data = causal_df) +\n  geom_point(mapping = aes(y = y, x = x))"
  },
  {
    "objectID": "02-chapter.html",
    "href": "02-chapter.html",
    "title": "2  Data and measurement",
    "section": "",
    "text": "3 (1) Map visual\nLoad in HDI data.\nhdi_df &lt;- read.table(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/HDI/data/hdi.dat\", header = TRUE)\nGet state boundaries using the {maps} package.\nstates_sf &lt;- maps::map(\"state\",\n                       plot = FALSE,\n                       fill = TRUE) %&gt;% \n  st_as_sf()\nPlot this.\nggplot(data = states_sf) +\n  geom_sf()\nJoin the HDI data to the spatial polygons.\nstate_hdi_sf &lt;- hdi_df |&gt;\n  mutate(ID = str_to_lower(state)) |&gt; \n  right_join(states_sf) |&gt;\n  st_as_sf()\n\nJoining with `by = join_by(ID)`\nMap out HDI using a continuous scale.\nggplot(data = state_hdi_sf) +\n  geom_sf(mapping = aes(fill = hdi)) +\n  scale_fill_viridis_c()\nLoad in the income/hdi data.\nincome_df &lt;- read.dta(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/HDI/data/state vote and income, 68-00.dta\")\nCreate a common id again.\nincome_df &lt;- income_df %&gt;% \n  mutate(st_state = str_to_lower(st_state))\nScatterplots.\nincome_hdi_df &lt;-income_df %&gt;%\n  filter(st_year == 2000) %&gt;%\n  ggplot() +\n  geom_text(mapping = aes(x = st_income, y = hdi, label = st_stateabb))\n  geom_point(mapping = aes(x = st_income, y = hdi))\n\nmapping: x = ~st_income, y = ~hdi \ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity"
  },
  {
    "objectID": "01-chapter.html#summary",
    "href": "01-chapter.html#summary",
    "title": "1  Overview",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nThis first chapter lays out the key challenges of statistical inference in general and regression modeling in particular.\n\n\nInference defined as using mathematical models to make general claims from particular data\nThere are three challenges to statistics, which all can be framed as problems of prediction:\n- Generalizing from sample to population;\n- Generalizing from treatment to control group;\n- Generalizing from observed measurements to the underlying construct of interest.\nThe key skills you learn in this book are:\n- Understanding regression models;\n- Constructing regression models;\n- Fitting regression models to data;\n- Displaying and interpreting the results of regression models;\nRegression is a method that allows researchers to summarize how predictions or average values of an outcome vary across individuals defined by a set of predictors. It is used for example to predict, to explore associations, to extrapolate and for causal inference. Exmaples are given.\nThere are four steps in statistical analysis:\n- Model building (starting);\n- Model fitting;\n- Understanding model fits;\n- Criticism.\nFitting models and making predictions can be down different frameworks. Three concerns are important everytime: - Information used;\n- Assumptions used;\n- Estimating and interpreting (classical or Bayesian framework).\nGelman et all. recommend to use the Bayesian framework. If information available you can use it, if not you can use weakly informative default priors. On this way you stable estimates and with the simulations you can express uncertainty.\nThe overall Bayesian regression in R is:\n\nBayes can take longer time. Here you can use\nfit&lt;-stan_glm(y~x,data=mydata,algorithm=\"optimizing\")\n\nrstanarmbrms\n\n\nfit&lt;-stan_glm(y~x,data=mydata)\n\n\n\nfit&lt;-brm(y~x,data=mydata)\n\n\n\nWhere y is the outcome, x is the predictor and mydata is the data frame. But you can do it also in classical framework:\nfit&lt;-lm(y~x,data=mydata)\nUsing Bayesian and simulation approaches can be more important when fitting multilevel or regularized regression models. This will be handled in their next book."
  },
  {
    "objectID": "01-chapter.html#presentation",
    "href": "01-chapter.html#presentation",
    "title": "1  Overview",
    "section": "1.2 Presentation",
    "text": "1.2 Presentation\nFirst some libraries are loaded.\n\nlibrary(rosdata) # for the ROSdata\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(rstanarm) # for the stan_glm function\nlibrary(brms) # for the brm function"
  },
  {
    "objectID": "02-chapter.html#summary",
    "href": "02-chapter.html#summary",
    "title": "2  Data and measurement",
    "section": "2.1 Summary",
    "text": "2.1 Summary\nBefore fitting a model, you need to understand the data and the measurements, the numbers and where they come from. This chapter demonstrates through examples how to use graphical tools to explore understand and measurements.\nThe first example (graphing Human Development Index) shows that you understand data better when you plot them in different ways. The second example (Political Ideology and party identification) show that details of measurement can be important and that gaps between measurement and reality can be large.\nKeep in mind that the issue of measurement is important because of two reasons:\n- We need to understand what the data actually mean.\n- Learning about accuracy, reliability and validity will set the foundation for understanding variance, correlation, and error (all part of linear models).\nFeatures of data quality:\nA measure is valid to the degree that it represents what you are trying to measure. Validity of a measuring process is defined as a property of giving the right answer across a wide range of plausible scenarios.\nA measure is reliable to the degree that it gives the same answer when applied to the same situation. A reliable measure is one that is precise and stable.\nSelection is the third feature of data quality, the idea that the data you see can be a non-representative sample of a larger population that you will not see.\nSeveral visualization suggestions are given in this chapter.\nThree uses of graphics in statistical analysis are given:\n- Displays of raw data (part of explorative analysis), just to understand data;\n- Graphs of fitted models and inferences (to understand model fits);\n- Graphs as communication tools (to communicate results)."
  },
  {
    "objectID": "02-chapter.html#presentation",
    "href": "02-chapter.html#presentation",
    "title": "2  Data and measurement",
    "section": "2.2 Presentation",
    "text": "2.2 Presentation\nOn 12-12-2023 Sam Langton gave this presentation on chapter 2 of Regression and Other Stories for the *Reading Club Session (chapter 2). His script you can also find here.\nFirst load packages for this chapter.\n\nlibrary(foreign)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(broom)\nlibrary(maps)\nlibrary(sf)\nlibrary(rosdata)\n\nGeneral discussion points he put on the agenda:\n- New GitHub repo intro.\n- Difference between visualisation issues and then scrutiny over a composite measure (section 2.1).\n- Validity and reliability: any examples from people’s work?\n- What did people think of the plots? I had some queries (e.g., 2.5, 2.7).\n- Grammar of Graphics-style approach (but without ggplot2 code).\n- Plotting regression results.\n\n2.2.1 Map visual\nLoad in HDI data.\n\n# take care of the path to the data\n\nhdi_df &lt;- read.table(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/HDI/data/hdi.dat\", header = TRUE)\n\nGet state boundaries using the maps-package.\n\nstates_sf &lt;- maps::map(\"state\",\n                       plot = FALSE,\n                       fill = TRUE) %&gt;% \n  st_as_sf()\n\nPlot this.\n\nggplot(data = states_sf) +\n  geom_sf()\n\n\n\n\n\nMap of states\n\n\n\nJoin the HDI data to the spatial polygons.\n\nstate_hdi_sf &lt;- hdi_df |&gt;\n  mutate(ID = str_to_lower(state)) |&gt; \n  right_join(states_sf) |&gt;\n  st_as_sf()\n\nMap out HDI using a continuous scale.\n\nggplot(data = state_hdi_sf) +\n  geom_sf(mapping = aes(fill = hdi)) +\n  scale_fill_viridis_c()\n\n\n\n\n\nMap of HDI\n\n\n\n\n\n2.2.2 Scatterplots\nLoad in the income/hdi data.\n\n# take care of the path to the data\n\nincome_df &lt;- read.dta(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/HDI/data/state vote and income, 68-00.dta\")\n\nCreate a common id again.\n\nincome_df &lt;- income_df %&gt;% \n  mutate(st_state = str_to_lower(st_state))\n\nScatterplots.\n\n\n# Remark: why not a scatterplot?\n\nincome_hdi_df &lt;-income_df %&gt;%\n  filter(st_year == 2000) %&gt;%\n  ggplot() +\n  geom_text(mapping = aes(x = st_income, y = hdi, label = st_stateabb))\n  geom_point(mapping = aes(x = st_income, y = hdi))\n\nmapping: x = ~st_income, y = ~hdi \ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\n\n\n2.2.3 Names plots\n\n# take care of the path to the data\n\nnames_df &lt;- read.csv(\"~/Desktop/WERK/Gelman/ROS-book/ROS-book/ROS-Examples-master/Names/data/allnames_clean.csv\") %&gt;% \n  as_tibble()\n\nMake it long and filter.\n\nnames_long_df &lt;- names_df |&gt; \n  pivot_longer(cols = c(-X, -name, -sex), names_to = \"year\", values_to = \"number\",\n               names_prefix = \"X\") |&gt;\n  mutate(year = as.numeric(year),\n         last_letter = str_sub(name, start = -1)) \n\nNow Figure 2.7.\n\nnames_long_df %&gt;% \n  filter(year == 1906 | year == 1956 | year == 2006,\n         sex == \"M\") %&gt;% \n  group_by(year, last_letter) %&gt;% \n  summarise(counts = sum(number)) %&gt;% \n  ungroup() %&gt;% \n  group_by(year) %&gt;% \n  mutate(total_counts = sum(counts),\n         prop_counts  = counts/total_counts) %&gt;% \n  ungroup() %&gt;% \n  ggplot(data = .) +\n  geom_col(mapping = aes(x = last_letter, y = prop_counts,\n                         fill = as.factor(year)), \n           position = \"dodge\") +\n  facet_wrap(~year)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\nFigure 2.7\n\n\n\nMake a line graph.\n\nnames_long_df %&gt;% \n  filter(sex == \"M\") %&gt;%\n  group_by(year, last_letter) %&gt;% \n  summarize(yearly_counts = sum(number)) %&gt;% \n  ungroup() %&gt;% \n  group_by(year) %&gt;% \n  mutate(year_total  = sum(yearly_counts),\n         yearly_prop = yearly_counts/year_total,\n         end_letter  = if_else(last_letter == \"d\" |\n                               last_letter == \"y\" |\n                               last_letter == \"n\",\n                               last_letter,\n                               \"(other)\")) %&gt;% \n  ungroup() %&gt;% \n  ggplot(data = .) +\n  geom_line(mapping = aes(x = year, y = yearly_prop, group = last_letter,\n                          colour = end_letter)) +\n  scale_colour_manual(values = c(\"grey80\", \"black\", \"tomato\", \"dodgerblue\")) \n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\nLine graph"
  },
  {
    "objectID": "01-chapter.html#regression-and-other-stories-reading-club-session-chapter-1",
    "href": "01-chapter.html#regression-and-other-stories-reading-club-session-chapter-1",
    "title": "1  Overview",
    "section": "1.3 Regression and Other Stories Reading Club Session (chapter 1)",
    "text": "1.3 Regression and Other Stories Reading Club Session (chapter 1)\nOn 14-11-2023 Alex Trinidad (University of Cologne and Netherlands Institute for the Stduy of Crime and Law Enforcement) presented the first chapter of the book Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari. The session was held online via Zoom. Here you can find Alex’ script Trinidad.\nFirst he loaded this package.\n\nlibrary(tidyverse)\n\n\nRegression to predict\n\nHow can we predict presidential vote share using economy growth? For this he loaded the ROS-data.\n\nelections_data &lt;- read.csv(url(\"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectionsEconomy/data/hibbs.dat\"), sep = \"\")\n\nThis another way to load these data.\n\nremotes::install_github(\"avehtari/ROS-Examples\", subdir = \"rpackage\")\n\nSkipping install of 'rosdata' from a github remote, the SHA1 (a049a104) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nelections_data &lt;- rosdata::hibbs\n\nLet us first explore economy growth.\n\nglimpse(elections_data)\n\nRows: 16\nColumns: 5\n$ year                &lt;int&gt; 1952, 1956, 1960, 1964, 1968, 1972, 1976, 1980, 19…\n$ growth              &lt;dbl&gt; 2.40, 2.89, 0.85, 4.21, 3.02, 3.62, 1.08, -0.39, 3…\n$ vote                &lt;dbl&gt; 44.60, 57.76, 49.91, 61.34, 49.60, 61.79, 48.95, 4…\n$ inc_party_candidate &lt;chr&gt; \"Stevenson\", \"Eisenhower\", \"Nixon\", \"Johnson\", \"Hu…\n$ other_candidate     &lt;chr&gt; \"Eisenhower\", \"Stevenson\", \"Kennedy\", \"Goldwater\",…\n\n\nTry the view-function yourself.\n\n# View(elections_data)\n\nUse visualization to understand the data.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth))\n\n\n\n\n\nPredicting elections from the economy 1952-2016\n\n\n\nAdd a line to the plot.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth)) +\n  geom_smooth(aes(x = year, y = growth), se = FALSE)\n\n\n\n\n\nPredicting elections from the economy 1952-2016 with line\n\n\n\nAdd the CI around the line.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth)) +\n  geom_smooth(aes(x = year, y = growth), se = TRUE)\n\n\n\n\n\nPredicting elections from the economy 1952-2016 with line and confidence interval\n\n\n\nFit ols-regression to obtain the predicted values.\n\nmod1 &lt;- lm(vote ~ growth, data = elections_data)\n\nSummarize the regression results.\n\nsummary(mod1)\n\n\nCall:\nlm(formula = vote ~ growth, data = elections_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9929 -0.6674  0.2556  2.3225  5.3094 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  46.2476     1.6219  28.514 8.41e-14 ***\ngrowth        3.0605     0.6963   4.396  0.00061 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.763 on 14 degrees of freedom\nMultiple R-squared:  0.5798,    Adjusted R-squared:  0.5498 \nF-statistic: 19.32 on 1 and 14 DF,  p-value: 0.00061\n\n\nPlot the predicted values.\n\nplot(elections_data$growth, elections_data$vote, xlab = \"Economic Growth\", ylab = \"Vote Share\")\nabline(coef(mod1), col = \"red\")\n\n\n\n\n\nPredicting elections from the economy 1952-2016 with line\n\n\n\nPredicted values with ggplot.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = growth, y = vote)) +\n  geom_abline(intercept = mod1[[1]][[1]], slope = mod1[[1]][[2]], color = \"red\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Data and linear fit\",\n       x = \"Average recent growth in personal income\",\n       y = \"Incumbent party's vote share\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nPredicted values with ggplot and geom_smooth.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = growth, y = vote)) +\n  geom_smooth(method = \"lm\", aes(x = growth, y = vote), color = \"blue\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Data and linear fit\",\n       x = \"Average recent growth in personal income\",\n       y = \"Incumbent party's vote share\")\n\n\n\n\n\nSketching regression\n\nOriginal \\(y = 46.3 + 3.0 x\\). Explore the descriptive stats to get some parameters based on the observed data.\n\nelections_data |&gt; \n  summarise(min_growth = min(growth),\n            max_growth = max(growth),\n            mean_growth = mean(growth),\n            sd_growth = sd(growth),\n            min_vote = min(vote),\n            max_vote = max(vote),\n            mean_vote = mean(vote),\n            sd_vote = sd(vote))\n\n  min_growth max_growth mean_growth sd_growth min_vote max_vote mean_vote\n1      -0.39       4.21      1.8975  1.395538     44.6    61.79    52.055\n   sd_vote\n1 5.608951\n\n\nSimulating the data (technique often used in this book).\n\nset.seed(123)\nN &lt;- 16\nsimu_growth &lt;- runif(N, -0.39, 4)\nsimu_vote &lt;- rnorm(N, 46.2476  + 3.0605*simu_growth, 3.763)\nsimu_elections &lt;- data.frame(N,simu_growth, simu_vote)\n\nModel the simulated data.\n\nsimu_mod &lt;- lm(simu_vote ~ simu_growth, data = simu_elections)\n\nSummarize the model.\n\nsummary(simu_mod)\n\n\nCall:\nlm(formula = simu_vote ~ simu_growth, data = simu_elections)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.355 -1.513 -0.488  1.839  5.962 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  43.6769     1.7558  24.876 5.49e-13 ***\nsimu_growth   4.0052     0.6948   5.765 4.90e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.448 on 14 degrees of freedom\nMultiple R-squared:  0.7036,    Adjusted R-squared:  0.6824 \nF-statistic: 33.23 on 1 and 14 DF,  p-value: 4.896e-05\n\n\nPlot the simulated data using base graphics.\n\n# Base graphic\nplot(simu_elections$simu_growth, simu_elections$simu_vote, xlab = \"Simulated Economic Growth\", ylab = \"Simulated Vote Share\")\nabline(coef(simu_mod), col = \"blue\")\n\n\n\n\n\nSimulated Data and linear fit\n\n\n\nPlot the samen using ggplot version.\n\nggplot(data = simu_elections) +\n  geom_point(aes(x = simu_growth, y = simu_vote)) +\n  geom_smooth(method = \"lm\", aes(x = simu_growth, y = simu_vote), color = \"blue\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Simulated Data and linear fit\",\n       x = \"Simulated Average recent growth in personal income\",\n       y = \"Simulated Incumbent party's vote share\")\n\n\n\n\nExercise 1.2(a) from ROS for sketching a regression model and data.\n\n\\(y = 30 + 10x\\) (residual \\(sd 3.9\\)) & values of X ranging from \\(0-4\\)\n\nDefine the data.\n\nset.seed(123)\nN &lt;- 50\nx &lt;- runif(N, 0, 4)\ny &lt;- rnorm(N, 30 + 10*x, 3.9)\ndata &lt;- data.frame(N, x, y)\n\nModel the data.\n\nlm_a &lt;- lm(y ~ x, data)\n\nPlot the data.\n\nplot(data$x, data$y, xlab = \"X Value\", ylab = \"Y value\")\nabline(coef(lm_a), col = \"red\", size = 1)\n\nWarning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): \"size\" is\nnot a graphical parameter\n\n\n\n\n\n\nExercise 1.2 from ROS\n\n\n\n\n\\(y = 30 + 10x\\) (residual \\(sd 10\\)) & values of X ranging from \\(0-4\\).\n\nDefine the data.\n\nset.seed(123)\nN &lt;- 50\nx &lt;- runif(N, 0, 4)\ny &lt;- rnorm(N, 30 + 10*x, 10)\ndata &lt;- data.frame(N, x, y)\n\nModel it.\n\nlm_b &lt;- lm(y ~ x, data)\n\nPlot it.\n\nplot(data$x, data$y, xlab = \"X Value\", ylab = \"Y value\")\nabline(coef(lm_b), col = \"blue\")\n\n\n\n\n\nContinuous predictor\n\n\n\nNow simulate a binary predictor example from the Aki Vehtari GH\nSee Figure 1.5 (page 10).\n\nset.seed(1411)\nN &lt;- 50\nx &lt;- runif(N, 0, 4)\ny &lt;- rnorm(N, 30 + 10*x, 10)\nx_binary &lt;- ifelse(x &lt; 3, 0, 1)\ndata_simu &lt;- data.frame(N, x, y, x_binary)\n\nModel it.\n\nlm_binary &lt;- lm(y ~ x_binary, data = data_simu)\n\nSummarize the model.\n\nsummary(lm_binary)\n\n\nCall:\nlm(formula = y ~ x_binary, data = data_simu)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-27.2063  -8.5257   0.5297   9.3644  27.8011 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.812      2.296  19.953  &lt; 2e-16 ***\nx_binary      19.033      3.827   4.974 8.81e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.99 on 48 degrees of freedom\nMultiple R-squared:  0.3401,    Adjusted R-squared:  0.3264 \nF-statistic: 24.74 on 1 and 48 DF,  p-value: 8.813e-06\n\n\nPlot the relationship.\n\nggplot(data = data_simu) +\n  geom_point(aes(x = x_binary, y = y)) +\n  geom_abline(intercept = lm_binary[[1]][[1]], slope = lm_binary[[1]][[2]],\n              color = \"blue\", size = 1) +\n  labs(y = \"Crime reduction\", \n       x =  NULL) +\n  scale_x_continuous(breaks = c(0,1),\n                     labels = c(\"Control\", \"Treatment\")) +\n  annotate(geom = \"text\", x = 0.50, y = 40,\n           label = paste(\"Estimated treatment effect is\\nslope of fitted line: \",\n                         round(lm_binary[[1]][[2]], digits = 2)))\n\n\n\n\n\nBinary predictor\n\n\n\nNon-linear relationship\n\nset.seed(1411)\nx &lt;- runif(N, 1, 7)\ny &lt;- rnorm(N, 7 + 30*exp(-x), 2)\ndata_simu$y &lt;- y\n\nFit the model.\n\nlm_nonlinear &lt;- lm(y ~ x, data = data_simu)\n\nSummarize the model.\n\nsummary(lm_nonlinear)\n\n\nCall:\nlm(formula = y ~ x, data = data_simu)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0484 -1.4874 -0.0243  1.7868  4.4113 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.1516     0.6188  21.253  &lt; 2e-16 ***\nx            -1.8761     0.2476  -7.579  9.6e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.21 on 48 degrees of freedom\nMultiple R-squared:  0.5447,    Adjusted R-squared:  0.5353 \nF-statistic: 57.43 on 1 and 48 DF,  p-value: 9.599e-10\n\n\nPlot the model outcome.\n\nggplot(data = data_simu) +\n  geom_point(aes(x = x, y = y)) +\n  geom_smooth(method = \"loess\", aes(x = x, y = y), color = \"blue\", size = 1, se = FALSE) +\n  labs(y = \"Theft counts per hour\", \n       x =  \"Hours of foot patrol\")  \n\n\n\n\n\nNon-linear relationship"
  },
  {
    "objectID": "01-chapter.html#more-examples",
    "href": "01-chapter.html#more-examples",
    "title": "1  Overview",
    "section": "1.4 More examples",
    "text": "1.4 More examples\nFirst look at dataset to predict US-elections (1952-2021) from the economy and explore data.\n\ndata(\"hibbs\")\nglimpse(hibbs)\n\nRows: 16\nColumns: 5\n$ year                &lt;int&gt; 1952, 1956, 1960, 1964, 1968, 1972, 1976, 1980, 19…\n$ growth              &lt;dbl&gt; 2.40, 2.89, 0.85, 4.21, 3.02, 3.62, 1.08, -0.39, 3…\n$ vote                &lt;dbl&gt; 44.60, 57.76, 49.91, 61.34, 49.60, 61.79, 48.95, 4…\n$ inc_party_candidate &lt;chr&gt; \"Stevenson\", \"Eisenhower\", \"Nixon\", \"Johnson\", \"Hu…\n$ other_candidate     &lt;chr&gt; \"Eisenhower\", \"Stevenson\", \"Kennedy\", \"Goldwater\",…\n\n\nReplicate the plot of Figure 1.1.\n\nggplot(data = hibbs,\n       mapping = aes(x = growth, y = vote)) +\n  # geom_label(mapping = aes(label = year), nudge_x = 0.3, fill = NA, size = 3) +\n  geom_point() \n\n\n\n\n\nPredicting elections from the economy 1952-2016\n\n\n\nNow run the first regression model using stanarm or brms. This simulation works with four chains and 2000 iterations per chain.\n\nrstanarmbrms\n\n\n\nM1 &lt;- stan_glm(vote ~ growth, data=hibbs)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.82 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.097 seconds (Warm-up)\nChain 1:                0.057 seconds (Sampling)\nChain 1:                0.154 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.5e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.057 seconds (Warm-up)\nChain 2:                0.046 seconds (Sampling)\nChain 2:                0.103 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.5e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.051 seconds (Warm-up)\nChain 3:                0.048 seconds (Sampling)\nChain 3:                0.099 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.075 seconds (Warm-up)\nChain 4:                0.053 seconds (Sampling)\nChain 4:                0.128 seconds (Total)\nChain 4: \n\n\nM1 is set on your computer and you can give a summary of this regression model.\n\nM1\n\nstan_glm\n family:       gaussian [identity]\n formula:      vote ~ growth\n observations: 16\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 46.3    1.6  \ngrowth       3.0    0.7  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 3.9    0.7   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nOr print the intercept (46.26) and the slope (3.05) of this model.\n\ncoef(M1)\n\n(Intercept)      growth \n  46.300290    3.037234 \n\n\n\n\n\nM2 &lt;- brm(vote ~ growth, data=hibbs)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.026 seconds (Warm-up)\nChain 1:                0.022 seconds (Sampling)\nChain 1:                0.048 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 8e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 2:                0.021 seconds (Sampling)\nChain 2:                0.051 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 3:                0.023 seconds (Sampling)\nChain 3:                0.051 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 4:                0.023 seconds (Sampling)\nChain 4:                0.053 seconds (Total)\nChain 4: \n\n\nM2 is set on your computer and you can give a summary of this regression model.\n\nM2 &lt;-\n  brm(data = hibbs,\n      vote ~ growth,\n      cores = 4, chains = 4, iter = 2000,\n      seed = 123)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nM2\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: vote ~ growth \n   Data: hibbs (Number of observations: 16) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    46.15      1.85    42.36    49.74 1.00     2993     2088\ngrowth        3.07      0.80     1.49     4.73 1.00     2757     1903\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.07      0.83     2.84     6.06 1.00     2683     1955\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nNow add line to plot.\n\nggplot(data = hibbs,\n       mapping = aes(x = growth, y = vote)) +\n  geom_point() +\n  geom_abline(slope     = coef(M1)[[\"growth\"]],\n              intercept = coef(M1)[[\"(Intercept)\"]]) \n\n\n\n\n\nPredicting elections from the economy 1952-2016\n\n\n\nWe also looked at the peacekeeping data (1.3). First open the data.\n\npeace_df &lt;- read_csv(\"~/Desktop/WERK/Gelman/ROS-book/ROS-book/ROS-Examples-master/Peacekeeping/data/minidata.csv\")\n\nExplore this dataset now.\n\nglimpse(peace_df)\n\nRows: 2,031\nColumns: 7\n$ ...1            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ cfdate          &lt;dbl&gt; 140, 150, 210, 125, 126, 200, 110, 165, 190, 125, 200,…\n$ faildate        &lt;dbl&gt; 66, 64, 74, 66, 64, 65, 63, 68, 63, 64, 62, 73, 72, 72…\n$ `peacekeepers?` &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ badness         &lt;chr&gt; \"white\", \"black\", \"white\", \"white\", \"white\", \"white\", …\n$ delay           &lt;dbl&gt; 1, 1, 3, 5, 1, 1, 6, 1, 1, 4, 2, 1, 1, 1, 2, 1, 4, 4, …\n$ `censored?`     &lt;dbl&gt; 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, …\n\n\nCreate date measure. It’s actually the same as delay.\n\npeace_df &lt;- peace_df |&gt;\n  mutate(time_diff = (faildate-cfdate)/365)\n\nLet us plot it …\n\n\n# Harrie: not working\n# peace_df |&gt;\n#  ggplot(data = .) +\n#  geom_histogram(mapping = aes(x = delay), bins = 10) +\n#  facet_wrap(~`peacekeepers?`) \n… or put it in a scatterplot.\n\nggplot(data = peace_df) +\n  geom_point(mapping = aes(y = delay,\n                           colour = as.factor(`censored?`),\n                           x = badness,\n                           )) +\n  facet_wrap(~`peacekeepers?`) \n\n\n\n\n\nOutcomes after civil war in countries with and without UN-peacekeepers\n\n\n\nMeans.\n\npeace_df |&gt; \n  group_by(`peacekeepers?`, `censored?`) |&gt; \n  summarise(mean_badness = mean(badness, na.rm = TRUE))\n\n# A tibble: 5 × 3\n# Groups:   peacekeepers? [2]\n  `peacekeepers?` `censored?` mean_badness\n            &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1               0           1           NA\n2               0           2           NA\n3               1           1           NA\n4               1           2           NA\n5               1          NA           NA\n\n\nSimple causal graph for reproducibility of simulated data.\n\nSEED &lt;- 1151\nset.seed(SEED)\nN &lt;- 50\nx &lt;- runif(N, 1, 5)\ny &lt;- rnorm(N, 10 + 3*x, 3)\nx_binary &lt;- ifelse(x&lt;3, 0, 1)\ncausal_df &lt;- data.frame(N, x, y, x_binary)\n\nPlot this.\n\nggplot(data = causal_df) +\n  geom_point(mapping = aes(y = y, x = x)) \n\n\n\n\n\nCausal graph of simulated data"
  },
  {
    "objectID": "03-chapter.html#summary",
    "href": "03-chapter.html#summary",
    "title": "3  Some basic methods in mathematics and probability",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nSimple methods from introductory mathematics and probability have three important roles in regression modelling.\n- Linear algebra and simple probability distributions are the building blocks for elaborate models.\n- It is useful to understand the basic ideas of inference separately from the details of particular class of model.\n- It is often useful in practice to construct quick estimates and comparisons for small parts of a problem - before fitting an elaborate model, or in understanding the output from such a model.\nThis chapter provides a quick review of some of these basic ideas.\nFirst some ideas from algebra are presented:\n- Weighted averages are used to adept to a target population (for eg. the average age of all North American as a weighted average).\n- Vectors are used to represent a collection of numbers and matrices are used to represent a collection of vectors.\n- To use linear regression effectively, you need to understand the algebra and geometry of straight lines, with the intercept and the slope.\n- To use logarithmic and log-log relationships for exponential and power-law growth and decline.\nHere an example of a regression line.\n\n#\nlibrary(tidyverse)\nlibrary(patchwork)\n# Thanks Solomon Kurz \n# set the global plotting theme\ntheme_set(theme_linedraw() +\n            theme(panel.grid = element_blank()))\n\na &lt;- 0\nb &lt;- 1\n\n# left\np1 &lt;-\n  tibble(x = 0:2) %&gt;% \n  mutate(y = a + b * x) %&gt;%\n  \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = 0:2) +\n  scale_y_continuous(breaks = 0:2, labels = c(\"a\", \"a+b\", \"a+2b\")) +\n  labs(subtitle = expression(y==a+bx~(with~b&gt;0)))\n\nb &lt;- -1\n\n# right\np2 &lt;-\n  tibble(x = 0:2) %&gt;% \n  mutate(y = a + b * x) %&gt;%\n  \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = 0:2) +\n  scale_y_continuous(breaks = 0:-2, labels = c(\"a\", \"a+b\", \"a+2b\")) +\n  labs(subtitle = expression(y==a+bx~(with~b&lt;0)))\n\n# combine with patchwork\nlibrary(patchwork)\n\np1 + p2\n\n\n\n\nProbabilistic distributions are used in regression modeling to help to characterize the variation that remains after predicting the average. These distributions allow us to get a handle on how uncertain our predictions are and, additionally, our uncertainty in the estimated parameters of the model. Mean (expected value), variance (mean of squared difference from the mean), and standard deviation (square root of variance) are the basic concepts of probability distributions.\nNormal distribution, binomial distribution, and Poisson distribution and Unclassified probability distributions are types of probability distributions presented here. They will be worked out in detail in the following chapters.\nIn regression we typically model as much of the data variation as possible with a deterministic model, with a probability distribution included to capture the error, or unexplained variation. Distributions can be used to compare using such as the mean, but also to look at shifts in quantiles for example. Probability distributions can also be used for predicting new outcomes."
  },
  {
    "objectID": "01-chapter.html#presentation-1",
    "href": "01-chapter.html#presentation-1",
    "title": "1  Overview",
    "section": "1.3 Presentation",
    "text": "1.3 Presentation\nOn 14-11-2023 Alex Trinidad (University of Cologne and Netherlands Institute for the Study of Crime and Law Enforcement) presented the first chapter of the book Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari: Overview. The session was held online via Zoom. Here you can find Alex’ script Trinidad.\nFirst he loaded this package.\n\nlibrary(tidyverse)\n\n\nRegression to predict\n\nHow can we predict presidential vote share using economy growth? For this he loaded the ROS-data.\n\nelections_data &lt;- read.csv(url(\"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectionsEconomy/data/hibbs.dat\"), sep = \"\")\n\nThis another way to load these data.\n\nremotes::install_github(\"avehtari/ROS-Examples\", subdir = \"rpackage\")\n\nSkipping install of 'rosdata' from a github remote, the SHA1 (a049a104) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nelections_data &lt;- rosdata::hibbs\n\nLet us first explore economy growth.\n\nglimpse(elections_data)\n\nRows: 16\nColumns: 5\n$ year                &lt;int&gt; 1952, 1956, 1960, 1964, 1968, 1972, 1976, 1980, 19…\n$ growth              &lt;dbl&gt; 2.40, 2.89, 0.85, 4.21, 3.02, 3.62, 1.08, -0.39, 3…\n$ vote                &lt;dbl&gt; 44.60, 57.76, 49.91, 61.34, 49.60, 61.79, 48.95, 4…\n$ inc_party_candidate &lt;chr&gt; \"Stevenson\", \"Eisenhower\", \"Nixon\", \"Johnson\", \"Hu…\n$ other_candidate     &lt;chr&gt; \"Eisenhower\", \"Stevenson\", \"Kennedy\", \"Goldwater\",…\n\n\nTry the view-function yourself.\n\n# View(elections_data)\n\nUse visualization to understand the data.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth))\n\n\n\n\n\nPredicting elections from the economy 1952-2016\n\n\n\nAdd a line to the plot.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth)) +\n  geom_smooth(aes(x = year, y = growth), se = FALSE)\n\n\n\n\n\nPredicting elections from the economy 1952-2016 with line\n\n\n\nAdd the CI around the line.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth)) +\n  geom_smooth(aes(x = year, y = growth), se = TRUE)\n\n\n\n\n\nPredicting elections from the economy 1952-2016 with line and confidence interval\n\n\n\nFit ols-regression to obtain the predicted values.\n\nmod1 &lt;- lm(vote ~ growth, data = elections_data)\n\nSummarize the regression results.\n\nsummary(mod1)\n\n\nCall:\nlm(formula = vote ~ growth, data = elections_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9929 -0.6674  0.2556  2.3225  5.3094 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  46.2476     1.6219  28.514 8.41e-14 ***\ngrowth        3.0605     0.6963   4.396  0.00061 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.763 on 14 degrees of freedom\nMultiple R-squared:  0.5798,    Adjusted R-squared:  0.5498 \nF-statistic: 19.32 on 1 and 14 DF,  p-value: 0.00061\n\n\nPlot the predicted values.\n\nplot(elections_data$growth, elections_data$vote, xlab = \"Economic Growth\", ylab = \"Vote Share\")\nabline(coef(mod1), col = \"red\")\n\n\n\n\n\nPredicting elections from the economy 1952-2016 with line\n\n\n\nPredicted values with ggplot.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = growth, y = vote)) +\n  geom_abline(intercept = mod1[[1]][[1]], slope = mod1[[1]][[2]], color = \"red\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Data and linear fit\",\n       x = \"Average recent growth in personal income\",\n       y = \"Incumbent party's vote share\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nPredicted values with ggplot and geom_smooth.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = growth, y = vote)) +\n  geom_smooth(method = \"lm\", aes(x = growth, y = vote), color = \"blue\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Data and linear fit\",\n       x = \"Average recent growth in personal income\",\n       y = \"Incumbent party's vote share\")\n\n\n\n\n\nSketching regression\n\nOriginal \\(y = 46.3 + 3.0 x\\). Explore the descriptive stats to get some parameters based on the observed data.\n\nelections_data |&gt; \n  summarise(min_growth = min(growth),\n            max_growth = max(growth),\n            mean_growth = mean(growth),\n            sd_growth = sd(growth),\n            min_vote = min(vote),\n            max_vote = max(vote),\n            mean_vote = mean(vote),\n            sd_vote = sd(vote))\n\n  min_growth max_growth mean_growth sd_growth min_vote max_vote mean_vote\n1      -0.39       4.21      1.8975  1.395538     44.6    61.79    52.055\n   sd_vote\n1 5.608951\n\n\nSimulating the data (technique often used in this book).\n\nset.seed(123)\nN &lt;- 16\nsimu_growth &lt;- runif(N, -0.39, 4)\nsimu_vote &lt;- rnorm(N, 46.2476  + 3.0605*simu_growth, 3.763)\nsimu_elections &lt;- data.frame(N,simu_growth, simu_vote)\n\nModel the simulated data.\n\nsimu_mod &lt;- lm(simu_vote ~ simu_growth, data = simu_elections)\n\nSummarize the model.\n\nsummary(simu_mod)\n\n\nCall:\nlm(formula = simu_vote ~ simu_growth, data = simu_elections)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.355 -1.513 -0.488  1.839  5.962 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  43.6769     1.7558  24.876 5.49e-13 ***\nsimu_growth   4.0052     0.6948   5.765 4.90e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.448 on 14 degrees of freedom\nMultiple R-squared:  0.7036,    Adjusted R-squared:  0.6824 \nF-statistic: 33.23 on 1 and 14 DF,  p-value: 4.896e-05\n\n\nPlot the simulated data using base graphics.\n\n# Base graphic\nplot(simu_elections$simu_growth, simu_elections$simu_vote, xlab = \"Simulated Economic Growth\", ylab = \"Simulated Vote Share\")\nabline(coef(simu_mod), col = \"blue\")\n\n\n\n\n\nSimulated Data and linear fit\n\n\n\nPlot the samen using ggplot version.\n\nggplot(data = simu_elections) +\n  geom_point(aes(x = simu_growth, y = simu_vote)) +\n  geom_smooth(method = \"lm\", aes(x = simu_growth, y = simu_vote), color = \"blue\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Simulated Data and linear fit\",\n       x = \"Simulated Average recent growth in personal income\",\n       y = \"Simulated Incumbent party's vote share\")\n\n\n\n\nExercise 1.2(a) from ROS for sketching a regression model and data.\n\n\\(y = 30 + 10x\\) (residual \\(sd 3.9\\)) & values of X ranging from \\(0-4\\)\n\nDefine the data.\n\nset.seed(123)\nN &lt;- 50\nx &lt;- runif(N, 0, 4)\ny &lt;- rnorm(N, 30 + 10*x, 3.9)\ndata &lt;- data.frame(N, x, y)\n\nModel the data.\n\nlm_a &lt;- lm(y ~ x, data)\n\nPlot the data.\n\nplot(data$x, data$y, xlab = \"X Value\", ylab = \"Y value\")\nabline(coef(lm_a), col = \"red\", size = 1)\n\nWarning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): \"size\" is\nnot a graphical parameter\n\n\n\n\n\n\nExercise 1.2 from ROS\n\n\n\n\n\\(y = 30 + 10x\\) (residual \\(sd 10\\)) & values of X ranging from \\(0-4\\).\n\nDefine the data.\n\nset.seed(123)\nN &lt;- 50\nx &lt;- runif(N, 0, 4)\ny &lt;- rnorm(N, 30 + 10*x, 10)\ndata &lt;- data.frame(N, x, y)\n\nModel it.\n\nlm_b &lt;- lm(y ~ x, data)\n\nPlot it.\n\nplot(data$x, data$y, xlab = \"X Value\", ylab = \"Y value\")\nabline(coef(lm_b), col = \"blue\")\n\n\n\n\n\nContinuous predictor\n\n\n\nNow simulate a binary predictor example from the Aki Vehtari GH\nSee Figure 1.5 (page 10).\n\nset.seed(1411)\nN &lt;- 50\nx &lt;- runif(N, 0, 4)\ny &lt;- rnorm(N, 30 + 10*x, 10)\nx_binary &lt;- ifelse(x &lt; 3, 0, 1)\ndata_simu &lt;- data.frame(N, x, y, x_binary)\n\nModel it.\n\nlm_binary &lt;- lm(y ~ x_binary, data = data_simu)\n\nSummarize the model.\n\nsummary(lm_binary)\n\n\nCall:\nlm(formula = y ~ x_binary, data = data_simu)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-27.2063  -8.5257   0.5297   9.3644  27.8011 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.812      2.296  19.953  &lt; 2e-16 ***\nx_binary      19.033      3.827   4.974 8.81e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.99 on 48 degrees of freedom\nMultiple R-squared:  0.3401,    Adjusted R-squared:  0.3264 \nF-statistic: 24.74 on 1 and 48 DF,  p-value: 8.813e-06\n\n\nPlot the relationship.\n\nggplot(data = data_simu) +\n  geom_point(aes(x = x_binary, y = y)) +\n  geom_abline(intercept = lm_binary[[1]][[1]], slope = lm_binary[[1]][[2]],\n              color = \"blue\", size = 1) +\n  labs(y = \"Crime reduction\", \n       x =  NULL) +\n  scale_x_continuous(breaks = c(0,1),\n                     labels = c(\"Control\", \"Treatment\")) +\n  annotate(geom = \"text\", x = 0.50, y = 40,\n           label = paste(\"Estimated treatment effect is\\nslope of fitted line: \",\n                         round(lm_binary[[1]][[2]], digits = 2)))\n\n\n\n\n\nBinary predictor\n\n\n\nNon-linear relationship\n\nset.seed(1411)\nx &lt;- runif(N, 1, 7)\ny &lt;- rnorm(N, 7 + 30*exp(-x), 2)\ndata_simu$y &lt;- y\n\nFit the model.\n\nlm_nonlinear &lt;- lm(y ~ x, data = data_simu)\n\nSummarize the model.\n\nsummary(lm_nonlinear)\n\n\nCall:\nlm(formula = y ~ x, data = data_simu)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0484 -1.4874 -0.0243  1.7868  4.4113 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.1516     0.6188  21.253  &lt; 2e-16 ***\nx            -1.8761     0.2476  -7.579  9.6e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.21 on 48 degrees of freedom\nMultiple R-squared:  0.5447,    Adjusted R-squared:  0.5353 \nF-statistic: 57.43 on 1 and 48 DF,  p-value: 9.599e-10\n\n\nPlot the model outcome.\n\nggplot(data = data_simu) +\n  geom_point(aes(x = x, y = y)) +\n  geom_smooth(method = \"loess\", aes(x = x, y = y), color = \"blue\", size = 1, se = FALSE) +\n  labs(y = \"Theft counts per hour\", \n       x =  \"Hours of foot patrol\")  \n\n\n\n\n\nNon-linear relationship"
  },
  {
    "objectID": "03-chapter.html#presentation",
    "href": "03-chapter.html#presentation",
    "title": "3  Some basic methods in mathematics and probability",
    "section": "3.2 Presentation",
    "text": "3.2 Presentation\nRemark: Following part has to be designed further (Harrie)\nWim Bernasco chaired the practice part of chapter 3 on Tuesday 23rd of January 2024. His original script you can find here\nOpen two libraries first.\n\n# For tidy data processing\nlibrary(tidyverse)\n# Access data from \"Regression and other stories\"\nlibrary(rosdata)\n\nWim looked at the excercises of chapter 3. He start with excercise 3.1: Weighted averages.\nYou often encounter weighted averages when you work with aggregated data, such as averages in subpopulations using\nGroups: the categories that are being weighted (here four age groups)\nShares: the proportions of each group in the sample\nMeans : the means values of ‘something’ in each group\n\nWe first calculate total sample size by summing the four categories:\n\nn_sample = 200 + 250 + 300 + 250\nn_sample\n\n[1] 1000\n\n\nNext we multiply the means of the groups with their share in the sample.\n\n50 * 200/n_sample + \n60 * 250/n_sample +\n40 * 300/n_sample +\n30 * 250/n_sample\n\n[1] 44.5\n\n\nWe can also do this more systematically and in a tidy way. Set up a little table that holds the relevant input data.\n\ntax_support &lt;- \n  tibble(age_class        = c(\"18-29\", \"30-44\", \"45-64\", \"65+\"),\n         tax_support      = c(    50 ,    60 ,     40,   30),\n         sample_frequency = c(   200 ,   250 ,    300,  250))\ntax_support\n\n# A tibble: 4 × 3\n  age_class tax_support sample_frequency\n  &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1 18-29              50              200\n2 30-44              60              250\n3 45-64              40              300\n4 65+                30              250\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote. I stumbled on the tribble function from the tibble package. This allows inputting observations in rows and variables in columns. Nice for small inline dataframes.\n\n\n\ntax_support_alt &lt;-\n  tribble(\n    ~age_class, ~tax_support, ~sample_frequency,\n       \"18-29\",   50,   200,\n       \"30-44\",   60,   250,\n       \"45-64\",   40,   300,\n       \"65+\"  ,   30,   250\n)\ntax_support_alt\n\n# A tibble: 4 × 3\n  age_class tax_support sample_frequency\n  &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1 18-29              50              200\n2 30-44              60              250\n3 45-64              40              300\n4 65+                30              250\n\n\nNext we calculate the weighted average. - We first calculate the share in the sample of each group\n- Next multiply shares with percentage tax support\n- And finally sum over the four groups\n\n\ntax_support |&gt;\n  mutate(sample_share = sample_frequency / sum(sample_frequency),\n         sample_tax_support = tax_support * sample_share) |&gt;\n  summarize(sample_tax_support = sum(sample_tax_support))\n\n# A tibble: 1 × 1\n  sample_tax_support\n               &lt;dbl&gt;\n1               44.5\n\n\nThen he looked at excercise 3.2. He was not sure whether he fully understood this exercise. He assumed the idea is to let us think about how other age distributions would affect the level of support. Thus, if the tax support in the four age groups is given, which age group distributions would yield an overall support percentage of 40? An obvious but unrealistic distribution would consist of only people aged 30-44, because among this group the support is precisely \\(40\\%\\).\nMathematically, the situation can be described with 2 equations with 4 unknowns.\nThe first equation would just constrain the four weights to sum to 1.\n\\[(eq 1) wght_18 + wght_30 + wght_45 + wght_65  = 1\\]\nThe second equation would constrain the weighted average to be 40\n\\[(eq 2) wght_18 * 50 + wght_30 * 60 + wght_45 * 40 + wght_65 * 30 = 40\\]\nTo find a deterministic solution, we need to assign values to two unknowns\nIf we fix the shares of the least extreme age classes “18-29” and “45-64” to their original values (.2 and .3), we should be able to get an overall tax support of \\(40\\%\\) by finding a suitable mix of age group 30-44 (supportlevel \\(60\\%\\)) and age group 65+ (support level \\(30\\%\\)).\n\\[(eq 1) wght_30 + wght_65  = .5\\] \\[(eq 2) wght_30 * 60 + wght_65 * 30 = 40 - 22 = 18\\]\n\\[(eq 1) wght_30 = .5 - wght_65\\] \\[(eq 2) (.5 - wght_65) * 60 + weight_65 * 30 = 18\\]\n\\[(eq 1) wght_30 = .5 - wght_65\\] \\[(eq 2) 30 - wght_65 * 60 + weight_65 * 30 = 18\\]\n\\[(eq 1) wght_30 = .5 - wght_65\\] \\[(eq 2) 30 - 30 * wght_65 = 18\\]\n\\[(eq 1) wght_30 = .5 - wght_65\\] \\[(eq 2) 30 * wght_65 = 12\\]\n\\[(eq 1) wght_30 = .5 - 0.4 = .1\\] \\[(eq 2) wght_65 = 12/30 = .4\\]\nSo, we get\n\n   50 * .2 + # this was fixed\n   60 * .1 + # this was calculated\n   40 * .3 + # this was fixed\n   30 * .4  # this was calculated\n\n[1] 40\n\n\nSo it worked.\nNow he looked at excercise 3.3 Plotting a line.\ncurve(expr = x,\n      from = 0,\n      to   = 20)\nThis gave an Error in x(x): object ‘x’ not found. Wim thinks the curve function needs at least one existing function name (log, exp, sqrt, sin, cos, …) or math symbol (+ - / * ^ …) beyond the ‘x’\nHere are some lines\n\ncurve(expr = x*1,\n      from = 0,\n      to   = 20)\n\n\n\n\n\nA line\n\n\n\nA flat line\n\ncurve(expr = 1 + 0 * x,\n      from = 0,\n      to   = 20)\n\n\n\n\n\nAnother flat line\n\n\n\nAnother flat line\n\ncurve(expr = 1 + 5 * x,\n      from = 0,\n      to   = 20)\n\n\n\n\n\nAnother flat line\n\n\n\nA log line\n\ncurve(expr = log(1 + x),\n      from = 0,\n      to   = 20)\n\n\n\n\n\nA log line\n\n\n\nAn exponential line\n\ncurve(expr = exp( x),\n      from = 0,\n      to   = 20)\n\n\n\n\n\nAn exponential line\n\n\n\nA square root line\n\ncurve(expr = sin(sqrt(x)),\n      from = 0,\n      to   = 2000)\n\n\n\n\n\nA square root line\n\n\n\nMile record data\nOpen dataset from rosdata.\n\ndata(\"mile\")\nglimpse(mile)\n\nRows: 32\nColumns: 6\n$ yr      &lt;int&gt; 1913, 1915, 1923, 1931, 1933, 1934, 1937, 1942, 1942, 1942, 19…\n$ month   &lt;dbl&gt; 5, 7, 8, 10, 7, 6, 8, 7, 7, 9, 7, 7, 7, 5, 6, 7, 8, 1, 11, 6, …\n$ min     &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ sec     &lt;dbl&gt; 14.4, 12.6, 10.4, 9.2, 7.6, 6.8, 6.4, 6.2, 6.2, 4.6, 2.6, 1.6,…\n$ year    &lt;dbl&gt; 1913.417, 1915.583, 1923.667, 1931.833, 1933.583, 1934.500, 19…\n$ seconds &lt;dbl&gt; 254.4, 252.6, 250.4, 249.2, 247.6, 246.8, 246.4, 246.2, 246.2,…\n\n\nHe guesses that year is a time variable that include \\(monthnumber/12\\) as decimals. Let us check the first two cases.\n\n1913 + 5/12\n\n[1] 1913.417\n\n1915 + 7/12\n\n[1] 1915.583\n\n\nHe thinks seconds equals \\(min * 60 + seconds\\). Let us check the first two cases.\n\n4 * 60 + 14.4\n\n[1] 254.4\n\n4 * 60 + 12.6\n\n[1] 252.6\n\n\nCorrect.\nLet us plot data using base R.\n\nplot(x = mile$year,\n     y = mile$seconds,\n     xlab = \"Year\",\n     ylab = \"Seconds\",\n     main = \"Mile record times\")\n\n\n\n\n\nMile record times\n\n\n\nLet us plot data using ggplot2.\n\nggplot(data = mile,\n       mapping = aes(x = year,\n                     y = seconds)) +\n  geom_point() +\n  labs(x = \"Year\",\n       y = \"Seconds\",\n       title = \"Mile record times\")\n\n\n\n\n\nMile record times\n\n\n\nEstimate the line\n\nfit &lt;- lm(formula = seconds ~ year,\n          data = mile)\nsummary(fit)\n\n\nCall:\nlm(formula = seconds ~ year, data = mile)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6129 -0.7457 -0.1876  0.7079  2.8540 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1006.8760    21.5319   46.76   &lt;2e-16 ***\nyear          -0.3931     0.0110  -35.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.383 on 30 degrees of freedom\nMultiple R-squared:  0.977, Adjusted R-squared:  0.9763 \nF-statistic:  1277 on 1 and 30 DF,  p-value: &lt; 2.2e-16\n\n\n\nfitted_intercept &lt;- fit$coeff[\"(Intercept)\"]\nfitted_slope &lt;- fit$coeff[\"year\"]\n\nggplot (add the estimated regression line).\n\nggplot(data = mile,\n       mapping = aes(x = year,\n                     y = seconds)) +\n  geom_point() +\n  labs(x = \"Year\",\n       y = \"Seconds\",\n       title = \"Mile record times\") +\n  geom_abline(intercept = fitted_intercept,\n              slope = fitted_slope,\n              color = \"red\")\n\n\n\n\nNow Exercise 3.3 Probability distributions.\nMake sure we all get the same numbers and create just 10 random numbers.\n\nset.seed(123456789)\nstandard_normal_10 &lt;- rnorm(n = 10, mean = 0, sd = 1)\nstandard_normal_10\n\n [1]  0.5048723  0.3958758  1.4155378 -0.7223243 -0.6183570 -1.5626204\n [7]  0.1279588 -0.1569521 -1.5153363  1.1616016\n\n\nNow create 1000 random numbers.\n\nstandard_normal_1000 &lt;- rnorm(n = 1000, mean = 0, sd = 1)\nstandard_normal_1000\n\n   [1] -1.061663412  1.052570502 -1.090317123 -0.950436352  0.188894236\n   [6] -1.306912792 -1.092966042  1.167427533  1.198129657 -1.962599513\n  [11]  0.726710608  1.067047285 -1.814585236  0.008796922 -0.326998699\n  [16]  0.578129296  1.219543458 -0.289854421 -1.278031954 -0.486875912\n  [21]  0.166523530 -1.807374582 -0.959307836  1.070328439  0.490747416\n  [26]  0.183632890 -0.531039138  0.742454212 -2.279520776  0.181297942\n  [31]  0.035720085 -0.497962631  0.278575421  0.644924868  1.442743659\n  [36] -0.269228559 -0.227812035 -0.764588649  0.064297676 -0.342314814\n  [41]  0.145374612  0.201355915  1.412424707 -0.778640243  0.776387284\n  [46] -1.766810278 -0.370483045  1.453107848 -2.085832080  1.486204368\n  [51] -0.304737496  0.374573279 -0.181112335 -0.410908190 -0.779801193\n  [56]  0.635655827  0.875235147  1.989753477  0.988853775 -0.477072983\n  [61] -0.483031634  0.478349278  0.847588504  0.819227723 -0.757599229\n  [66] -0.651704273 -0.819780063  0.314466775 -0.642793191  0.462462467\n  [71]  1.228671825  0.768043545  1.822470356 -0.015831647  1.193765090\n  [76]  1.174421433 -0.933883537  0.045310865  0.586513664 -0.212695193\n  [81]  0.635938151  0.466727204 -0.805346886  1.128882229 -0.752167305\n  [86]  0.405607127  0.718402581  1.311548133 -1.472449144  0.263474358\n  [91]  0.476863064  0.093451594  1.079127690  0.115624208  0.881592204\n  [96] -0.717208531  0.379197599  0.810392054 -0.022602077 -0.393734353\n [101]  0.890440211  0.218751754  1.819710000 -0.113118367 -1.376020109\n [106] -0.687003544  0.404233603  0.473349427 -1.472634623  0.255850678\n [111]  1.636241747  0.776948518  0.452961045  0.239798184  1.064148860\n [116] -1.157485450 -1.325870918 -0.556262305  0.695556913  1.849128706\n [121] -0.546115504  1.199722352 -0.373461962  1.695054564  0.345647018\n [126] -0.300373425 -0.758852355 -0.439536762 -0.911627889 -1.062711164\n [131]  0.583977566  2.254306580  0.892527745  0.330880226  1.047716407\n [136]  1.800470621 -0.077667469  0.953396171 -0.362331474 -0.572677198\n [141] -0.115801010  0.578694219  1.252305478  0.062864303 -0.500846468\n [146]  0.301033028 -0.091441959 -1.021024978  1.532396390 -1.614638791\n [151]  0.124376539  1.516339118 -0.637421537 -1.460134494 -0.104171149\n [156]  0.283330247  0.519644134 -0.389178901 -1.467731996  0.752390305\n [161]  0.684931478  0.736974346  1.206144962  0.646814918  0.386761747\n [166] -0.473459072 -0.916432260  2.014166539 -0.514303795  1.231067547\n [171] -1.095055517  0.232072469 -0.114339641  0.986292747  1.527863140\n [176]  2.014417439  1.296457948  1.176163496 -1.225085267  0.598638538\n [181] -0.860532922 -1.115510398  0.986917934  1.046605040  1.064473124\n [186] -0.034352015 -1.267312770  1.458750000 -0.060433145 -2.170809880\n [191] -0.143198447  1.523751904 -1.153228885  0.287193566  2.187552388\n [196] -0.871026690  0.366228076 -1.792825981  0.260461287 -0.948887018\n [201] -1.264107233 -0.746167040  1.184711617 -0.661406340  0.467005725\n [206]  0.103082290  1.316218759 -0.497935393  1.138849799 -1.171413659\n [211]  1.117012728  0.637569129 -0.791989257  1.261678127  0.858112119\n [216] -1.592634470 -1.091449434 -0.796549497  0.085050778 -0.730252490\n [221] -1.602584244 -0.901320612 -0.886749798  1.151294724  0.309119664\n [226]  2.038658729  1.211958055  3.474272822  1.178823234 -0.230296119\n [231]  1.009140512 -1.273829957 -1.021244836 -1.492451022  1.706622490\n [236]  0.308095833  0.487263757  0.110756620 -1.644784500 -0.136517072\n [241]  0.032738565 -2.191747811 -0.669109526 -1.754765985  1.611450969\n [246]  0.895454818  0.553153680 -0.087780126  0.855040077  0.936633200\n [251]  0.699012460 -0.595959971 -0.659724284 -0.944726851 -1.690323694\n [256] -0.646178087  1.942268568 -1.468156945 -1.382826339 -0.024835730\n [261] -1.861458816  1.627128509 -0.967444398  0.598099381 -0.781000166\n [266]  1.389464966  0.740786881 -1.110800078 -1.061679873 -1.203555098\n [271] -0.793647940 -2.204102357 -2.064291309 -0.437772936 -0.138580802\n [276]  0.961009454 -0.229892306 -0.301744029  0.048549949 -0.449776280\n [281]  0.169249675  0.675056502 -0.500345601 -0.365878737 -0.242695372\n [286] -3.209428609 -0.910249547  0.410409535 -0.289355259 -0.242433101\n [291] -0.374580838  1.224609382 -0.088864179 -0.657562516 -0.604385696\n [296]  1.624285959 -0.355296373  0.551783490  0.508375879  1.434368913\n [301] -1.544486681  0.092913037  1.043645741 -0.834306330 -0.830830232\n [306]  1.101482704  0.426233880 -1.246908215 -0.020578899  1.770266726\n [311] -1.332695326 -0.263359784 -0.418167033 -1.122167976  1.383596914\n [316] -0.225961154  1.956001955  0.556120791 -0.472229035  0.300668554\n [321] -0.130231331  0.955784876 -1.052625010  0.004550020 -1.393051290\n [326] -0.493776107  1.169367787 -2.114861104 -0.166370681  0.188014598\n [331] -0.688554635 -0.013073488 -0.358876245 -0.686900784  0.157453582\n [336] -1.133225737  0.594070754  0.104203357  0.456306505  1.062001676\n [341]  1.138354085  0.589601469 -1.275931385 -1.535351680 -0.046042782\n [346] -0.475597556 -1.153112495  0.713947427 -0.149839484 -0.271762650\n [351] -0.687745248  0.517564144  0.348138961 -0.679341958 -1.324600825\n [356]  1.397828480  1.346973795 -0.104330915 -2.361519048 -0.175852535\n [361] -1.505001048  0.129477581  1.764175012  1.328798813  0.836866024\n [366] -2.362406109 -1.138177605  0.271132478 -0.093236944  0.051361765\n [371]  1.386927323 -0.566704897  1.153903566 -1.589266147  0.329436651\n [376]  0.088564886  1.875675814  0.077425546 -0.458071276  1.568209844\n [381] -0.486269417  0.504462937  0.217098490 -0.409793474  0.706634773\n [386]  2.195885188  1.132788213  0.504536042 -0.174252770 -0.599933669\n [391] -1.954054940 -0.069901873  1.169662122  2.083517584  0.853412450\n [396] -0.520293201 -1.680256197  0.873476626  1.233214590 -0.193196296\n [401]  0.147949643 -0.398916472  1.778745720  0.086539909 -0.297969568\n [406]  0.116413625  1.899864108  2.483199340 -0.894249076 -0.076765904\n [411] -0.012192129 -0.398980102 -0.459238207 -0.251333763  2.074148220\n [416]  1.569777149  0.854486953  0.334459125  0.679615566 -0.042708895\n [421] -0.203061214  1.214076884 -0.521328638  0.229114549  0.525066822\n [426]  1.188338302  0.427710361 -1.421136379  0.143552343  0.473221986\n [431] -0.198549896  1.970774927  0.295754866 -0.378338481 -1.063870405\n [436]  0.833742686 -0.170188377 -0.875738533 -0.753818508  0.313852108\n [441]  1.191384962 -2.013544749 -0.482764032 -1.018394100  0.883980522\n [446]  0.868265011 -0.716689781 -0.328336168  0.445388131  0.808218401\n [451] -0.245129836 -0.462961003  0.472176237 -1.108926213  0.264350455\n [456]  0.065236046 -0.659740935 -0.834721475 -0.047011623  2.104034142\n [461]  0.552332769  0.575845196  0.572305308 -0.554099128 -0.128096008\n [466]  1.064662013  0.264881005  1.116264740 -0.122539936 -0.588059473\n [471] -0.004782334  0.441794941 -0.847832766  0.461424280  0.440298949\n [476]  0.632027126  0.020824169  0.263561913  0.683976740 -0.464759965\n [481] -0.300120971  0.292900455 -1.241606668  1.105953221 -1.656445633\n [486] -1.849171732 -0.146179655 -0.695640837  3.439082240  1.737941866\n [491]  0.262743745  0.393416276  1.307892651 -0.237314968 -0.346125076\n [496]  0.724831062 -0.401482410 -1.373619084  0.638895460  0.003968125\n [501] -0.674968388  0.193619964  0.177968482  1.247622709  0.489814371\n [506] -0.420219336 -0.692194069  1.093291604  1.255184569  0.564204925\n [511] -1.313939867  0.173787730 -0.929701162 -0.782137726 -1.667410988\n [516]  0.740487733 -1.013003119  0.780451647 -0.589248358 -1.185496824\n [521]  0.851914808  0.967248766 -0.887859965 -0.303154470  1.270233228\n [526]  1.354051183 -1.171188141  1.028680789  0.920419370  0.080897387\n [531]  0.068353979  0.737337395  0.600238792  0.518477067  0.706829703\n [536]  1.136746362  0.566868850  1.365698974 -0.278756451 -0.963526322\n [541] -0.618407835 -1.713982474  0.778309203  0.048128751 -1.647713624\n [546] -0.858599686  1.280295710 -1.312078482  1.064281888 -0.022734798\n [551] -1.516398938  0.160523885  0.568027170  0.548243036 -1.270928598\n [556] -1.134353721 -0.371437680 -0.474229834  0.588971771 -0.154372881\n [561] -0.399462674 -0.885228517  0.245562886  0.044495174 -1.099781368\n [566]  0.105450879 -0.745713326  2.055700616  0.287482763  1.819771770\n [571] -2.053217367 -0.829851580 -0.111969764  0.635369233 -0.796990434\n [576] -0.082430680  1.382544041 -0.544069788 -0.332240758 -0.529927033\n [581]  0.627006126  1.281252408 -0.368337208 -0.111418120 -0.234561670\n [586] -0.957478502 -0.258631603  0.016980225  1.370711391  0.605315194\n [591] -0.249766570  0.574472542 -3.597103912  0.752491668  0.710582823\n [596]  0.678380741  2.427473277 -1.925260520  0.665844476  0.927951182\n [601] -0.154336724 -0.709491830  2.044347694  1.001991508  0.236733242\n [606] -1.366002534  0.671055357 -0.397176994  0.502649789  0.458973865\n [611]  2.465271774 -0.563320742  0.985853967 -1.034794357 -0.506161507\n [616]  0.686855052 -0.621446426 -0.886162258  1.070097794 -1.249397856\n [621]  1.373161924  0.452060231 -0.226413028  1.135791176 -2.599336943\n [626] -0.695679854 -0.524972049 -1.432982612 -0.196297963 -0.197364255\n [631]  0.437270479  0.240184208  1.734828309  1.007838817 -0.840800669\n [636] -0.217787881 -0.806885122 -0.425109014  1.064361312 -0.095524724\n [641] -0.360679543  2.553888936  0.874702052 -0.399647291  1.281519443\n [646]  0.143696017 -0.184071995  0.773536282  1.208995946 -1.443964650\n [651]  1.704565722  0.151092665 -0.386336152  1.700635556 -0.447619380\n [656] -0.802619056  0.478169841 -1.726794357  0.794543822 -1.098598091\n [661] -0.112688081 -0.123328898  0.383079065  1.437280265 -0.827945773\n [666] -0.921464613  1.275686480 -0.609020453  0.567700693 -0.067568694\n [671]  0.263102443 -1.118755278 -0.233364331  0.284440714  0.550787304\n [676] -1.995004217  0.021388669 -1.240658153 -0.367147762 -0.315910150\n [681]  0.012538445  0.261414638 -0.630428323 -1.489224242 -0.920304580\n [686]  0.418211735  1.117202974 -0.256624352 -0.723933594  0.132992540\n [691]  0.455976443 -1.146687546 -0.638927251  0.142464327 -1.352204135\n [696] -0.827456054  0.039485351  0.273559480 -0.260044050 -1.010501636\n [701]  0.464349573 -1.172870547 -0.948163024  1.631448800 -0.906406230\n [706]  1.351258712  1.173865000 -1.408513779  0.247462194 -1.307426095\n [711]  0.078941207  0.461255221  0.785763241  0.009963033 -0.800305541\n [716]  0.658311701  0.392627960 -0.229549160  1.560655175 -0.768765163\n [721]  1.747516949 -0.579797142  0.838998611  0.939737555 -1.393067273\n [726] -0.425247688 -0.527556921 -1.451242034  0.259286789  0.357873412\n [731] -0.436711867  1.990611222 -2.361342303  0.343075849  1.777521472\n [736] -1.112170909 -0.315248954 -1.940446528  0.066176854  1.074680534\n [741]  0.433425561  1.753004402 -0.153075922 -1.082046080 -0.616556844\n [746]  0.197640507  0.249302480 -0.657164791 -0.654614637  1.540667243\n [751] -2.117823346  1.213275331  1.189345512  1.289635262  0.713855407\n [756] -2.177418938 -0.708704134 -0.969202170 -0.560387016  1.780782495\n [761] -0.463928505 -1.705198663  0.913893796 -2.239207954 -0.833030342\n [766] -0.711704149 -0.241475806 -0.058950994  1.881455070 -1.691607804\n [771] -1.456375745  1.007027634  1.258668255  0.271908131 -0.632830111\n [776]  0.363236134 -1.010927191  0.709293300  0.265092917  0.964131471\n [781] -0.652849104 -0.946919982 -0.302566767 -0.092183711  0.036661966\n [786]  0.855724019 -0.306199790 -1.011848609  0.331970316 -1.149080148\n [791]  0.686612922  1.016417193 -1.168403664  0.130488017  0.499820809\n [796]  0.836004098  0.882092423 -0.923788679  0.362713947 -0.986376212\n [801] -0.289023817  1.716984713 -0.604825003 -0.887997589  2.189761180\n [806]  0.616147892 -0.070885258  0.187712877 -0.279178878  0.801337056\n [811] -0.184017677 -0.500316578 -0.232925824  1.130197892 -0.411551252\n [816]  1.449588512 -0.615619485 -0.360117130 -1.396929387 -0.301968789\n [821] -1.023579202  1.490018690  1.451143275 -1.112025404 -2.154004334\n [826]  0.594907350  0.125912332 -1.045329921  0.703605861  1.158327194\n [831] -0.991926505 -0.636717166  0.581674061  0.498512357 -1.291860746\n [836]  1.754029315 -1.983711899 -1.053370289  0.366732548 -0.589422117\n [841]  1.019718375 -1.288139052  0.796488008  0.066149444 -0.364504236\n [846] -0.011913497 -0.719727749  1.345936118 -0.398459506  0.464172459\n [851] -1.587640386  0.554777066  0.161092232 -0.086646320 -0.757580101\n [856] -0.938208598  0.693590238  0.014984160  1.806777756 -0.791618926\n [861]  1.346059179  0.289152891  0.895315132 -1.519466618  0.447917424\n [866] -1.050875322  0.231461815  0.366616181  0.058179416  0.745690251\n [871] -0.321905108  0.532372013  0.789830270  0.722903093  1.170468288\n [876]  2.027930729 -0.557478394 -0.732282306  0.480262119  0.872305792\n [881] -0.989503339  0.335795907  0.598070984  1.674561176  0.461297478\n [886] -0.082137113 -1.591828205 -0.843280800  0.253762075  0.138178447\n [891] -0.407648835 -0.977331578  1.224018910 -0.523250746 -1.176379330\n [896] -0.736417273  0.410979263 -0.208284951 -0.333544957 -0.010397638\n [901] -0.313326946 -1.550235920  1.262284021  0.531112825 -0.508363666\n [906]  0.986317751 -0.458063647 -0.436006681 -0.281568471  0.913795075\n [911] -0.516905798  1.250219043  0.425193200 -0.410191687 -0.018618618\n [916]  0.658877918  1.445628123  1.050116596 -0.491743132  0.040724242\n [921]  0.833149865  2.050862851  0.107636282 -0.713484299 -2.165188362\n [926] -0.907123352 -0.932327085  1.611345287 -1.730749632  0.682894538\n [931] -0.220628460  0.753799190 -0.688570841  2.273481065  0.970725056\n [936] -1.862272422  0.686123002 -2.385840317 -1.562201736 -2.037963829\n [941] -1.009468936 -0.953763956 -1.071801680  0.040896489  0.466888773\n [946] -2.966652069 -0.309560597  0.163401578  1.314137130  0.383647077\n [951] -0.093980638 -0.659186963 -0.328385885  1.525500018  1.099814865\n [956] -0.993356483  0.253701541  0.231036930  1.555578093 -1.063792388\n [961] -2.337635701 -0.988355667 -0.691879012  1.045906070  0.720456196\n [966]  0.508436253 -2.549703896 -0.625790693  0.654047517 -0.275809974\n [971] -1.884396110 -0.541622748 -0.426294977 -1.053806175 -0.340509661\n [976]  1.050333781  1.270531517 -1.248564294  0.301348195 -0.061202647\n [981] -1.690766727  0.183984501  0.471025896 -0.771705189  0.844689989\n [986]  1.013304989 -0.205374923 -1.799614069 -0.376543475 -0.164823241\n [991] -0.612814310 -0.304382116 -0.971282247 -0.466646850  0.690276118\n [996] -0.088841389  0.137389530  1.156245952 -1.748779220  1.088166820\n\n\nhe most basic histogram (single variable distribution) is created with the hist function.\n\nhist(x = standard_normal_1000,\n     breaks = 20,\n     col = \"lightblue\",\n     main = \"Histogram of 1000 standard normal random numbers\",\n     xlab = \"Standard normal random numbers\")\n\n\n\n\nDensity for the standard normal (mean = 0, sd = 1)\n\nx_axis &lt;-\n  seq(min(standard_normal_1000), \n      max(standard_normal_1000), \n      length = 40)\n\ndensity &lt;- dnorm(x_axis, \n                 mean = mean(standard_normal_1000), \n                 sd = sd(standard_normal_1000))\n\nBy assumption (theoretical distibution has mean 0.00000 and sd 0.00000).\n\ndensity_theoretical &lt;- dnorm(x_axis, mean = 0, sd = 1)\n\nplot(x = x_axis, y = density, \n     type = \"l\",\n     col = \"red\",\n     lwd = 2,\n     ylab = \"Density\",\n     main = \"Density of standard normal distribution\")\n\n\n\n\nCreating 1000 random numbers\n\nsd2_normal_1000 &lt;- rnorm(n = 1000, mean = 0, sd = 2)\nsd2_normal_1000\n\n   [1] -0.805535535 -2.470653664  1.463633275  1.157352600  2.537858348\n   [6]  0.444059159 -2.698281064 -2.076201264  2.511087440  0.558001105\n  [11] -1.793074012  1.446574024  1.454320627 -3.553532326 -2.809216317\n  [16] -1.634807822 -0.063999002  1.334217196  2.391584516  3.320468840\n  [21]  1.177130052 -2.519482326  2.492598284  0.727747067  1.672149074\n  [26] -0.090603978  3.973857933  0.652545798  2.964922417 -0.888716450\n  [31] -0.380968986 -0.566561328 -5.311837711  0.042149203  1.098434138\n  [36]  3.341083484  2.071248422 -2.115238486  0.635372959  2.046447650\n  [41] -0.832459822  0.554883391 -0.739068659  3.173172438 -1.141539330\n  [46]  4.167914353 -2.134196001 -1.382608666  2.649210393  0.604698498\n  [51] -2.522584462 -0.204609355  0.050878444  3.206902627 -1.026103627\n  [56]  0.558484999 -3.085526097  0.818560979  4.034049720 -4.748784822\n  [61]  0.545536425  2.017444580  1.376727867  1.895787438  2.731876016\n  [66] -4.307090675  1.809024356 -1.430342343 -1.857736118 -0.372438061\n  [71] -2.531176309 -3.299196702  0.828854250 -3.238836387 -1.664569057\n  [76]  1.316157233 -2.707624553 -1.416067097 -1.100911682 -1.272749175\n  [81] -0.328514436 -0.274600978 -0.854907645  0.151309519 -1.832957561\n  [86]  0.944395254 -2.372203347 -2.201937668 -2.960786834  4.273969967\n  [91]  0.454057772  5.230495634 -1.796191993  1.496818946  0.970732400\n  [96] -3.342906981 -0.680294659  2.005707352 -0.351813586  0.863306288\n [101]  2.406876705 -1.718899618  3.060860586 -1.618351598 -1.117695482\n [106]  0.754765472 -1.298933213  1.342846689  1.926086121  3.237975716\n [111]  5.475666792  2.126579457  2.627201116  0.118959010 -2.339279576\n [116]  2.509248734  0.893310454  0.058319037 -0.972556770 -0.923787211\n [121] -2.817874822 -4.105928151 -1.701231005 -1.574055428 -0.004620614\n [126]  1.298286664 -0.896420482 -1.397186516  0.381827201  0.616862080\n [131]  0.645402392 -1.184598629  0.494789981 -1.846165549 -2.318228203\n [136] -2.345395143 -0.304069739 -0.036951573  2.791479292 -2.524186999\n [141]  4.437951265  1.302149401 -0.930335858 -1.569516711  1.043261403\n [146] -1.866329176 -4.888744330 -3.509724463  0.023586067  1.189160667\n [151]  3.363289941  1.056928770  1.874451325 -2.946278795 -0.123469429\n [156] -4.708799126  2.760169269 -1.071708780  0.781876354  1.810121281\n [161] -0.098268174  0.730575785  2.330467610  0.297380907  2.175841626\n [166] -0.961934755  2.426957948 -2.543578401  4.230768317 -0.587654521\n [171] -0.454950073 -3.532875220 -3.710667454 -1.690924358 -0.683535297\n [176]  1.819372278 -2.384667455  2.551939581 -0.787281121  0.463737301\n [181] -0.188490745 -3.383111471 -0.352577498 -0.680022452 -4.568619691\n [186]  4.081703806  1.657204159  1.957626322  0.504377587  2.170399071\n [191] -2.862373458 -2.404077771 -1.279544241  1.758045609  0.062893567\n [196] -2.689739814 -0.727100432 -3.374278456  0.702240106  0.729365881\n [201] -1.255484833  1.907340630 -0.823444532 -1.668059050  0.375594048\n [206]  0.259118290 -2.026439336 -0.660554414 -0.779850720 -0.776275768\n [211] -1.437490289  1.478075666 -1.249453088 -0.771332804  1.460328112\n [216]  2.318357146  1.345128574 -0.667274843  1.487596593  0.694527088\n [221]  3.573729084 -1.210797275  0.560512867 -2.469800428 -0.594442589\n [226]  1.143988787 -0.027158871  0.769370032 -0.637396010 -0.892342272\n [231]  0.157950160 -3.070163603 -1.329621680  2.723556016  1.492266185\n [236]  0.384141823 -1.038783498 -1.189064042 -0.633848852 -1.170560809\n [241] -1.014204394 -3.271385662 -0.305599745 -4.608972333  1.197969670\n [246] -0.324711656 -0.737323602 -2.886546834 -4.574729679 -3.446418556\n [251] -2.232540613 -0.032030480 -2.386899032  0.104349734 -2.963409425\n [256]  2.651192478 -0.568814031 -1.112819948  1.204344636  0.356197128\n [261]  0.817141058  0.757148694 -0.919444881 -3.341667987  4.579231045\n [266] -0.175179851  0.381080019  0.480201556  1.748917817  0.784951109\n [271]  1.858617052  0.578360705  1.059301448  0.068876978  3.252680917\n [276] -0.346376302  3.821825268 -1.080094362  1.172659323 -1.224710743\n [281]  0.882301565  0.957048193  1.848400688 -1.407914244 -2.460613961\n [286]  2.068697823 -3.648338344 -0.951702394 -3.040103337 -0.311689939\n [291]  2.134595523 -2.019822582 -0.692931326 -0.385048903  0.149178867\n [296]  0.903408284 -0.251941153  1.092124856  0.438042395 -0.599377101\n [301] -0.054469174 -0.622187975 -4.304748967  1.329997282  1.457387946\n [306]  1.969481777  0.992614177  0.090790649 -1.114740444 -0.359900283\n [311] -1.864196722 -2.674807045  1.921791207 -2.847126902 -0.834545212\n [316] -1.833757152  1.098928116 -0.659619515 -1.182045409 -0.025203060\n [321] -1.713990353  2.222971348 -2.533660285  1.195442052  3.917163400\n [326] -2.261351420  1.674319611  5.144081467 -0.805460628  1.972629766\n [331] -2.980291079  2.593664177 -1.766945545 -1.948707212 -1.207169809\n [336]  1.498574338 -1.240035698 -2.148326171 -1.228417373 -1.563237250\n [341]  2.775455694  1.693776177 -1.432937615 -0.225397363 -5.187247710\n [346] -0.584451441 -0.122186595  1.066647298 -3.184796877  3.325406694\n [351]  0.809861120 -2.622997484  1.670686218 -0.681023265 -1.875077614\n [356] -1.744244772 -0.797749823 -3.499220975  0.583080617  0.768389198\n [361] -1.013585035  0.832055579 -1.921622341 -1.015599156 -2.478835820\n [366]  1.882166599 -4.318436080  0.913840840  1.405166636  3.136701544\n [371] -4.762369533 -1.931292223  2.591708212 -0.283086676 -1.578225786\n [376]  0.173695279 -0.801419197 -0.822329857  0.616449959  1.037006494\n [381]  0.515611072  1.494683054 -1.421626156  0.174341375 -2.241991085\n [386]  0.227076923 -3.218672922 -1.221066075 -0.621761875  4.344055955\n [391] -1.642490393 -2.070151526  2.709319499  1.334979373  0.219441903\n [396]  0.006464551  0.623900747  0.605555476  2.414283173  1.349477736\n [401]  3.661080012  0.109997082 -1.652563735 -1.212979428 -6.355460691\n [406]  1.275324660 -2.310634035 -0.758157216 -1.568659790 -2.020700818\n [411]  0.039181822  0.481822269  0.495822054 -0.005742262  0.870405487\n [416]  2.390003135  0.243052422 -1.175709188  0.658329796  0.955799710\n [421]  1.461178381 -0.747042592  1.295949476 -4.440949810  4.037876213\n [426]  2.349762051 -1.176526306  0.909108734 -1.592807825  3.028703496\n [431]  1.083244834 -1.144851753  0.952586655  3.322007353 -1.336616613\n [436]  1.281287664  2.404578437  2.744901517  1.179803546  0.502251076\n [441]  1.491685787 -0.104739095  2.224476777 -3.529141550  1.149888693\n [446]  1.307555806  1.023593020  3.898587647  0.681769010 -1.369798814\n [451] -0.037362627  3.300188826  1.419107910 -4.050179243  0.058031608\n [456]  1.858255868 -3.024600321  2.082104814 -3.180907210 -0.822109997\n [461]  2.494585956  0.151118613 -2.927095884  0.027830571 -2.068958048\n [466] -0.469633529  0.334131820  1.003360929  1.975914365 -0.212402328\n [471]  2.558351278  0.160839816  0.685339832  2.018855663  1.257676758\n [476]  2.169306991  0.801417669  3.171043953  1.840491479  0.923644842\n [481]  1.771587459 -1.765809425  1.582972138  1.421723696 -0.150122135\n [486] -0.505983760 -0.844799752 -1.381478961  0.296039304  0.322699612\n [491] -0.977116857 -0.790787273 -0.647183140 -0.995917214  0.694741307\n [496]  2.910185642 -3.368174753 -5.290754841  3.435493608 -1.101624010\n [501] -0.528316485 -0.132415768  2.328741399 -0.657797943  2.718855370\n [506]  1.372219089  1.111028296 -0.873034298 -3.567380428 -1.393160771\n [511]  2.937375700  3.882797245  0.870113251 -2.430256351  0.507850592\n [516]  1.572298491 -0.817760213  0.221309834 -1.371898045 -0.296434020\n [521] -0.149089729 -1.692419141 -0.299983428  0.951306475  0.171622958\n [526]  4.491547604 -0.150382115  0.289210221 -1.696673309  2.573212602\n [531] -1.528130763  0.382153221 -1.848769071  0.051897674  2.616104452\n [536]  1.287661005  0.470211812 -1.672938280 -0.006579393  1.247330869\n [541] -1.425251238  0.263327101  1.368447843  0.935245028 -1.129894224\n [546] -1.529481753 -3.431243515  0.361147573 -0.776446795  0.247555281\n [551] -2.338870853  0.467602027  2.762276464  4.198637419  0.100890912\n [556]  0.718894991  0.804756218 -1.838111708  2.898307555  0.059065924\n [561] -1.127232075  3.972926087  3.168110233  2.479023998 -3.469565768\n [566] -0.834672862 -1.088081489 -0.140917230  2.053553042  0.356887044\n [571] -0.585600477  0.028040290 -0.733541223 -1.498438721  0.561214380\n [576]  0.301077447  3.282449530  0.680394472 -3.790049792  2.574229427\n [581] -0.571488097  0.687719756 -2.205994424  2.044236660 -0.360011770\n [586] -1.325548776  7.318332969 -2.723137197  3.427021142  3.699394734\n [591]  4.265665974 -0.175937657 -2.956334232  0.292679509  3.169958795\n [596]  2.149900183 -1.354792527 -1.142408561  1.812910003  0.527985295\n [601] -2.753730994 -0.120785903 -1.173106566  1.161374030 -3.562383417\n [606]  1.990870697 -0.363423274  0.594704402 -1.386223011 -1.946825318\n [611]  1.524879343 -1.900466722  2.941634742 -1.106424680 -2.665025348\n [616]  0.124182045 -2.297900885 -1.646414522 -0.470572969 -0.701512109\n [621]  1.115578918 -1.603994220 -1.439237975 -0.322906419  2.098614589\n [626] -3.735937988 -2.161299156 -0.737831376  3.337499567  2.030265594\n [631]  1.671712090 -3.241658722  3.916987200  2.468111241 -1.155519355\n [636] -0.864832939  2.282547587  0.921894457 -0.425817034  0.531721274\n [641] -1.920000950 -2.753559867  1.007581408 -0.469487235 -2.766152330\n [646]  1.161533639 -1.619061241  3.191071832 -1.267702118  1.261624884\n [651] -2.485300984  3.676584835  1.442681376  4.228335466 -2.565670196\n [656] -0.432796152  0.552307090  1.326084832  1.569648320  1.465337508\n [661] -1.691256264 -0.202747241  3.733266052 -0.259884834 -1.751936625\n [666] -2.852230624 -3.806217242  1.478477383  0.643200385  1.063602328\n [671]  2.782614816  1.760365726  0.393688181 -2.260348279 -1.454927448\n [676]  5.736566838 -2.943965529  3.221087511  0.316999163  3.910373691\n [681] -0.900343347  0.924624834 -0.991183231  1.486200848  2.054296096\n [686]  1.654476304  0.802019601  2.302285055 -1.946501709  3.082148201\n [691] -0.068073222  3.463226182  0.021979271  1.145548286  3.480070978\n [696]  1.426358322 -0.059692805 -1.313094558 -0.247968891  1.903923405\n [701]  1.000562233 -2.322068494 -1.566494140  0.387140547 -0.172732613\n [706]  0.043133160 -3.641148578 -3.095773369  1.447760628  2.416189618\n [711] -1.813790173  0.182824573 -2.633320730  5.063179516  0.035128706\n [716]  3.635256687 -4.125815962  0.112115066 -1.325023887 -0.605023041\n [721] -3.400748857 -0.487534747 -0.251389128 -1.058684439  2.527215037\n [726] -0.209299496  2.065797647  1.158619707  0.979095249  0.326293718\n [731] -0.692619532 -1.461819513 -1.625709136  0.203300902 -0.224659427\n [736]  2.109984652 -1.368063714  2.515184023  0.155704264  2.829460796\n [741] -0.554374745  0.633237860 -0.825516573 -1.524065160 -1.333763123\n [746]  1.799732829 -0.376235538 -0.008361295  2.001515084 -0.040880906\n [751]  4.861890610  1.814449685  0.296990553  1.809077903  2.627251102\n [756]  1.584323123  0.538055807 -1.592634900  2.551368273  1.400144361\n [761] -1.069393880  2.248289930  0.042145065  1.451008426 -2.026966722\n [766]  5.684383758 -1.324255229  2.046237672 -1.335262255  0.112918059\n [771]  4.322460355  2.076133627 -3.450429565 -1.766631571 -0.284352923\n [776]  3.260119194  1.064752421  1.977071253 -1.150037444  3.091329098\n [781]  3.281667541  2.416849222 -0.827404505  0.486540734  0.287693422\n [786] -1.044939202  1.833357329  1.082061035  0.286604198  0.197369430\n [791] -0.556207035 -2.949837892  2.446451678  0.249488736  3.129267288\n [796]  0.667131112  2.406279877  0.262643248 -1.228317762  0.415323618\n [801] -0.774766122 -2.087390879 -2.960615223 -4.979187478  1.354703459\n [806] -1.626998742  3.505530354  0.515266397  1.293727668 -1.606565345\n [811] -1.926217538  0.185186340 -0.318530052  0.773636013 -0.977508770\n [816]  1.132634171  1.271720553 -2.104816770 -1.175209326 -1.438639728\n [821] -4.547817522 -2.772452359 -4.223988469  2.332908899 -3.084216687\n [826]  2.096650679 -1.022734202 -1.040697883  0.600696983  0.195408896\n [831] -1.330653893  0.025033017  0.047856770 -2.052780105 -1.255814332\n [836] -1.599274443  1.001252641  2.223273661  0.168238674 -2.670740686\n [841] -3.655143261 -0.279084113  0.694553955  1.867303296 -4.320610448\n [846] -1.118036741 -1.027051493  1.252293865  1.245389914  2.486548964\n [851]  0.982562809 -2.881469020  1.503867495  0.297844280  1.373660423\n [856] -0.107135604 -0.904836081 -3.301635375 -2.574473167 -1.008483156\n [861] -0.456191906  3.009423335 -2.893789577  1.304347514  1.501944529\n [866]  0.720550021  0.825245953  0.557756452 -0.632119849  0.619865681\n [871]  3.238962321 -2.169005017  0.586082779  0.803746619 -1.501403543\n [876] -3.591333418 -3.643934375  0.737344628 -3.309568839 -2.092137126\n [881]  3.599142989 -1.019759356 -0.137673265  0.512457124 -1.376401673\n [886] -2.015182041 -1.433688017 -0.751323016 -3.264995030 -0.197663390\n [891]  2.916065377 -2.820506485 -0.341937173  1.447079000 -1.036776061\n [896] -0.326333690 -2.994543179 -0.896737854 -1.952298898  1.541150988\n [901]  0.727408000 -2.430644876  0.025320669 -0.094429190 -3.605139534\n [906] -1.805932551  2.315234557 -0.462093640  2.646937989  0.255509320\n [911]  0.824244333  1.603310369 -0.976424810 -1.216080669 -1.226961505\n [916] -1.429644118  1.329876478 -2.604959983  1.497696923  0.195146922\n [921] -1.852292544 -1.937666405  2.591516972  1.854583333 -1.296704420\n [926]  0.528411342  0.751789588 -3.103031494  1.642934302  2.820609732\n [931] -0.110627953  2.227752091 -0.397810959  2.459821442  1.056074530\n [936]  1.633635740  0.771495152  0.981929063 -0.622982663  0.319695829\n [941] -2.518042568  4.972810571 -0.375623506 -1.564386468 -0.910232326\n [946] -0.515514486  0.027602652  2.064451075  0.879104462  1.654418415\n [951] -0.194344037 -1.764194542 -2.078217498  0.473487343  1.837236802\n [956]  2.923759255 -0.755409999  2.632074685 -0.938044362  0.521710226\n [961] -2.659739138  2.252766327  1.145604395  0.548929624  1.273072896\n [966] -0.334890208 -5.297369554  0.981329338 -0.404384768  2.142651355\n [971] -0.772191995 -3.466564728 -2.982068767 -3.733874568 -3.142587305\n [976]  0.439655839 -3.075927735 -2.637939685  1.488745881  1.455846691\n [981]  2.857304777 -1.175035987  0.210858137 -0.877615463  3.705199185\n [986] -1.832992903 -1.593362264 -4.133311058  0.957242425 -3.395637917\n [991]  1.212445713 -0.745039171  0.544802007 -2.579228454  4.745667771\n [996]  1.626899967 -0.605120196  1.379424547 -0.269868619 -1.543155808\n\nx_axis &lt;-\n  seq(min(sd2_normal_1000), \n      max(sd2_normal_1000), \n      length = 40)\n\ndensity &lt;- dnorm(x_axis, mean = mean(sd2_normal_1000), \n                 sd = sd(sd2_normal_1000))\n\n\nplot(x = x_axis, y = density, type = \"l\",\n     col = \"red\", \n     lwd = 2,\n     ylab = \"Density\",\n     main = \"Density of standard normal distribution\")\n\n\n\n\nExercise 3.4.\n\npoisson_35_1000 &lt;- \n  rpois(n = 1000, lambda = 3.5)\n\npoisson_35_1000 |&gt; hist(breaks = length(unique(poisson_35_1000)))\n\n\n\n\n\nPoisson distribution with lambda = 3.5\n\n\n\nExcercise 3.5 Binomial distribution.\n\nbinom_03_20_1000 &lt;- \n  rbinom(n = 1000, size = 20, prob = 0.3)\n\nbinom_03_20_1000 |&gt; hist(breaks = length(unique(binom_03_20_1000)))\n\n\n\n\n\nBinomial distribution with p = 0.3, n = 20\n\n\n\nExercise 3.6 Linear transformations.\nThe mean must be increased from 35 to 100 by adding 65, the standard deviation must be increased from 10 to 15 by multiplying with 1.5.\nTransformation \\(X' = aX + b\\) \\(mean(X') = a * mean(X) + b\\) \\(sd(X') = a * sd(X)\\)\nNow mean(X’) = 100, mean(X) = 35, sd(X’) = 15, sd(X) = 10\nSubstituting this: \\(100 = a * 35 + b\\) \\(15 = a * 10\\)\n\\(100 = 15/10 * 35 + b\\) $ b = 47.5$ $ a = (100 - b) / 35 = 1.5$\ntransformation: \\(X` = 1.5 * X + 47.5\\)\n\noriginal_scores = rnorm(n = 1000, mean = 35, sd = 10)\noriginal_scores |&gt; hist(breaks = 10)\n\n\n\ntransformed_scores = 1.5 * original_scores + 47.5\ntransformed_scores |&gt; hist(breaks = 10)\n\n\n\n\nNew range:\nLowest (X=0) is \\(0 * 1.5 + 47.5 = 47.5\\)\nHighest (X=50) is \\(50 * 1.5 + 47.5 = 122.5\\)\nSimple. First multiply to get the standard deviation right.\n\ntransformed_1 &lt;- original_scores * 1.5\n# Check that is ~15 now\nsd(transformed_1)\n\n[1] 14.82577\n\n\nWhat is the mean after the first transformation?\n\nmean(transformed_1)\n\n[1] 52.33731\n\n\nAdd the difference between the target mean (100) and the current mean\n\ntransformed_2 &lt;- transformed_1 + (100 - mean(transformed_1))\ntransformed_2 |&gt; hist(breaks = 10)\n\n\n\n\n\nplot(original_scores, transformed_scores, type = \"l\")\n\n\n\n\n\nOriginal scores vs transformed scores\n\n\n\nExercise 3.8 Correlated random variables\n\ncorrelation_hw = .3\nmean_husbands = 69.1\nsd_husbands = 2.9\nmean_wives = 63.7\nsd_wives = 2.7\n\nweighted sum:\n\n.5 * mean_husbands + .5 * mean_wives\n\n[1] 66.4\n\n\nStandard deviation of .5 * husband + .5 wife\n\nsqrt(.5^2 * sd_husbands + \n       .5^2 * sd_wives + \n       2 * .5 * .5 * correlation_hw)\n\n[1] 1.24499"
  },
  {
    "objectID": "04-chapter.html#summary",
    "href": "04-chapter.html#summary",
    "title": "4  Statistical Inference",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nStatistical inference can be formulated as a set of operation on data that yield estimates and uncertainty statements about predictions and parameters of some underlying process of population. From a mathematical standpoint, these probabilistic uncertainty statements are derived based on some assumed probability model for observed data.\nIn this chapter:\n- the basics of probability models are sketched (estimation, bias, and variance);\n- the interpretation of statistical inferences and statistical errors in applied work;\n- the theme of uncertainty in statistical inference is introduced;\n- a mistake to use hypothesis tests or statistical significance to attribute certainty from noisy data are discussed.\nStatistical inference is used to learn from incomplete or imperfect data.\n- In the sampling model we are for example interested in learning some characteristics of a population from a sample.\n- In the measurement model we are interested in learning about the underlying pattern or law.\n- Model error refers to the inevitable imperferction of the model.\n\nSome definitions are given. The sampling distribution is the set of possible datasets that could have been observed if the data collection process had been re-done, along with the probabilities of these possible values. It is said to be a generative model in that it represents a random process which, if known, could generate a new dataset. Parameters are the unknown numbers that determine a statistical model, e.g. \\(y_i=a+bx_i+\\epsilon_i\\) in which the errors \\(\\epsilon_I\\) are normally distributed with mean 0 and standard deviation \\(\\sigma\\). Thre parameters \\(a\\) and \\(b\\) are called coeffients and \\(\\sigma\\) is a scale or variance parameter.\nThe standard error (\\(\\sigma/ \\sqrt{n}\\)) is the estimated standard deviation of an estimate and can give us a sense of our uncertainty about the quantity of interest. The confidence interval represents a range of values of a parameter or quantity of that are roughly consistent with the data, given the assumed sampling distribution.\n\n\n\n\n\nBias and unmodeled uncertainty are also discussed. Roughly speaking, an estimate is unbiased if it is correct on average. Take into account that random samples and randomized experiments are imperfect in reality, and any approximations become even more tenuous when applied to observational data. Also, survey respondents are not balles drawn from an ure, and the probabilties in the “urn” are changing over time. So, improve data collection, expand the model, and increase stated uncertainty.\nPerforming data analysis is the possibility of mistakenly coming to strong conclusions that do not reflect real patterns in the underlying population. Statistical theories of hypothesis testing and error analysis have been developed to quantify these possibilities in the context of inference and decision making.\nA commonly used decision rule that we do not recommend is to consider a result as stable or real if it is “statistically significant” and to taken “non-statistically” results to be noisy and to be treated with skepticism. The concepts of hypothesis testing are reviewed with a simple hypothetical example. Estimate, standard error, degrees of freedom, null and alternative hypotheses and p-value, as well as the general formulation, confidence intervals to compare results, and Type 1 and Type 2-errors, important in conventional hypthesis testing, are presented.\nThey present the problems with the concept of statistical significance (some examples are given):\n\nStatistical significance is not the same as practical significance;\n\nNon-significance is not the same as zero;\n\nThe difference between “significant” and “non-significant” is not itself statistically significant;\n\nStatistical significance can be attained by multiple comparisons or multiple potential comparisons;\n\nThe statistical significant estimates tend to be overestimated;\n\n\nIn this book they try to move beyond hypothesis testing. The most important aspect of their statistical method is its ability to incorporate more information into the analysis. General rules are:\n- Analyse all your data;\n- Present all your comparisons;\n- Make your data public.\nBayesian methods can reduce now-common pattern of the researchers getting jerked around by noise patterns that happen to exceed the statistical significance threshold. We can move forward by accepting uncertainty and embracing variation."
  },
  {
    "objectID": "04-chapter.html#presentation",
    "href": "04-chapter.html#presentation",
    "title": "4  Statistical Inference",
    "section": "4.2 Presentation",
    "text": "4.2 Presentation\n\n4.2.1 1. ESTIMATION\nSet the true data-generating parameters (e.g mean 175 cm and sd=6 cm). Simulate 1000 random samples from the sampling distribution. The estimate of the mean is 175/1000=0.175 and the standard error is \\(\\sqrt{0.175(1-0.175)/1000}\\).\n\nlibrary(tidyverse)\n\nn&lt;-1000\nmu&lt;-175\nsigma&lt;-6\n\nestimate&lt;-mu/n\nse&lt;-sqrt(estimate*(1-estimate)/n)\n\nestimate\n\n[1] 0.175\n\nse\n\n[1] 0.01201561\n\n\nThis is figure 4.2 on page 52 of the ROS-book, (but for persons with \\(\\mu\\)=175 and \\(\\sigma\\)=6 and CI=95%).\n\n\n\n\n\nLet us look at different estimations using different distributions\nNormal distribution\nThe Normal distribution, frequently encountered in real-world scenarios like IQ scores or heights in a population, exemplifies a symmetric bell-shaped curve. It symbolizes situations where most observations cluster around a central mean, with fewer occurrences as we move away from the center.\nLet us generate a normal distribution in R\n\nset.seed(4)\n\nn&lt;-1000\nmu&lt;-175\nsd&lt;-6\n\nsamplenorm &lt;- rnorm(n=n, mean = mu, sd = sd)\nprint(samplenorm)\n\n   [1] 176.3005 171.7450 180.3469 178.5759 184.8137 179.1357 167.3125 173.7211\n   [9] 186.3792 185.6612 178.3996 175.0943 177.2983 174.7292 175.2061 176.0142\n  [17] 181.9902 174.7348 174.3978 173.2993 184.2449 175.9910 182.8457 182.7295\n  [25] 178.5574 173.3023 182.5353 180.4590 169.4318 182.4411 175.9208 181.3116\n  [33] 170.4747 166.1069 180.1668 172.5729 173.6356 180.6046 172.2046 171.1747\n  [41] 183.0623 176.0892 182.7551 164.8717 170.0740 169.8271 175.5931 172.7461\n  [49] 179.3434 164.2157 171.0175 171.2576 174.5222 177.6137 186.8254 171.4194\n  [57] 171.6850 179.1758 174.0660 183.0934 168.5889 181.3867 167.1237 187.3822\n  [65] 175.7883 173.6099 172.6159 180.3366 178.1570 173.9724 175.9521 172.0860\n  [73] 169.2466 176.0831 179.3304 172.7828 176.4252 171.0045 170.2192 174.6898\n  [81] 182.7216 173.7151 171.5515 166.1756 168.8036 167.1609 169.9705 168.2161\n  [89] 177.2125 173.7892 167.3340 170.2119 175.9545 178.6888 179.1277 174.7177\n  [97] 188.9819 171.5346 180.8109 173.3348 179.1088 174.3093 172.8611 174.3654\n [105] 175.2693 164.6430 184.3347 179.6585 168.4090 164.6319 177.5658 179.4674\n [113] 180.1913 176.8320 174.3159 177.5419 170.2137 171.3748 185.2901 170.7043\n [121] 174.2006 169.0014 186.2426 172.9757 180.8396 180.9270 169.3525 177.0951\n [129] 171.4335 160.7065 181.4681 179.0095 169.2122 163.1486 171.4914 180.8157\n [137] 178.3138 174.5071 164.9397 182.2756 181.0030 179.3160 169.9338 178.7319\n [145] 170.6643 172.3031 167.8270 177.3428 171.9017 180.4592 180.2619 170.1028\n [153] 184.2358 183.2472 172.1005 178.3021 169.8558 170.7582 162.4175 181.5966\n [161] 177.0522 177.9450 169.4080 166.4326 180.8546 165.7220 175.1062 170.3517\n [169] 173.6239 173.3537 185.7764 172.1313 171.4314 161.4524 185.0956 175.4337\n [177] 172.3599 178.7594 170.2012 168.2321 168.8499 175.4264 177.2903 165.2645\n [185] 186.4033 170.7029 177.2828 177.6451 176.5440 173.9233 170.8592 174.9975\n [193] 178.3935 167.7475 172.9230 171.0988 169.6624 183.8622 167.8271 185.5030\n [201] 182.2884 165.7132 173.1865 181.2352 170.3929 184.1480 160.4675 178.3380\n [209] 181.6332 175.9986 173.6472 173.6295 173.4809 187.4096 184.4991 168.7445\n [217] 174.9497 166.9955 175.8792 170.2731 173.2660 179.0010 174.1802 176.3445\n [225] 181.7032 166.3690 178.8062 172.0048 181.7576 174.3989 168.3409 178.9475\n [233] 174.7402 178.7703 169.3790 172.8557 174.7296 172.9117 172.8462 177.3737\n [241] 169.0963 174.8638 169.6927 177.9102 170.0108 182.0389 175.3661 175.9405\n [249] 177.8134 178.2710 177.3736 169.4718 170.9406 163.3366 175.7199 182.5965\n [257] 168.4121 177.6274 178.6984 175.1072 167.5649 171.9747 182.9007 179.7666\n [265] 177.9102 189.2262 173.3098 176.1940 179.8614 174.2664 181.0463 178.1055\n [273] 173.6202 173.9942 160.1438 171.3931 175.1891 182.3567 170.8047 165.9751\n [281] 173.6489 174.9464 166.1298 163.9135 169.9238 182.5345 166.8970 176.9429\n [289] 175.7128 178.8977 179.0897 190.1051 176.8151 171.0171 175.3777 167.8740\n [297] 172.4770 170.5143 174.2420 168.8880 157.9625 184.0688 183.6541 178.0322\n [305] 170.4562 176.4954 173.0989 175.6175 173.7303 176.6233 173.1580 180.3392\n [313] 177.3283 161.3537 171.7003 176.9823 171.4111 167.4277 179.5798 180.0458\n [321] 175.9494 181.2790 171.0829 169.2357 173.7931 179.1615 169.0118 169.0022\n [329] 172.8201 181.3460 181.4737 182.4280 180.1247 170.1188 172.7930 181.7061\n [337] 175.6824 172.3328 166.3257 172.6933 178.5588 176.4703 164.5252 175.3745\n [345] 168.4838 174.3549 177.7072 181.5662 180.4358 169.0149 170.7050 177.5356\n [353] 177.5976 175.5072 178.3126 171.7734 175.7545 181.0233 181.4310 172.2979\n [361] 180.7506 180.4259 167.6120 173.2573 165.0873 175.2245 162.7439 175.6676\n [369] 176.8672 171.1666 180.5966 173.3639 182.4510 168.6359 186.3287 167.9914\n [377] 177.5140 165.0851 176.0614 174.1365 189.1762 182.3018 177.4197 178.8258\n [385] 169.6655 167.1088 176.0688 166.4572 165.9779 168.5476 175.8507 176.5732\n [393] 182.1635 164.8786 176.4403 168.2528 169.8988 172.9676 174.6417 174.0767\n [401] 167.9073 177.7376 173.5703 179.8730 170.6161 161.8343 181.0092 182.0043\n [409] 179.6179 184.5279 177.1081 171.7940 173.2907 174.6837 175.3867 171.5280\n [417] 174.9245 180.4340 171.9356 176.2941 171.4973 167.6979 176.0213 175.7865\n [425] 173.5628 171.6011 175.2342 163.5333 174.8496 178.9904 182.9374 194.0451\n [433] 163.7887 166.9112 188.8219 176.5329 180.2099 177.1350 168.1271 176.7753\n [441] 170.8242 182.1101 184.1512 177.5251 184.3550 184.7708 176.0387 172.9651\n [449] 167.3368 175.3625 168.1939 174.6198 170.6045 181.4442 183.8732 159.8019\n [457] 170.0816 176.9132 169.1794 172.6834 174.7823 179.7242 177.5752 178.9389\n [465] 170.5090 175.0712 171.3593 183.4359 171.2194 169.7519 167.8415 177.2060\n [473] 170.7998 172.4473 181.0823 177.5232 182.9519 181.2662 171.0825 165.0413\n [481] 165.9402 176.8773 163.6139 171.3688 172.8938 168.9379 172.3095 180.2082\n [489] 175.4747 171.0006 174.3556 177.8512 169.9789 177.5687 171.1245 178.4599\n [497] 174.4553 173.9630 172.2442 171.5101 165.1319 170.0801 164.9306 177.5450\n [505] 172.9073 174.6860 173.3520 178.0918 167.1669 185.1946 171.9849 173.1706\n [513] 171.0713 176.6411 164.6553 172.0624 174.1834 172.5586 184.3143 180.1923\n [521] 177.0916 167.4493 177.8550 180.2376 171.7385 176.1666 177.3933 175.7831\n [529] 171.4324 179.2104 174.0764 178.5159 182.0067 161.1849 179.9211 179.3327\n [537] 175.5859 184.8261 171.7635 172.2839 167.5466 175.2655 175.0417 169.0385\n [545] 179.2913 178.6017 174.6706 179.1865 179.8870 161.0766 169.6793 178.8225\n [553] 183.9945 165.9929 191.1443 174.8586 173.5576 169.6362 177.7443 172.6035\n [561] 182.0057 169.7596 178.3841 180.6413 169.1942 178.0516 167.4975 175.8997\n [569] 176.8315 173.2165 169.8081 169.4348 188.2816 177.0650 176.6204 168.4245\n [577] 169.8408 169.7689 169.6009 182.9644 178.1600 177.8984 175.9679 173.5079\n [585] 170.2348 185.9665 179.9097 179.3525 173.8722 179.0800 178.0907 174.5562\n [593] 181.7125 179.0411 167.5459 171.6942 179.5166 164.1199 173.2918 172.8374\n [601] 162.0808 171.5635 179.4461 173.6032 166.9186 170.1889 184.6292 181.5929\n [609] 168.9044 175.2881 179.6288 179.3065 178.3843 173.5536 162.6096 182.9504\n [617] 179.4637 173.3033 172.6543 172.5473 172.0080 180.2914 178.4694 175.1730\n [625] 181.3795 173.4530 170.8783 161.5516 175.1817 177.6270 171.9826 177.7249\n [633] 177.2791 168.0624 181.2331 181.9749 172.7856 177.0616 176.0245 179.6633\n [641] 168.0312 176.1685 176.5582 171.9874 175.2606 161.0022 170.1649 179.9476\n [649] 165.4296 183.9768 176.7932 175.3362 174.5279 171.1617 172.9852 175.3590\n [657] 183.1746 172.0421 168.1482 178.0145 176.6084 187.5866 177.3944 184.5429\n [665] 174.3564 179.4428 174.5281 177.0473 163.1721 174.2642 180.4773 177.2657\n [673] 172.3563 184.2765 184.0423 177.3115 177.6150 166.1909 170.9607 168.1038\n [681] 173.2097 176.6253 177.5164 169.5940 179.4821 183.2283 179.6963 159.4081\n [689] 166.3268 175.6284 176.2239 171.2271 168.5785 169.4870 174.1204 179.0990\n [697] 169.2745 173.5953 171.2134 177.4830 178.3669 172.0445 180.5726 169.9651\n [705] 178.8326 173.7490 173.8855 175.0712 164.7177 173.1406 171.1161 171.3302\n [713] 176.7242 176.9159 183.5438 174.2366 179.5104 171.1300 165.9824 172.3503\n [721] 183.0923 181.7491 179.0898 175.8273 173.7617 163.2590 176.8675 185.4290\n [729] 175.6903 189.6953 171.4013 174.5819 179.4579 171.7583 175.9273 170.8340\n [737] 167.2333 169.9068 184.0773 181.0792 180.7404 177.6897 168.9968 178.4046\n [745] 188.0100 170.4205 189.3268 171.6272 179.5782 177.9251 171.6455 177.5705\n [753] 181.2469 180.8483 169.8184 171.9712 179.3183 176.7131 171.4935 178.0538\n [761] 168.9737 172.2706 172.9966 176.4678 167.7677 166.2438 177.2449 175.9971\n [769] 183.3820 168.8774 176.7339 186.7523 175.0972 160.4599 166.0638 171.7933\n [777] 174.4958 176.0125 173.8064 182.4868 177.5966 174.5411 174.1630 173.5527\n [785] 170.2318 180.7200 168.6894 170.2439 189.3804 168.4227 176.9918 166.6709\n [793] 182.2858 168.9626 171.6058 175.3917 169.0998 166.0040 172.4577 168.8744\n [801] 171.2524 180.9639 165.8983 174.9051 167.8020 176.3407 178.1912 173.5877\n [809] 174.5304 174.0143 180.9660 176.4168 171.8161 176.8438 175.7131 184.9183\n [817] 168.4244 167.4362 172.2017 160.7962 173.7030 171.5894 181.0190 187.5406\n [825] 168.5974 172.3370 171.2558 179.8538 178.7709 173.7321 179.8079 165.4156\n [833] 181.3500 173.6950 177.4711 184.8207 174.7219 175.7104 185.0329 173.8536\n [841] 174.1746 173.7351 176.4499 172.4342 175.5271 184.1371 170.4378 175.4352\n [849] 164.0053 167.3978 176.5985 176.9167 171.8656 180.2809 171.6939 172.8589\n [857] 182.3716 171.6387 165.8503 176.5086 182.4137 185.2196 168.5470 178.3297\n [865] 168.3180 172.3714 179.7264 171.2545 171.1418 178.8702 176.0546 176.8902\n [873] 172.7712 165.4273 167.5585 176.4379 190.1518 176.3350 162.5239 184.9793\n [881] 163.7052 174.3882 174.9796 172.4220 175.6522 181.7312 166.9428 175.6826\n [889] 172.8361 168.6664 183.1259 181.6548 168.2531 178.3351 174.2015 167.8605\n [897] 167.5118 180.5481 162.6892 176.4364 169.8607 179.9990 176.8079 175.2372\n [905] 166.4246 178.7328 172.1177 173.0623 165.0378 177.9752 172.6144 182.7204\n [913] 181.4144 179.6116 175.8298 174.4598 177.9538 180.7253 188.9995 175.1861\n [921] 177.2068 168.1731 174.5865 166.8697 174.4562 174.6252 173.4309 168.2558\n [929] 172.7765 165.7470 179.1304 166.6375 180.8101 176.7656 173.4839 181.6822\n [937] 163.8699 184.4192 175.1047 166.1247 182.1621 171.2101 181.3818 165.8083\n [945] 176.5571 168.5627 167.1225 171.4656 169.3424 164.4577 175.6844 173.7918\n [953] 179.8717 180.2524 181.1321 178.8533 171.4996 166.4923 179.9321 164.3387\n [961] 175.6181 176.9516 172.3750 178.4242 182.6580 163.7554 183.3456 173.2007\n [969] 172.4220 160.6177 181.4829 169.8757 176.0406 172.4963 174.4400 173.5338\n [977] 174.0041 169.6055 177.0068 160.3112 169.4219 179.2625 179.9109 185.8882\n [985] 178.5573 171.9915 176.7849 173.5124 177.6660 173.5537 176.7312 177.0606\n [993] 177.4033 175.5103 170.7951 177.3313 182.8744 174.5323 178.5439 180.7461\n\n\nSummarize over sample.\n\nmean(samplenorm)\n\n[1] 174.7934\n\nsd(samplenorm)\n\n[1] 5.815821\n\nconfint(lm(samplenorm~1), level=0.95)\n\n               2.5 %   97.5 %\n(Intercept) 174.4325 175.1543\n\n\nPlot it!\n\nhist(samplenorm, breaks=30, col = \"skyblue\", border = \"black\", main = \"Normal Distribution\", xlab = \"Value\", ylab = \"Frequency\")\n\n\n\n\n\nNormal distribution\n\n\n\nBinomial distributions\nWhen N available things all have the same probability \\(p\\) of being in a certain state (eg. being counted, male or dead)\nGenerating a binomial distribution in R\n\nset.seed(5)\n\nn&lt;-1000.  # sample size\nN&lt;- 16    # numbers of individuals\np&lt;- 0.8   # probability of success (counted, male or)\n\nsamplebinom &lt;- rbinom(n=n, size = N, prob = p)\nprint(samplebinom)\n\n   [1] 14 12 11 14 15 12 13 11 10 15 14 13 14 13 14 14 13 11 13 11 11 12 14 14\n  [25] 15 13 13 10 14 10 13 15 14 15 16 13 13 13 13 13 11 14 11 13 10 13 12 15\n  [49] 12 12 13 13 10  9 10 13 14 14 14 15 12 14 11  9  9 11 11 14 10 11 11 12\n  [73] 14 11 16 10 15 12 13 14 10 11 14 15 13 15 10 12 14 12 12 14 14 13 14 11\n  [97] 14 12 13 13 10 12 13 10 11 13 14 13 13 14 14 13 12 14 13 13 13 15 13 14\n [121] 15  9 11 11 14 15 12 12 14 14 13 14 16 11 13 16 15 13 13 13 13 14 13 11\n [145] 12 12 13 12 12 12 13 12 11 15 13 12 13 14 14 11 12 14 15 11 11 12 13 14\n [169] 12  9 14 10  9 15 14 11 13 10 14 15 11 12 14 13 12 15 11 14 11 14 12 12\n [193] 13 10 14 14 13 12 13 12 16 15 11 12 12 13 13 14 13 14 11 12 13 13 16 14\n [217] 14 15 11 16 14 13 10 14  8 11 13 11 15 14 12 15 15 10 12 14 13 15 13 11\n [241] 11 14 11  8 15 13 15 15 11 14 12 15 14 14 11 15 15 10 15 12 11 13 16 14\n [265] 15 15 13 14 12 13 12 14 12 14 14 15 12 13 12 12 14 14 13 14  9 14 11 11\n [289] 13 11 14 12 13 12 13 13 15 16 14 11 11 14 12 14 14  9 12 15 14 11 11 14\n [313] 13 12 11 13 14 15 13 15 16 12  9 14 13 15 10 13 14 12 14 16 13  9 13 14\n [337] 13 13 14 14 15 14 13 13 15 15 15 13 13 11 12 14 10 13 14 13 11 15 14 15\n [361]  9 12 14 14 12 11 11 10 15 13 13 14 13 12 14 13 12 13 15 11 12 14 12 14\n [385] 14 13 12 10 13 11 11 11 12 11 13 10 14 11 12 14 13 14 13 16 11 14 14 15\n [409] 12 13 12 12 12 15 11 12 13 12 12 12 11 13 12 14 12 14 14 12 12 14 14 12\n [433] 13 12 10 11 12 12 12 14 13 12 13 14  9 15 13 16 15 15 13 10 15 13 14 13\n [457] 14 14 16 15 14 15 11 12 14  9 14 13 14 10 10  9 10 12 14 12 13 13 11 12\n [481] 12 16 13 11 13 13 10 13 15 13 14 13 12 11 13 13 15 10 12 13 13 14 13 12\n [505] 13 16 14 12 15 11 15 15 14 14 14 14 12 15 13 15 15 13 13 12 11 13 14 14\n [529] 13 12  9 11 12 12 14 12 15 12 14 14 13 15 14 11 14 12 10 11 10 11 13 13\n [553] 15 15 13 12 12 13 13 10 13 11 14 13 14  9 11 13 14 11 14 12 10 11 13 12\n [577] 10 14 14 13 13 13 12 14 12 10 13 14 13 12 14 13 11 12 11 12 13 13 12 13\n [601] 11 14 13 10  9 13 13 12 15 12 14 13 11 10 13 13 13 10 12 11 14 15 11 13\n [625] 12 13 15 10 13 11 10 12 11 14 13 14 15 12 12 15 13 12 11 13 12 11 12 14\n [649] 14 16 15 13 12 10 13 15 15 11 14 12 15 13 13 14 13 14 14 13 12 11 14 14\n [673] 15 14 13 12 12 14 12 10 16 13 16 12 15 14 12 11 14 13 14 15 14 14 15 13\n [697] 13 13 11 12 16  9 12 14 11 14 14 10 13 13 12 11 10 15 16 16 13 14 12 16\n [721] 14 12  9 13 12 14 11 13 14 14 11 14 14 10 13 11 12 11 13 13 13 13 11 13\n [745] 15 13 12 11 14 11 11 12 14 12 11 15 12 12 13  9  9 12 11 15 11 13 11 13\n [769] 10  8 10 11 12 13 14 10 14 13 13 15 14 14 13 12 14 13 12 12 12 15 16 14\n [793] 14 12 12 14 15 12 13 12 14 12 12 15 15 13 14 12 12 16 14 15 13 13  9 15\n [817] 14 11 16 10 11 13 14 14 14 11 12 13 13 13  9 12 14 16 13 12 13 13 15 13\n [841] 13 13 10 15 12 11 15 12 13 10 13 11 12 15 15 12 12 14 12 15 14 13 15 13\n [865] 12 15 12 14 10 11 12 12 11 14 10 12 11 12 12 12 14 12 15 13 13 12 11 14\n [889] 12 13 12 13 11 12 13 11 10 11 13 16 12 12 14 11 13 13 13 14 13 11 13 13\n [913] 15 13 12 12 15 15 12 13 14 12 15 11 12 11 15 10  9 14 12 12 11 14 15 15\n [937] 12 14 14 11 11 12 13 14 16 12 13 12 13 12 11 14 13 14 12 11 12 15 12 13\n [961] 13 14 13 11 14 13 13 14 16 13 13 14 14 13 14 11 12 11 12 13 14 12 14 14\n [985] 14 13 14 13 10 10 13 11 15 11 11 12 13 16 16 13\n\n\nSummarize over sample\n\nmean(samplebinom)\n\n[1] 12.784\n\nsd(samplebinom)\n\n[1] 1.610571\n\nconfint(lm(samplebinom~1), level=0.95)\n\n               2.5 %   97.5 %\n(Intercept) 12.68406 12.88394\n\n\nPlot it!\n\nhist(samplebinom, breaks=30, col = \"skyblue\", border = \"black\", main = \"Binomial Distribution\", xlab = \"Value\", ylab = \"Frequency\")\n\n\n\n\n\nBinomial distribution"
  },
  {
    "objectID": "05-chapter.html#summary",
    "href": "05-chapter.html#summary",
    "title": "5  Simulation",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nIn this book Regression and Other Stories and in practice Gelman et all. use simulation for different reasons:\n- use probability models to mimic variation in the world (tools of simulation can help us better to understand how this variation plays out);\n- use simulation to approximate the sampling distribution of data and propagate this to the sampling distribution of statistical estimates and procedures;\n- regression models are not deterministic; they produce probabilisitc predictions. Simulation is the most convenient and general way to represent uncertainties in forecasts.\nBecause of this, chapter 5 introduces simulations (basic ideas and tools to perform it in R).\nThere are many settings where it makes sense to use a set of simulation draws to summarize a distribution, which can represent a simulation from a probability model, a prediction for a future outcome from a fitted regression, or uncertainty about parameters in a fitted model.\nIn this book they use the Bayesian simulation approach for regression models. Bootstrap is also a simulation approach, but it is not as general as Bayesian simulation. It is very general, any estimate can be simulated, and it is easy to use with complex models. But it has limitations (for example leading to an answer with an inappropriately high level of certainty). This method is not used in the book."
  },
  {
    "objectID": "05-chapter.html#presentation",
    "href": "05-chapter.html#presentation",
    "title": "5  Simulation",
    "section": "5.2 Presentation",
    "text": "5.2 Presentation\nDifferent examples of simulations of discrete, continuous, and mixed discrete/ continuous models are presented in this chapter.\nIt starts with a discrete model example: Across the world the probability a baby will be born a girl is about \\(48.8\\%\\), with the probability of a boy then being about \\(51.2\\%\\). If you wanted to get a sense of how many girls you’d expect out of 400 births, you could simulate using the rbinom() function.\n\nset.seed(5)\n# use set seed here because we want to get the same results every time we run the code\n\nrbinom(n = 1, size = 400, prob = 0.488)\n\n[1] 188\n\n\nGraph results of 10000 simulations:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(5)\n# set the global plotting theme\ntheme_set(theme_linedraw() +\n            theme(panel.grid = element_blank()))\n\n# set the seed\nset.seed(5)\n\n# simulate\ntibble(girls = rbinom(n = 10000, size = 400, prob = .488)) %&gt;% \n  \n# plot\nggplot(aes(x = girls)) +\ngeom_histogram(binwidth = 2) +\nscale_x_continuous(\"Number of girls out of 400 single births \\ \n                   in 10.000 observations\", breaks = 7:9 * 25) +\nscale_y_continuous(expand = expansion(mult = c(0, 0.05)))\n\n\n\n\n\nSimulating 400 births\n\n\n\nAlso simulations of other models are presented. For example, the normal, binomial and Poisson distributions are presented.\nDefine mean=3 and sd=0,5\n\nmean = 3\nsd = 0.5\nn_sims = 10000\n\nPlot the results of 10000 simulations of a normal distribution with mean=3 and sd=0,5:\nThen plot the results in histograms:\n\nlibrary(tidyverse)\n\nset.seed(660)\n\nd &lt;- tibble(x = rnorm(n_sims, mean = mean, sd = sd))\n\nd %&gt;% \n  ggplot(aes(x)) +\n  geom_histogram(binwidth = 0.1) +\n  labs(\n    title =\n      str_glue(\n        \"Normal distribution with mean {mean} and standard deviation {sd}\"\n      )\n  )\n\n\n\n\n\nSimulating a normal distribution\n\n\n\nHere data for a binomial distribution:\n\nsize &lt;- 20\nprob &lt;- 0.6\n\nLet us plot the results of 10000 simulations of a binomial distribution with size=20 and prob=0.6:\n\nset.seed(660)\n\nd &lt;- tibble(x = rbinom(n_sims, size = size, prob = prob))\n\nd %&gt;% ggplot(aes(x)) +\n  geom_bar() +\n  scale_x_continuous(breaks = seq(0, size), minor_breaks = NULL) +\n  coord_cartesian(xlim = c(0, size)) +\n  labs(\n    title =\n      str_glue(\n        \"Binomial distribution with size {size} and probability {prob}\"\n      )\n  )\n\n\n\n\n\nSimulating a binomial distribution\n\n\n\nNow a Poisson distribution with mean 5:\n\nlambda &lt;- 5\n\nset.seed(660)\n\nd &lt;- tibble(x = rpois(n_sims, lambda))\n\nd %&gt;% ggplot(aes(x)) +\n  geom_bar() +\n  scale_x_continuous(breaks = seq(0, 2 * lambda), minor_breaks = NULL) +\n  coord_cartesian(xlim = c(0, 2 * lambda)) +\n  labs(\n    title = str_glue(\"Poisson distribution with mean {lambda}\")\n  )\n\n\n\n\n\nSimulating a Poisson distribution\n\n\n\nHere you find a tidyverse oriented flow, as Kurz defined it.\n\nset.seed(5)\n\ntibble(z = rnorm(1e4, mean = 5, sd = 2)) %&gt;% \n  summarise(mean   = mean(z),\n            median = median(z),\n            sd     = sd(z),\n            mad_sd = mad(z))\n\n# A tibble: 1 × 4\n   mean median    sd mad_sd\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1  5.00   4.99  2.02   2.03"
  },
  {
    "objectID": "03-chapter.html",
    "href": "03-chapter.html",
    "title": "3  Some basic methods in mathematics and probability",
    "section": "",
    "text": "4 Check that is ~15 now\nsd(transformed_1)\n\n[1] 14.82577\nWhat is the mean after the first transformation\nmean(transformed_1)\n\n[1] 52.33731\nAdd the difference between the target mean (100) and the current mean\ntransformed_2 &lt;- transformed_1 + (100 - mean(transformed_1))\ntransformed_2 |&gt; hist(breaks = 10)\nplot(original_scores, transformed_scores, type = \"l\")\nExercise 3.8 Correlated random variables\ncorrelation_hw = .3\nmean_husbands = 69.1\nsd_husbands = 2.9\nmean_wives = 63.7\nsd_wives = 2.7\nweighted sum:\n.5 * mean_husbands + .5 * mean_wives\n\n[1] 66.4\nStandard deviation of .5 * husband + .5 wife\nsqrt(.5^2 * sd_husbands + \n       .5^2 * sd_wives + \n       2 * .5 * .5 * correlation_hw)\n\n[1] 1.24499"
  },
  {
    "objectID": "06-chapter.html#summary",
    "href": "06-chapter.html#summary",
    "title": "6  Background on regression modelling",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nAt a pure mathematical level, the methods described in this book have two purposes:\n- prediction (regression to predict an outcome variable, or more precisely the distribution of the outcome, given some set of inputs);\n- comparison (comparing predictions for different values of the inputs, to make simple comparisons between groups, or to estimate causal effects (see chapters 18-21).\nIn this chapter:\n- the favored technique of fake-data simulation is used to understand a simple regression;\n- with using an example (height and earnings) warning agains unwarranted causal interpretations;\n- historical origins of regression are discussed ;"
  },
  {
    "objectID": "06-chapter.html#presentation",
    "href": "06-chapter.html#presentation",
    "title": "6  Background on regression modelling",
    "section": "6.2 Presentation",
    "text": "6.2 Presentation"
  },
  {
    "objectID": "04-chapter.html#poisson-distribution",
    "href": "04-chapter.html#poisson-distribution",
    "title": "4  Statistical Inference",
    "section": "4.3 Poisson distribution",
    "text": "4.3 Poisson distribution\nThis is about frequency of rare events in a specific time or space (for example number of emails people receive on day basis, or number of cars passing a certain point in a given time)\nGenerating a Poisson distribution in R.\n\nset.seed(6)\n\nn&lt;-1000 # sample size\nlambda&lt;- 5 # average number of events in a given time or space\n\nsamplepois &lt;- rpois(n=n, lambda = lambda)\nprint(samplepois)\n\n   [1]  5  9  3  4  7 10  9  7  5  2  6  8  2  4  7  3  5  6  3  6  9  7  2  3\n  [25]  6  4  4  8  4  4  5  3  9  6  3  8  4  2 11  7  6  6  2  9 10  7  2  4\n  [49]  5 10  2  4  5  7 11  8  8  5  4  3  3  8  6  6  2  5  3 11  8  2  4  5\n  [73]  6  4  9  2  4  8  2  4  3  2  4  7  4  5  4  5  5  6  3  2  4  5  9  4\n  [97]  5  6  5  3  5  7  6  3  4  5  8  7  3  6  7  3  2  4  2  9  5  6  5  3\n [121]  5  5  6  5  3  6  2  5  4  7  5  5  4  3  3  2  3 10  6  4  4  3  4  6\n [145]  5  3  4  1  4  8  3  4  4  7  8  7  7  7  8  1 12  8  5  9 11  6  5  3\n [169]  2  9  2  7  8  0  4  6  4  4  3  6  1  2  6 13  8  7  7  2  5  5  2  2\n [193]  7  4  3 11  4 11  3  1  3 12  5  4  5  3  3  4  5  3  4  2  5 12  6  4\n [217]  4  8  3  8  3  8  3  5  2 11  7  5  4  6  6  5  5  6  6  5  3  4  7  3\n [241]  4  4  5  5  4  4  4  8  3  4  4 10  1  7  7  7  4  8  2  9  5  3  7  7\n [265]  3  6  6  5  8  2  6  6  8 11  7  2  5  7  2  4  3  2  6  7  7  5  8  3\n [289]  7  4  2  7  5  7  1  5  7  6  5  5  6  3  4  2  2  5  7  5  3  3  4  5\n [313]  2  5  4  4  2  7  2  7  7  6  6  4  6  6  6  5  9  5  7  6  5 14  3  9\n [337]  3  5  4  8  7  4  1  2  7  4  5  6  5  8  7  4  2  5  5  3  8  6  6  7\n [361]  5  9  7 10  5  5  3  4  6  6  5  5  5  9  5  5  3  6  4  6  8  3  5  2\n [385]  8  5  6  6  3  6  1  4  7  6  4  6  4  5  6  8  8  2  3  5  5  6  4  3\n [409]  3  7  3  4  6  8  8  5  1  4  2  7  4  5  6  3  4  3  8  4  5  5  3  3\n [433]  3  2  6  7  6  3  5  5  4  7  5  4  3  6  8  7  2 11  4  2  4  9 10  7\n [457]  3  4  7  5  5  6  4  5  6  8  5  1  5  8  4 10  7  3  5  6  5  3  8  6\n [481]  0  6  5  6  4 10  8  4  4  6  4  2  4  3  6  5  2  3  3  7  4  4  2  8\n [505]  6  3  4  5  9  2  4  2  9  7 10  5  1  5  4  3  4  3  8  4  5  3  7  2\n [529]  8  3  2  6  3  5  4  3  5  5  5  4  2  4  8  4  5  2  3  7  8  5  3  6\n [553]  9  2  7  7  4  4  2  7  7  1  2  6  7  5  6  3  8  5  3  5  3  1  4  4\n [577]  4  5  7  5  4  3  3  5  4  5  3  2  5  5  4  4  5  2  6  6  4  5  3  5\n [601]  6  4  5  9  2  4  7  4  3  6  4 10  4  6  3  6  5 10  3  5  1  3  2  3\n [625]  7  6  6  4  2  5  6  4  4  5  7  6  4  6  4  3  4  6  8  2  3  3  5  6\n [649]  5  6  3  5  9  6  8  8  6  5  7  2  7  8  8  3  2  6  6  3  7  4  7  3\n [673]  8  5  7  6  3  3  7  5  8  3  5  3  4  8  4  4  6  8  3  4  3  7  5  5\n [697]  5  5  3  2  5  5  4  2  6  9  5  3  7 10 10 10  2  2  3  3  6  4  4  7\n [721]  2  3  4  7  7  5  6  2  5  7  0  7  4  2  2  7  7  5  5  6 10  4  3  5\n [745]  3  5  5  6  7  2  3  8  7  7  4  7  8  3  4  5  5  3  2  5  2  8  5  7\n [769]  3  6  5  3 10  5  1  6  6  1  5  6  2  3  8  4  3  6  4  4  2  7  1  1\n [793]  8  3  9  3  7  6  5  5  4  2  3  3  5  3  3  3  3  4  4  3  6  6  8  3\n [817]  1  6  7  6  3  5  4  2 10  5  6  5  3  4  5  2  4  5  5  1  2  4  8  2\n [841]  4  4  8  6  9  3  3  6  3  5  6  3  8  9  5 10  5  9  5  7  6  4  6  7\n [865]  5  3  2  6  8  5  4  3  2  6  8  6  6  4  6  2  2  2  6  3  7  6  7  3\n [889] 10  3  6  1  5  5  5  4  6  6  3  3  5  7  3  7  7  9  2  7  7  4  6  6\n [913]  5  5  1  7  3  4  4  4  5  6  3  5  4  2  4  3  2  5  8  1  5  4  5  6\n [937]  4  6  7  1  7  4  6  3  6  2  4  5  2  1  7  4  2  8  4  4  5  4  6  5\n [961]  6  7  1  6  5  6 10  4  4  4  2  4  6  7 12  4  4 12  2  8  3  3  6  8\n [985]  3  3  3  7  9  6  2 10  8  3  6  2  2  6  4  6\n\n\nSummarize over sample\n\nsummary(samplepois)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   3.000   5.000   5.009   6.000  14.000 \n\n\nPlot it!\n\nhist(samplepois, breaks=30, col = \"skyblue\", border = \"black\", main = \"Poisson Distribution\", xlab = \"Value\", ylab = \"Frequency\")\n\n\n\n\n\nPoisson distribution"
  },
  {
    "objectID": "04-chapter.html#uncertainty",
    "href": "04-chapter.html#uncertainty",
    "title": "4  Statistical Inference",
    "section": "4.4 2. UNCERTAINTY",
    "text": "4.4 2. UNCERTAINTY\nUncertainty can often be compared visually, see for example figure 4.3 on page 53. It shows the proportion of American adults supporting the death penalty over the years (from a serie of Gallup polls)\nLet us load packages and the data.\n\npolls &lt;- matrix(scan(\"ROS-Examples-master/Death/data/polls.dat\"), ncol=5, byrow=TRUE)\n\nView(polls)\n\n\ndeath_penalty &lt;- \n  polls |&gt;\n  matrix(ncol = 5, byrow = TRUE) %&gt;% \n  as_tibble(\n    .name_repair = ~ c(\"year\", \"month\", \"favor\", \"not_in_favor\", \"no_opinion\")\n  ) |&gt; \n  transmute(\n    date = lubridate::make_date(year = year, month = month),\n    favor = favor / (favor + not_in_favor),\n    favor_sd = sqrt(favor * (1 - favor) / 1000)\n  )\n\nLook at data set now\n\nView(death_penalty)\n\nAre you in favor of the death penalty for a person convicted of murder?\n\ndeath_penalty %&gt;% \n  ggplot(aes(date, favor)) +\n  geom_pointrange(\n    aes(ymin = favor - favor_sd, ymax = favor + favor_sd),\n    size = 0.2\n  ) +\n  scale_x_date(\n    breaks = lubridate::make_date(year = seq(1940, 2000, 10)),\n    minor_breaks = lubridate::make_date(year = seq(1936, 2004, 2)),\n    date_labels = \"%Y\"\n  ) +\n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +\n  labs(\n    title = \n      \"Are you in favor of the death penalty for a person convicted of murder?\",\n    x = \"Year\",\n    y = \"Percentage in favor of those with an opinion\",\n    caption = \"Source: Gallup\"\n  )\n\nWarning: Removed 21 rows containing missing values (`geom_pointrange()`).\n\n\n\n\n\n\nDeath penalty"
  },
  {
    "objectID": "04-chapter.html#significance-testing",
    "href": "04-chapter.html#significance-testing",
    "title": "4  Statistical Inference",
    "section": "4.5 3. SIGNIFICANCE TESTING",
    "text": "4.5 3. SIGNIFICANCE TESTING\n\nStatistical significance is conventionally defined as a \\(p\\)-value less than 0.05, relative to some null hypothesis or prespecified value that would indicate no effect present, as discussed below in the context of hypothesis testing. For fitted regressions, this roughly corresponds to coefficient estimates being labeled as statistically significant if they are at least two standard errors from zero, or not statistically significant otherwise.\n\n\nn &lt;- 20\ny &lt;- 8\n\n# the estimated probability\n(p &lt;- y / n)\n\n[1] 0.4\n\n# the standard error\n(se &lt;- sqrt(p * (1 - p) / n))\n\n[1] 0.1095445\n\n# Not significant because .5 (the expected value, NH) is within the border. \np + c(-2 * se, 2 * se)\n\n[1] 0.180911 0.619089\n\n\nThe hypothesis test is based on a test statistic that summarizes the deviation of the data from what would be expected under the null hypothesis. The conventional test statistic in this sort of problem is the absolute value of the \\(t\\)-score. It is all summarized in \\(p\\)-value. The confidence interval is often more interesting than the \\(p\\)-value, because it gives a range of plausible values for the parameter of interest, rather than just a binary decision about whether the parameter is different from zero.\nOpen some libraries\n\nlibrary(broom)\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.26.1\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nlibrary(brms)\n\nLoading 'brms' package (version 2.20.4). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following objects are masked from 'package:rstanarm':\n\n    dirichlet, exponential, get_y, lasso, ngrps\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n\nDataset\nI used this information here\n\nset.seed(1)\nnA &lt;- 60                        #sample size from Population A\nnB &lt;- 40                        #sample size from Population B\nmuA &lt;- 105                      #population mean of Population A\nmuB &lt;- 77.5                     #population mean of Population B\nsigma &lt;- 3                      #standard deviation of both populations (equally varied)\nyA &lt;- rnorm(nA, muA, sigma)     #Population A sample\nyB &lt;- rnorm(nB, muB, sigma)     #Population B sample\ny &lt;- c(yA, yB)                  #combined dataset\nx &lt;- factor(rep(c(\"A\", \"B\"), c(nA, nB)))  #categorical listing of the populations\nxn &lt;- as.numeric(x)  #numerical version of the population category for means parameterization. \n# Should not start at 0.\nmy_data &lt;- data.frame(y, x, xn)  # dataset\nhead(my_data)\n\n         y x xn\n1 103.1206 A  1\n2 105.5509 A  1\n3 102.4931 A  1\n4 109.7858 A  1\n5 105.9885 A  1\n6 102.5386 A  1\n\nView(my_data)\n\n\n\n\n\nBoxplot\n\n\n\n\n\n\nHistogram of ES\n\n\n\n\n\n\nHistogram of ES\n\n\n\nTraditional approachBayesian approach: use rstanarmBayesian approach: use brms\n\n\nUse ttest\n\nt.test(y ~ x, data = my_data)\n\n\n    Welch Two Sample t-test\n\ndata:  y by x\nt = 48.479, df = 76.318, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n 26.36115 28.61978\nsample estimates:\nmean in group A mean in group B \n      105.32285        77.83238 \n\n\nUse broom\n\nlm(y ~ x, data = my_data) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    105.      0.350     301.  3.31e-147\n2 xB             -27.5     0.553     -49.7 2.46e- 71\n\n\nPlot it!\n\nggplot(my_data, aes(x = x, y = y)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, height = 0) +\n  labs(\n    title = \"Boxplot of the two groups\",\n    x = \"Group\",\n    y = \"y\"\n  )\n\n\n\n\n\ndata_rstanarm&lt;-stan_glm(y ~ x, data = my_data, family = gaussian())\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.00304 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 30.4 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.054 seconds (Warm-up)\nChain 1:                0.05 seconds (Sampling)\nChain 1:                0.104 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.3e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.052 seconds (Warm-up)\nChain 2:                0.062 seconds (Sampling)\nChain 2:                0.114 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.4e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.052 seconds (Warm-up)\nChain 3:                0.053 seconds (Sampling)\nChain 3:                0.105 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.3e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.064 seconds (Warm-up)\nChain 4:                0.097 seconds (Sampling)\nChain 4:                0.161 seconds (Total)\nChain 4: \n\n\n\nprint(data_rstanarm)\n\nstan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n observations: 100\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 105.3    0.4 \nxB          -27.5    0.6 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.7    0.2   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nsummary(data_rstanarm)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 100\n predictors:   2\n\nEstimates:\n              mean   sd    10%   50%   90%\n(Intercept) 105.3    0.4 104.9 105.3 105.8\nxB          -27.5    0.6 -28.2 -27.5 -26.8\nsigma         2.7    0.2   2.5   2.7   3.0\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 94.3    0.4 93.8  94.3  94.8 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  3543 \nxB            0.0  1.0  3878 \nsigma         0.0  1.0  3407 \nmean_PPD      0.0  1.0  3424 \nlog-posterior 0.0  1.0  1951 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\nposterior_interval(data_rstanarm)\n\n                    5%       95%\n(Intercept) 104.730411 105.92330\nxB          -28.428786 -26.57562\nsigma         2.426685   3.09435\n\n\nProbability statements are possible\n\nmcmc = as.matrix(data_rstanarm)\n# Percentage change (relative to Group A)\nES = 100 * mcmc[, \"xB\"]/mcmc[, \"(Intercept)\"]\nhist(ES)\n\n\nProbability that the effect is greater than 10% (a decline of &gt;10%)\n\nsum(-1 * ES &gt; 10)/length(ES)\n\n[1] 1\n\n\nProbability that the effect is greater than 25% (a decline of &gt;25%)\n\nsum(-1 * ES &gt; 25)/length(ES)\n\n[1] 0.99125\n\n\n\n\n\ndata_brms &lt;- brm(y ~ x, data = my_data, family = gaussian())\n\nCompiling Stan program...\n\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nclang -mmacosx-version-min=10.13 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:88:\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name 'namespace'\nnamespace Eigen {\n^\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected ';' after top level declarator\nnamespace Eigen {\n               ^\n               ;\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: 'complex' file not found\n#include &lt;complex&gt;\n         ^~~~~~~~~\n3 errors generated.\nmake: *** [foo.o] Error 1\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 1:                0.029 seconds (Sampling)\nChain 1:                0.063 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 6e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.029 seconds (Warm-up)\nChain 2:                0.022 seconds (Sampling)\nChain 2:                0.051 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 3:                0.022 seconds (Sampling)\nChain 3:                0.05 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 5e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 4:                0.019 seconds (Sampling)\nChain 4:                0.053 seconds (Total)\nChain 4: \n\n\n\nprint(data_brms)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x \n   Data: my_data (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   105.34      0.35   104.66   106.01 1.00     4603     3135\nxB          -27.49      0.55   -28.55   -26.43 1.00     4276     3006\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.74      0.20     2.38     3.17 1.00     4246     2904\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nsummary(data_brms)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x \n   Data: my_data (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   105.34      0.35   104.66   106.01 1.00     4603     3135\nxB          -27.49      0.55   -28.55   -26.43 1.00     4276     3006\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.74      0.20     2.38     3.17 1.00     4246     2904\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nmcmc = as.matrix(data_brms)\n# Percentage change (relative to Group A)\nES = 100 * mcmc[, \"b_xB\"]/mcmc[, \"b_Intercept\"]\nhist(ES)\n\n\nProbability that the effect is greater than 10% (a decline of &gt;10%)\n\nsum(-1 * ES &gt; 10)/length(ES)\n\n[1] 1\n\n\nProbability that the effect is greater than 25% (a decline of &gt;25%)\n\nsum(-1 * ES &gt; 25)/length(ES)\n\n[1] 0.99125\n\n\n\n\n\nSteps of (Bayesian) regression analysis (will look at this in more detail later):\n1. Identify and collect the data (also visualization);\n2. Choose a statistical model;\n3. Specify prior distributions;\n4. Obtain posterior distributions;\n5. Posterior predictive checks;\n6. Interpret results (also visualization)."
  }
]