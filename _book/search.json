[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ROS-Notebook",
    "section": "",
    "text": "The Reading Club\n\n\nThis is a Notebook around Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari. The book is available at https://avehtari.github.io/ROS-Examples/. The notesbook is available at https://github.com/Jonkman1/Regression_reading_club_NSCR.\nThis Notebook is part of the NSC-R Workshops. The NSC-R Workshops is a series of one-hour online instructional sessions to support participants in developing their data science skills in R, and to promote open science principles. The NSC-R workshop meetings are organized by a team mostly affiliated with the Netherlands Institute for the Study of Crime and Law Enforcement (NSCR), but they are open to everyone, regardless of affiliation or skill level.\nThere are workshops on specific topics and Tidy Tuesday workshops that cover more basic skills and materials. A collection of these latter workshop materials have been brought together by Harrie Jonkman in a single document that you can find here.\nStarting November 2023 we are transforming the workshops on specific topics into a reading group, where we explore the world of data analysis using the book Regression and Other Stories by Gelman, Hill and Vehtari as a source of inspiration. Click here for more information.\nIn this Notebook you find short summaries of the chapters of the ROS-book and presentations of these chapters by different chairs. The content of the Notebook will grow over time.\n\n\nMaterials\n\nHere you find the book.\nHere you find background information and syntaxes."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Chapter_2.html",
    "href": "Chapter_2.html",
    "title": "2  Chapter 2",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "01-chapter.html",
    "href": "01-chapter.html",
    "title": "1  Overview",
    "section": "",
    "text": "2 Summary\nThis chapter lays out the key challenges of statistical inference in general and regression modeling in particular.\n\n\nInference defined as using mathematical models to make general claims from particular data\nThere are three challenges to statistics, which all can be framed as problems of prediction: - Generalizing from sample to population;\n- Generalizing from treatment to control group;\n- Generalizing from observed measurements to the underlying construct of interest.\nThe key skills you learn in this book are: - Understanding regression models;\n- Constructing regression models;\n- Fitting regression models to data;\n- Displaying and interpreting the results of regression models;\nRegression is a method that allows researchers to summarize how predictions or average values of an outcome vary across individuals defined by a set of predictors. It is used for example to predict, to explore associations, to extrapolate and for causal inference. Exmaples are given.\nThere are four steps in statistical analysis: - Model building (starting);\n- Model fitting;\n- Understanding model fits;\n- Criticism.\nFitting models and making predictions can be down different frameworks. Three concerns are important everytime: - Information used;\n- Assumptions used;\n- Estimating and interpreting (classical or Bayesian framework).\nThey recommend to use the Bayesian framework. If information available you can use it, if not you can use weakly informative default priors. On this way you stable estimates and with the simulations you can express uncertainty.\nThe overall Bayesian regression in R is:\nfit&lt;-stan_glm(y~x,data=mydata)\n::: {.column-margin} Bayes can take longer time. Here you can use\nfit&lt;-stan_glm(y~x,data=mydata,algorithm=\"optimizing\")\nWhere y is the outcome, x is the predictor and mydata is the data frame. But you can do it also in classical framework:\nfit&lt;-lm(y~x,data=mydata)\nUsing Bayesian and simulation approaches can be more important when fitting multilevel or regularized regression models. This will be handled in their next book.\n\n3 Presentation\n\nFirst load libraries.\n\nlibrary(rosdata)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.26.1\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\n\n\nAttaching package: 'rstanarm'\n\n\nThe following objects are masked from 'package:rosdata':\n\n    kidiq, roaches, wells\n\n\nExplore data.\n\ndata(\"hibbs\")\nglimpse(hibbs)\n\nRows: 16\nColumns: 5\n$ year                &lt;int&gt; 1952, 1956, 1960, 1964, 1968, 1972, 1976, 1980, 19…\n$ growth              &lt;dbl&gt; 2.40, 2.89, 0.85, 4.21, 3.02, 3.62, 1.08, -0.39, 3…\n$ vote                &lt;dbl&gt; 44.60, 57.76, 49.91, 61.34, 49.60, 61.79, 48.95, 4…\n$ inc_party_candidate &lt;chr&gt; \"Stevenson\", \"Eisenhower\", \"Nixon\", \"Johnson\", \"Hu…\n$ other_candidate     &lt;chr&gt; \"Eisenhower\", \"Stevenson\", \"Kennedy\", \"Goldwater\",…\n\n\nReplicate the plot.\n\nggplot(data = hibbs,\n       mapping = aes(x = growth, y = vote)) +\n  # geom_label(mapping = aes(label = year), nudge_x = 0.3, fill = NA, size = 3) +\n  geom_point() \n\n\n\n\nRun model.\n\nM1 &lt;- stan_glm(vote ~ growth, data=hibbs)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.005053 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 50.53 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.059 seconds (Warm-up)\nChain 1:                0.048 seconds (Sampling)\nChain 1:                0.107 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.6e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.055 seconds (Warm-up)\nChain 2:                0.049 seconds (Sampling)\nChain 2:                0.104 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.6e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.057 seconds (Warm-up)\nChain 3:                0.055 seconds (Sampling)\nChain 3:                0.112 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.084 seconds (Warm-up)\nChain 4:                0.052 seconds (Sampling)\nChain 4:                0.136 seconds (Total)\nChain 4: \n\n\nModel summary.\n\nM1\n\nstan_glm\n family:       gaussian [identity]\n formula:      vote ~ growth\n observations: 16\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 46.3    1.7  \ngrowth       3.0    0.8  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 3.9    0.8   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nPrint slope.\n\ncoef(M1)\n\n(Intercept)      growth \n   46.29257     3.01333 \n\n\nAdd line to plot.\n\nggplot(data = hibbs,\n       mapping = aes(x = growth, y = vote)) +\n  geom_point() +\n  geom_abline(slope     = coef(M1)[[\"growth\"]],\n              intercept = coef(M1)[[\"(Intercept)\"]])\n\n\n\n\nPeacekeeping data.\n\npeace_df &lt;- read_csv(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/Peacekeeping/minidata.csv\")\n\nNew names:\nRows: 96 Columns: 7\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (6): cfdate, faildate, peacekeepers?, badness, delay, censored?\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nThis one is weird: badness var is ethnicity?!\n\npeace_df &lt;- read_csv(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/Peacekeeping/data/minidata.csv\")\n\nNew names:\nRows: 2031 Columns: 7\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): badness dbl (6): ...1, cfdate, faildate, peacekeepers?, delay, censored?\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nExplore\n\nglimpse(peace_df)\n\nRows: 2,031\nColumns: 7\n$ ...1            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ cfdate          &lt;dbl&gt; 140, 150, 210, 125, 126, 200, 110, 165, 190, 125, 200,…\n$ faildate        &lt;dbl&gt; 66, 64, 74, 66, 64, 65, 63, 68, 63, 64, 62, 73, 72, 72…\n$ `peacekeepers?` &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ badness         &lt;chr&gt; \"white\", \"black\", \"white\", \"white\", \"white\", \"white\", …\n$ delay           &lt;dbl&gt; 1, 1, 3, 5, 1, 1, 6, 1, 1, 4, 2, 1, 1, 1, 2, 1, 4, 4, …\n$ `censored?`     &lt;dbl&gt; 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, …\n\n\nCreate date measure. It’s actually the same as delay.\n\npeace_df &lt;- peace_df %&gt;% \n  mutate(time_diff = (faildate-cfdate)/365)\n\nPlot\n\npeace_df %&gt;% \n  ggplot(data = .) +\n  geom_histogram(mapping = aes(x = delay), bins = 10) +\n  facet_wrap(~`peacekeepers?`)\n\n\n\n\nScatter.\n\nggplot(data = peace_df) +\n  geom_point(mapping = aes(y = delay,\n                           colour = as.factor(`censored?`),\n                           x = badness,\n                           )) +\n  facet_wrap(~`peacekeepers?`)\n\n\n\n\nMeans.\n\npeace_df %&gt;% \n  group_by(`peacekeepers?`, `censored?`) %&gt;% \n  summarise(mean_badness = mean(badness, na.rm = TRUE))\n\nWarning: There were 5 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `mean_badness = mean(badness, na.rm = TRUE)`.\nℹ In group 1: `peacekeepers? = 0`, `censored? = 1`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\n\n`summarise()` has grouped output by 'peacekeepers?'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 5 × 3\n# Groups:   peacekeepers? [2]\n  `peacekeepers?` `censored?` mean_badness\n            &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1               0           1           NA\n2               0           2           NA\n3               1           1           NA\n4               1           2           NA\n5               1          NA           NA\n\n\nSimple causal graph for reproducibility of simulated data\n\nSEED &lt;- 1151\nset.seed(SEED)\nN &lt;- 50\nx &lt;- runif(N, 1, 5)\ny &lt;- rnorm(N, 10 + 3*x, 3)\nx_binary &lt;- ifelse(x&lt;3, 0, 1)\ncausal_df &lt;- data.frame(N, x, y, x_binary)\n\nPlot\n\nggplot(data = causal_df) +\n  geom_point(mapping = aes(y = y, x = x))"
  },
  {
    "objectID": "02-chapter.html",
    "href": "02-chapter.html",
    "title": "2  Data and measurement",
    "section": "",
    "text": "3 (1) Map visual\nLoad in HDI data.\nhdi_df &lt;- read.table(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/HDI/data/hdi.dat\", header = TRUE)\nGet state boundaries using the {maps} package.\nstates_sf &lt;- maps::map(\"state\",\n                       plot = FALSE,\n                       fill = TRUE) %&gt;% \n  st_as_sf()\nPlot this.\nggplot(data = states_sf) +\n  geom_sf()\nJoin the HDI data to the spatial polygons.\nstate_hdi_sf &lt;- hdi_df |&gt;\n  mutate(ID = str_to_lower(state)) |&gt; \n  right_join(states_sf) |&gt;\n  st_as_sf()\n\nJoining with `by = join_by(ID)`\nMap out HDI using a continuous scale.\nggplot(data = state_hdi_sf) +\n  geom_sf(mapping = aes(fill = hdi)) +\n  scale_fill_viridis_c()\nLoad in the income/hdi data.\nincome_df &lt;- read.dta(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/HDI/data/state vote and income, 68-00.dta\")\nCreate a common id again.\nincome_df &lt;- income_df %&gt;% \n  mutate(st_state = str_to_lower(st_state))\nScatterplots.\nincome_hdi_df &lt;-income_df %&gt;%\n  filter(st_year == 2000) %&gt;%\n  ggplot() +\n  geom_text(mapping = aes(x = st_income, y = hdi, label = st_stateabb))\n  geom_point(mapping = aes(x = st_income, y = hdi))\n\nmapping: x = ~st_income, y = ~hdi \ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity"
  },
  {
    "objectID": "01-chapter.html#summary",
    "href": "01-chapter.html#summary",
    "title": "1  Overview",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nThis first chapter lays out the key challenges of statistical inference in general and regression modeling in particular.\n\n\nInference defined as using mathematical models to make general claims from particular data\nThere are three challenges to statistics, which all can be framed as problems of prediction:\n- Generalizing from sample to population;\n- Generalizing from treatment to control group;\n- Generalizing from observed measurements to the underlying construct of interest.\nThe key skills you learn in this book are:\n- Understanding regression models;\n- Constructing regression models;\n- Fitting regression models to data;\n- Displaying and interpreting the results of regression models;\nRegression is a method that allows researchers to summarize how predictions or average values of an outcome vary across individuals defined by a set of predictors. It is used for example to predict, to explore associations, to extrapolate and for causal inference. Exmaples are given.\nThere are four steps in statistical analysis:\n- Model building (starting);\n- Model fitting;\n- Understanding model fits;\n- Criticism.\nFitting models and making predictions can be down different frameworks. Three concerns are important everytime: - Information used;\n- Assumptions used;\n- Estimating and interpreting (classical or Bayesian framework).\nGelman et all. recommend to use the Bayesian framework. If information available you can use it, if not you can use weakly informative default priors. On this way you stable estimates and with the simulations you can express uncertainty.\nThe overall Bayesian regression in R is:\n\nBayes can take longer time. Here you can use\nfit&lt;-stan_glm(y~x,data=mydata,algorithm=\"optimizing\")\n\nrstanarmbrms\n\n\nfit&lt;-stan_glm(y~x,data=mydata)\n\n\n\nfit&lt;-brm(y~x,data=mydata)\n\n\n\nWhere y is the outcome, x is the predictor and mydata is the data frame. But you can do it also in classical framework:\nfit&lt;-lm(y~x,data=mydata)\nUsing Bayesian and simulation approaches can be more important when fitting multilevel or regularized regression models. This will be handled in their next book."
  },
  {
    "objectID": "01-chapter.html#presentation",
    "href": "01-chapter.html#presentation",
    "title": "1  Overview",
    "section": "1.2 Presentation",
    "text": "1.2 Presentation\nFirst some libraries are loaded.\n\nlibrary(rosdata) # for the ROSdata\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(rstanarm) # for the stan_glm function\nlibrary(brms) # for the brm function"
  },
  {
    "objectID": "02-chapter.html#summary",
    "href": "02-chapter.html#summary",
    "title": "2  Data and measurement",
    "section": "2.1 Summary",
    "text": "2.1 Summary\nBefore fitting a model, you need to understand the data and the measurements, the numbers and where they come from. This chapter demonstrates through examples how to use graphical tools to explore understand and measurements.\nThe first example (graphing Human Development Index) shows that you understand data better when you plot them in different ways. The second example (Political Ideology and party identification) show that details of measurement can be important and that gaps between measurement and reality can be large.\nKeep in mind that the issue of measurement is important because of two reasons:\n- We need to understand what the data actually mean.\n- Learning about accuracy, reliability and validity will set the foundation for understanding variance, correlation, and error (all part of linear models).\nFeatures of data quality:\nA measure is valid to the degree that it represents what you are trying to measure. Validity of a measuring process is defined as a property of giving the right answer across a wide range of plausible scenarios.\nA measure is reliable to the degree that it gives the same answer when applied to the same situation. A reliable measure is one that is precise and stable.\nSelection is the third feature of data quality, the idea that the data you see can be a non-representative sample of a larger population that you will not see.\nSeveral visualization suggestions are given in this chapter.\nThree uses of graphics in statistical analysis are given:\n- Displays of raw data (part of explorative analysis), just to understand data;\n- Graphs of fitted models and inferences (to understand model fits);\n- Graphs as communication tools (to communicate results)."
  },
  {
    "objectID": "02-chapter.html#presentation",
    "href": "02-chapter.html#presentation",
    "title": "2  Data and measurement",
    "section": "2.2 Presentation",
    "text": "2.2 Presentation\nOn 12-12-2023 Sam Langton gave this presentation on chapter 2 of Regression and Other Stories for the *Reading Club Session (chapter 2). His script you can also find here.\nFirst load packages for this chapter.\n\nlibrary(foreign)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(broom)\nlibrary(maps)\nlibrary(sf)\nlibrary(rosdata)\n\nGeneral discussion points he put on the agenda:\n- New GitHub repo intro.\n- Difference between visualisation issues and then scrutiny over a composite measure (section 2.1).\n- Validity and reliability: any examples from people’s work?\n- What did people think of the plots? I had some queries (e.g., 2.5, 2.7).\n- Grammar of Graphics-style approach (but without ggplot2 code).\n- Plotting regression results.\n\n2.2.1 Map visual\nLoad in HDI data.\n\n# take care of the path to the data\n\nhdi_df &lt;- read.table(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/HDI/data/hdi.dat\", header = TRUE)\n\nGet state boundaries using the maps-package.\n\nstates_sf &lt;- maps::map(\"state\",\n                       plot = FALSE,\n                       fill = TRUE) %&gt;% \n  st_as_sf()\n\nPlot this.\n\nggplot(data = states_sf) +\n  geom_sf()\n\n\n\n\n\nMap of states\n\n\n\nJoin the HDI data to the spatial polygons.\n\nstate_hdi_sf &lt;- hdi_df |&gt;\n  mutate(ID = str_to_lower(state)) |&gt; \n  right_join(states_sf) |&gt;\n  st_as_sf()\n\nMap out HDI using a continuous scale.\n\nggplot(data = state_hdi_sf) +\n  geom_sf(mapping = aes(fill = hdi)) +\n  scale_fill_viridis_c()\n\n\n\n\n\nMap of HDI\n\n\n\n\n\n2.2.2 Scatterplots\nLoad in the income/hdi data.\n\n# take care of the path to the data\n\nincome_df &lt;- read.dta(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/HDI/data/state vote and income, 68-00.dta\")\n\nCreate a common id again.\n\nincome_df &lt;- income_df %&gt;% \n  mutate(st_state = str_to_lower(st_state))\n\nScatterplots.\n\n\n# Remark: why not a scatterplot?\n\nincome_hdi_df &lt;-income_df %&gt;%\n  filter(st_year == 2000) %&gt;%\n  ggplot() +\n  geom_text(mapping = aes(x = st_income, y = hdi, label = st_stateabb))\n  geom_point(mapping = aes(x = st_income, y = hdi))\n\nmapping: x = ~st_income, y = ~hdi \ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\n\n\n2.2.3 Names plots\n\n# take care of the path to the data\n\nnames_df &lt;- read.csv(\"~/Desktop/WERK/Gelman/ROS-book/ROS-book/ROS-Examples-master/Names/data/allnames_clean.csv\") %&gt;% \n  as_tibble()\n\nMake it long and filter.\n\nnames_long_df &lt;- names_df |&gt; \n  pivot_longer(cols = c(-X, -name, -sex), names_to = \"year\", values_to = \"number\",\n               names_prefix = \"X\") |&gt;\n  mutate(year = as.numeric(year),\n         last_letter = str_sub(name, start = -1)) \n\nNow Figure 2.7.\n\nnames_long_df %&gt;% \n  filter(year == 1906 | year == 1956 | year == 2006,\n         sex == \"M\") %&gt;% \n  group_by(year, last_letter) %&gt;% \n  summarise(counts = sum(number)) %&gt;% \n  ungroup() %&gt;% \n  group_by(year) %&gt;% \n  mutate(total_counts = sum(counts),\n         prop_counts  = counts/total_counts) %&gt;% \n  ungroup() %&gt;% \n  ggplot(data = .) +\n  geom_col(mapping = aes(x = last_letter, y = prop_counts,\n                         fill = as.factor(year)), \n           position = \"dodge\") +\n  facet_wrap(~year)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\nFigure 2.7\n\n\n\nMake a line graph.\n\nnames_long_df %&gt;% \n  filter(sex == \"M\") %&gt;%\n  group_by(year, last_letter) %&gt;% \n  summarize(yearly_counts = sum(number)) %&gt;% \n  ungroup() %&gt;% \n  group_by(year) %&gt;% \n  mutate(year_total  = sum(yearly_counts),\n         yearly_prop = yearly_counts/year_total,\n         end_letter  = if_else(last_letter == \"d\" |\n                               last_letter == \"y\" |\n                               last_letter == \"n\",\n                               last_letter,\n                               \"(other)\")) %&gt;% \n  ungroup() %&gt;% \n  ggplot(data = .) +\n  geom_line(mapping = aes(x = year, y = yearly_prop, group = last_letter,\n                          colour = end_letter)) +\n  scale_colour_manual(values = c(\"grey80\", \"black\", \"tomato\", \"dodgerblue\")) \n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\nLine graph"
  },
  {
    "objectID": "01-chapter.html#regression-and-other-stories-reading-club-session-chapter-1",
    "href": "01-chapter.html#regression-and-other-stories-reading-club-session-chapter-1",
    "title": "1  Overview",
    "section": "1.3 Regression and Other Stories Reading Club Session (chapter 1)",
    "text": "1.3 Regression and Other Stories Reading Club Session (chapter 1)\nOn 14-11-2023 Alex Trinidad (University of Cologne and Netherlands Institute for the Stduy of Crime and Law Enforcement) presented the first chapter of the book Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari. The session was held online via Zoom. Here you can find Alex’ script Trinidad.\nFirst he loaded this package.\n\nlibrary(tidyverse)\n\n\nRegression to predict\n\nHow can we predict presidential vote share using economy growth? For this he loaded the ROS-data.\n\nelections_data &lt;- read.csv(url(\"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectionsEconomy/data/hibbs.dat\"), sep = \"\")\n\nThis another way to load these data.\n\nremotes::install_github(\"avehtari/ROS-Examples\", subdir = \"rpackage\")\n\nSkipping install of 'rosdata' from a github remote, the SHA1 (a049a104) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nelections_data &lt;- rosdata::hibbs\n\nLet us first explore economy growth.\n\nglimpse(elections_data)\n\nRows: 16\nColumns: 5\n$ year                &lt;int&gt; 1952, 1956, 1960, 1964, 1968, 1972, 1976, 1980, 19…\n$ growth              &lt;dbl&gt; 2.40, 2.89, 0.85, 4.21, 3.02, 3.62, 1.08, -0.39, 3…\n$ vote                &lt;dbl&gt; 44.60, 57.76, 49.91, 61.34, 49.60, 61.79, 48.95, 4…\n$ inc_party_candidate &lt;chr&gt; \"Stevenson\", \"Eisenhower\", \"Nixon\", \"Johnson\", \"Hu…\n$ other_candidate     &lt;chr&gt; \"Eisenhower\", \"Stevenson\", \"Kennedy\", \"Goldwater\",…\n\n\nTry the view-function yourself.\n\n# View(elections_data)\n\nUse visualization to understand the data.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth))\n\n\n\n\n\nPredicting elections from the economy 1952-2016\n\n\n\nAdd a line to the plot.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth)) +\n  geom_smooth(aes(x = year, y = growth), se = FALSE)\n\n\n\n\n\nPredicting elections from the economy 1952-2016 with line\n\n\n\nAdd the CI around the line.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth)) +\n  geom_smooth(aes(x = year, y = growth), se = TRUE)\n\n\n\n\n\nPredicting elections from the economy 1952-2016 with line and confidence interval\n\n\n\nFit ols-regression to obtain the predicted values.\n\nmod1 &lt;- lm(vote ~ growth, data = elections_data)\n\nSummarize the regression results.\n\nsummary(mod1)\n\n\nCall:\nlm(formula = vote ~ growth, data = elections_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9929 -0.6674  0.2556  2.3225  5.3094 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  46.2476     1.6219  28.514 8.41e-14 ***\ngrowth        3.0605     0.6963   4.396  0.00061 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.763 on 14 degrees of freedom\nMultiple R-squared:  0.5798,    Adjusted R-squared:  0.5498 \nF-statistic: 19.32 on 1 and 14 DF,  p-value: 0.00061\n\n\nPlot the predicted values.\n\nplot(elections_data$growth, elections_data$vote, xlab = \"Economic Growth\", ylab = \"Vote Share\")\nabline(coef(mod1), col = \"red\")\n\n\n\n\n\nPredicting elections from the economy 1952-2016 with line\n\n\n\nPredicted values with ggplot.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = growth, y = vote)) +\n  geom_abline(intercept = mod1[[1]][[1]], slope = mod1[[1]][[2]], color = \"red\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Data and linear fit\",\n       x = \"Average recent growth in personal income\",\n       y = \"Incumbent party's vote share\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nPredicted values with ggplot and geom_smooth.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = growth, y = vote)) +\n  geom_smooth(method = \"lm\", aes(x = growth, y = vote), color = \"blue\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Data and linear fit\",\n       x = \"Average recent growth in personal income\",\n       y = \"Incumbent party's vote share\")\n\n\n\n\n\nSketching regression\n\nOriginal \\(y = 46.3 + 3.0 x\\). Explore the descriptive stats to get some parameters based on the observed data.\n\nelections_data |&gt; \n  summarise(min_growth = min(growth),\n            max_growth = max(growth),\n            mean_growth = mean(growth),\n            sd_growth = sd(growth),\n            min_vote = min(vote),\n            max_vote = max(vote),\n            mean_vote = mean(vote),\n            sd_vote = sd(vote))\n\n  min_growth max_growth mean_growth sd_growth min_vote max_vote mean_vote\n1      -0.39       4.21      1.8975  1.395538     44.6    61.79    52.055\n   sd_vote\n1 5.608951\n\n\nSimulating the data (technique often used in this book).\n\nset.seed(123)\nN &lt;- 16\nsimu_growth &lt;- runif(N, -0.39, 4)\nsimu_vote &lt;- rnorm(N, 46.2476  + 3.0605*simu_growth, 3.763)\nsimu_elections &lt;- data.frame(N,simu_growth, simu_vote)\n\nModel the simulated data.\n\nsimu_mod &lt;- lm(simu_vote ~ simu_growth, data = simu_elections)\n\nSummarize the model.\n\nsummary(simu_mod)\n\n\nCall:\nlm(formula = simu_vote ~ simu_growth, data = simu_elections)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.355 -1.513 -0.488  1.839  5.962 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  43.6769     1.7558  24.876 5.49e-13 ***\nsimu_growth   4.0052     0.6948   5.765 4.90e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.448 on 14 degrees of freedom\nMultiple R-squared:  0.7036,    Adjusted R-squared:  0.6824 \nF-statistic: 33.23 on 1 and 14 DF,  p-value: 4.896e-05\n\n\nPlot the simulated data using base graphics.\n\n# Base graphic\nplot(simu_elections$simu_growth, simu_elections$simu_vote, xlab = \"Simulated Economic Growth\", ylab = \"Simulated Vote Share\")\nabline(coef(simu_mod), col = \"blue\")\n\n\n\n\n\nSimulated Data and linear fit\n\n\n\nPlot the samen using ggplot version.\n\nggplot(data = simu_elections) +\n  geom_point(aes(x = simu_growth, y = simu_vote)) +\n  geom_smooth(method = \"lm\", aes(x = simu_growth, y = simu_vote), color = \"blue\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Simulated Data and linear fit\",\n       x = \"Simulated Average recent growth in personal income\",\n       y = \"Simulated Incumbent party's vote share\")\n\n\n\n\nExercise 1.2(a) from ROS for sketching a regression model and data.\n\n\\(y = 30 + 10x\\) (residual \\(sd 3.9\\)) & values of X ranging from \\(0-4\\)\n\nDefine the data.\n\nset.seed(123)\nN &lt;- 50\nx &lt;- runif(N, 0, 4)\ny &lt;- rnorm(N, 30 + 10*x, 3.9)\ndata &lt;- data.frame(N, x, y)\n\nModel the data.\n\nlm_a &lt;- lm(y ~ x, data)\n\nPlot the data.\n\nplot(data$x, data$y, xlab = \"X Value\", ylab = \"Y value\")\nabline(coef(lm_a), col = \"red\", size = 1)\n\nWarning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): \"size\" is\nnot a graphical parameter\n\n\n\n\n\n\nExercise 1.2 from ROS\n\n\n\n\n\\(y = 30 + 10x\\) (residual \\(sd 10\\)) & values of X ranging from \\(0-4\\).\n\nDefine the data.\n\nset.seed(123)\nN &lt;- 50\nx &lt;- runif(N, 0, 4)\ny &lt;- rnorm(N, 30 + 10*x, 10)\ndata &lt;- data.frame(N, x, y)\n\nModel it.\n\nlm_b &lt;- lm(y ~ x, data)\n\nPlot it.\n\nplot(data$x, data$y, xlab = \"X Value\", ylab = \"Y value\")\nabline(coef(lm_b), col = \"blue\")\n\n\n\n\n\nContinuous predictor\n\n\n\nNow simulate a binary predictor example from the Aki Vehtari GH\nSee Figure 1.5 (page 10).\n\nset.seed(1411)\nN &lt;- 50\nx &lt;- runif(N, 0, 4)\ny &lt;- rnorm(N, 30 + 10*x, 10)\nx_binary &lt;- ifelse(x &lt; 3, 0, 1)\ndata_simu &lt;- data.frame(N, x, y, x_binary)\n\nModel it.\n\nlm_binary &lt;- lm(y ~ x_binary, data = data_simu)\n\nSummarize the model.\n\nsummary(lm_binary)\n\n\nCall:\nlm(formula = y ~ x_binary, data = data_simu)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-27.2063  -8.5257   0.5297   9.3644  27.8011 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.812      2.296  19.953  &lt; 2e-16 ***\nx_binary      19.033      3.827   4.974 8.81e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.99 on 48 degrees of freedom\nMultiple R-squared:  0.3401,    Adjusted R-squared:  0.3264 \nF-statistic: 24.74 on 1 and 48 DF,  p-value: 8.813e-06\n\n\nPlot the relationship.\n\nggplot(data = data_simu) +\n  geom_point(aes(x = x_binary, y = y)) +\n  geom_abline(intercept = lm_binary[[1]][[1]], slope = lm_binary[[1]][[2]],\n              color = \"blue\", size = 1) +\n  labs(y = \"Crime reduction\", \n       x =  NULL) +\n  scale_x_continuous(breaks = c(0,1),\n                     labels = c(\"Control\", \"Treatment\")) +\n  annotate(geom = \"text\", x = 0.50, y = 40,\n           label = paste(\"Estimated treatment effect is\\nslope of fitted line: \",\n                         round(lm_binary[[1]][[2]], digits = 2)))\n\n\n\n\n\nBinary predictor\n\n\n\nNon-linear relationship\n\nset.seed(1411)\nx &lt;- runif(N, 1, 7)\ny &lt;- rnorm(N, 7 + 30*exp(-x), 2)\ndata_simu$y &lt;- y\n\nFit the model.\n\nlm_nonlinear &lt;- lm(y ~ x, data = data_simu)\n\nSummarize the model.\n\nsummary(lm_nonlinear)\n\n\nCall:\nlm(formula = y ~ x, data = data_simu)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0484 -1.4874 -0.0243  1.7868  4.4113 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.1516     0.6188  21.253  &lt; 2e-16 ***\nx            -1.8761     0.2476  -7.579  9.6e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.21 on 48 degrees of freedom\nMultiple R-squared:  0.5447,    Adjusted R-squared:  0.5353 \nF-statistic: 57.43 on 1 and 48 DF,  p-value: 9.599e-10\n\n\nPlot the model outcome.\n\nggplot(data = data_simu) +\n  geom_point(aes(x = x, y = y)) +\n  geom_smooth(method = \"loess\", aes(x = x, y = y), color = \"blue\", size = 1, se = FALSE) +\n  labs(y = \"Theft counts per hour\", \n       x =  \"Hours of foot patrol\")  \n\n\n\n\n\nNon-linear relationship"
  },
  {
    "objectID": "01-chapter.html#more-examples",
    "href": "01-chapter.html#more-examples",
    "title": "1  Overview",
    "section": "1.4 More examples",
    "text": "1.4 More examples\nFirst look at dataset to predict US-elections (1952-2021) from the economy and explore data.\n\ndata(\"hibbs\")\nglimpse(hibbs)\n\nRows: 16\nColumns: 5\n$ year                &lt;int&gt; 1952, 1956, 1960, 1964, 1968, 1972, 1976, 1980, 19…\n$ growth              &lt;dbl&gt; 2.40, 2.89, 0.85, 4.21, 3.02, 3.62, 1.08, -0.39, 3…\n$ vote                &lt;dbl&gt; 44.60, 57.76, 49.91, 61.34, 49.60, 61.79, 48.95, 4…\n$ inc_party_candidate &lt;chr&gt; \"Stevenson\", \"Eisenhower\", \"Nixon\", \"Johnson\", \"Hu…\n$ other_candidate     &lt;chr&gt; \"Eisenhower\", \"Stevenson\", \"Kennedy\", \"Goldwater\",…\n\n\nReplicate the plot of Figure 1.1.\n\nggplot(data = hibbs,\n       mapping = aes(x = growth, y = vote)) +\n  # geom_label(mapping = aes(label = year), nudge_x = 0.3, fill = NA, size = 3) +\n  geom_point() \n\n\n\n\n\nPredicting elections from the economy 1952-2016\n\n\n\nNow run the first regression model using stanarm or brms. This simulation works with four chains and 2000 iterations per chain.\n\nrstanarmbrms\n\n\n\nM1 &lt;- stan_glm(vote ~ growth, data=hibbs)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.82 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.097 seconds (Warm-up)\nChain 1:                0.057 seconds (Sampling)\nChain 1:                0.154 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.5e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.057 seconds (Warm-up)\nChain 2:                0.046 seconds (Sampling)\nChain 2:                0.103 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.5e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.051 seconds (Warm-up)\nChain 3:                0.048 seconds (Sampling)\nChain 3:                0.099 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.075 seconds (Warm-up)\nChain 4:                0.053 seconds (Sampling)\nChain 4:                0.128 seconds (Total)\nChain 4: \n\n\nM1 is set on your computer and you can give a summary of this regression model.\n\nM1\n\nstan_glm\n family:       gaussian [identity]\n formula:      vote ~ growth\n observations: 16\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 46.3    1.6  \ngrowth       3.0    0.7  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 3.9    0.7   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nOr print the intercept (46.26) and the slope (3.05) of this model.\n\ncoef(M1)\n\n(Intercept)      growth \n  46.300290    3.037234 \n\n\n\n\n\nM2 &lt;- brm(vote ~ growth, data=hibbs)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.026 seconds (Warm-up)\nChain 1:                0.022 seconds (Sampling)\nChain 1:                0.048 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 8e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 2:                0.021 seconds (Sampling)\nChain 2:                0.051 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 3:                0.023 seconds (Sampling)\nChain 3:                0.051 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 4:                0.023 seconds (Sampling)\nChain 4:                0.053 seconds (Total)\nChain 4: \n\n\nM2 is set on your computer and you can give a summary of this regression model.\n\nM2 &lt;-\n  brm(data = hibbs,\n      vote ~ growth,\n      cores = 4, chains = 4, iter = 2000,\n      seed = 123)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nM2\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: vote ~ growth \n   Data: hibbs (Number of observations: 16) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    46.15      1.85    42.36    49.74 1.00     2993     2088\ngrowth        3.07      0.80     1.49     4.73 1.00     2757     1903\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.07      0.83     2.84     6.06 1.00     2683     1955\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nNow add line to plot.\n\nggplot(data = hibbs,\n       mapping = aes(x = growth, y = vote)) +\n  geom_point() +\n  geom_abline(slope     = coef(M1)[[\"growth\"]],\n              intercept = coef(M1)[[\"(Intercept)\"]]) \n\n\n\n\n\nPredicting elections from the economy 1952-2016\n\n\n\nWe also looked at the peacekeeping data (1.3). First open the data.\n\npeace_df &lt;- read_csv(\"~/Desktop/WERK/Gelman/ROS-book/ROS-book/ROS-Examples-master/Peacekeeping/data/minidata.csv\")\n\nExplore this dataset now.\n\nglimpse(peace_df)\n\nRows: 2,031\nColumns: 7\n$ ...1            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ cfdate          &lt;dbl&gt; 140, 150, 210, 125, 126, 200, 110, 165, 190, 125, 200,…\n$ faildate        &lt;dbl&gt; 66, 64, 74, 66, 64, 65, 63, 68, 63, 64, 62, 73, 72, 72…\n$ `peacekeepers?` &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ badness         &lt;chr&gt; \"white\", \"black\", \"white\", \"white\", \"white\", \"white\", …\n$ delay           &lt;dbl&gt; 1, 1, 3, 5, 1, 1, 6, 1, 1, 4, 2, 1, 1, 1, 2, 1, 4, 4, …\n$ `censored?`     &lt;dbl&gt; 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, …\n\n\nCreate date measure. It’s actually the same as delay.\n\npeace_df &lt;- peace_df |&gt;\n  mutate(time_diff = (faildate-cfdate)/365)\n\nLet us plot it …\n\n\n# Harrie: not working\n# peace_df |&gt;\n#  ggplot(data = .) +\n#  geom_histogram(mapping = aes(x = delay), bins = 10) +\n#  facet_wrap(~`peacekeepers?`) \n… or put it in a scatterplot.\n\nggplot(data = peace_df) +\n  geom_point(mapping = aes(y = delay,\n                           colour = as.factor(`censored?`),\n                           x = badness,\n                           )) +\n  facet_wrap(~`peacekeepers?`) \n\n\n\n\n\nOutcomes after civil war in countries with and without UN-peacekeepers\n\n\n\nMeans.\n\npeace_df |&gt; \n  group_by(`peacekeepers?`, `censored?`) |&gt; \n  summarise(mean_badness = mean(badness, na.rm = TRUE))\n\n# A tibble: 5 × 3\n# Groups:   peacekeepers? [2]\n  `peacekeepers?` `censored?` mean_badness\n            &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1               0           1           NA\n2               0           2           NA\n3               1           1           NA\n4               1           2           NA\n5               1          NA           NA\n\n\nSimple causal graph for reproducibility of simulated data.\n\nSEED &lt;- 1151\nset.seed(SEED)\nN &lt;- 50\nx &lt;- runif(N, 1, 5)\ny &lt;- rnorm(N, 10 + 3*x, 3)\nx_binary &lt;- ifelse(x&lt;3, 0, 1)\ncausal_df &lt;- data.frame(N, x, y, x_binary)\n\nPlot this.\n\nggplot(data = causal_df) +\n  geom_point(mapping = aes(y = y, x = x)) \n\n\n\n\n\nCausal graph of simulated data"
  },
  {
    "objectID": "03-chapter.html#summary",
    "href": "03-chapter.html#summary",
    "title": "3  Some basic methods in mathematics and probability",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nSimple methods from introductory mathematics and probability have three important roles in regression modelling.\n- Linear algebra and simple probability distributions are the building blocks for elaborate models.\n- It is useful to understand the basic ideas of inference separately from the details of particular class of model.\n- It is often useful in practice to construct quick estimates and comparisons for small parts of a problem - before fitting an elaborate model, or in understanding the output from such a model.\nThis chapter provides a quick review of some of these basic ideas.\nFirst some ideas from algebra are presented:\n- Weighted averages are used to adept to a target population (for eg. the average age of all North American as a weighted average).\n- Vectors are used to represent a collection of numbers and matrices are used to represent a collection of vectors.\n- To use linear regression effectively, you need to understand the algebra and geometry of straight lines, with the intercept and the slope.\n- To use logarithmic and log-log relationships for exponential and power-law growth and decline.\nHere an example of a regression line.\n\n#\nlibrary(tidyverse)\nlibrary(patchwork)\n# Thanks Solomon Kurz \n# set the global plotting theme\ntheme_set(theme_linedraw() +\n            theme(panel.grid = element_blank()))\n\na &lt;- 0\nb &lt;- 1\n\n# left\np1 &lt;-\n  tibble(x = 0:2) %&gt;% \n  mutate(y = a + b * x) %&gt;%\n  \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = 0:2) +\n  scale_y_continuous(breaks = 0:2, labels = c(\"a\", \"a+b\", \"a+2b\")) +\n  labs(subtitle = expression(y==a+bx~(with~b&gt;0)))\n\nb &lt;- -1\n\n# right\np2 &lt;-\n  tibble(x = 0:2) %&gt;% \n  mutate(y = a + b * x) %&gt;%\n  \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = 0:2) +\n  scale_y_continuous(breaks = 0:-2, labels = c(\"a\", \"a+b\", \"a+2b\")) +\n  labs(subtitle = expression(y==a+bx~(with~b&lt;0)))\n\n# combine with patchwork\nlibrary(patchwork)\n\np1 + p2\n\n\n\n\nProbabilistic distributions are used in regression modeling to help to characterize the variation that remains after predicting the average. These distributions allow us to get a handle on how uncertain our predictions are and, additionally, our uncertainty in the estimated parameters of the model. Mean (expected value), variance (mean of squared difference from the mean), and standard deviation (square root of variance) are the basic concepts of probability distributions.\nNormal distribution, binomial distribution, and Poisson distribution and Unclassified probability distributions are types of probability distributions presented here. They will be worked out in detail in the following chapters.\nIn regression we typically model as much of the data variation as possible with a deterministic model, with a probability distribution included to capture the error, or unexplained variation. Distributions can be used to compare using such as the mean, but also to look at shifts in quantiles for example. Probability distributions can also be used for predicting new outcomes."
  },
  {
    "objectID": "01-chapter.html#presentation-1",
    "href": "01-chapter.html#presentation-1",
    "title": "1  Overview",
    "section": "1.3 Presentation",
    "text": "1.3 Presentation\nOn 14-11-2023 Alex Trinidad (University of Cologne and Netherlands Institute for the Study of Crime and Law Enforcement) presented the first chapter of the book Regression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari: Overview. The session was held online via Zoom. Here you can find Alex’ script Trinidad.\nFirst he loaded this package.\n\nlibrary(tidyverse)\n\n\nRegression to predict\n\nHow can we predict presidential vote share using economy growth? For this he loaded the ROS-data.\n\nelections_data &lt;- read.csv(url(\"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectionsEconomy/data/hibbs.dat\"), sep = \"\")\n\nThis another way to load these data.\n\nremotes::install_github(\"avehtari/ROS-Examples\", subdir = \"rpackage\")\n\nSkipping install of 'rosdata' from a github remote, the SHA1 (a049a104) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nelections_data &lt;- rosdata::hibbs\n\nLet us first explore economy growth.\n\nglimpse(elections_data)\n\nRows: 16\nColumns: 5\n$ year                &lt;int&gt; 1952, 1956, 1960, 1964, 1968, 1972, 1976, 1980, 19…\n$ growth              &lt;dbl&gt; 2.40, 2.89, 0.85, 4.21, 3.02, 3.62, 1.08, -0.39, 3…\n$ vote                &lt;dbl&gt; 44.60, 57.76, 49.91, 61.34, 49.60, 61.79, 48.95, 4…\n$ inc_party_candidate &lt;chr&gt; \"Stevenson\", \"Eisenhower\", \"Nixon\", \"Johnson\", \"Hu…\n$ other_candidate     &lt;chr&gt; \"Eisenhower\", \"Stevenson\", \"Kennedy\", \"Goldwater\",…\n\n\nTry the view-function yourself.\n\n# View(elections_data)\n\nUse visualization to understand the data.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth))\n\n\n\n\n\nPredicting elections from the economy 1952-2016\n\n\n\nAdd a line to the plot.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth)) +\n  geom_smooth(aes(x = year, y = growth), se = FALSE)\n\n\n\n\n\nPredicting elections from the economy 1952-2016 with line\n\n\n\nAdd the CI around the line.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth)) +\n  geom_smooth(aes(x = year, y = growth), se = TRUE)\n\n\n\n\n\nPredicting elections from the economy 1952-2016 with line and confidence interval\n\n\n\nFit ols-regression to obtain the predicted values.\n\nmod1 &lt;- lm(vote ~ growth, data = elections_data)\n\nSummarize the regression results.\n\nsummary(mod1)\n\n\nCall:\nlm(formula = vote ~ growth, data = elections_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9929 -0.6674  0.2556  2.3225  5.3094 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  46.2476     1.6219  28.514 8.41e-14 ***\ngrowth        3.0605     0.6963   4.396  0.00061 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.763 on 14 degrees of freedom\nMultiple R-squared:  0.5798,    Adjusted R-squared:  0.5498 \nF-statistic: 19.32 on 1 and 14 DF,  p-value: 0.00061\n\n\nPlot the predicted values.\n\nplot(elections_data$growth, elections_data$vote, xlab = \"Economic Growth\", ylab = \"Vote Share\")\nabline(coef(mod1), col = \"red\")\n\n\n\n\n\nPredicting elections from the economy 1952-2016 with line\n\n\n\nPredicted values with ggplot.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = growth, y = vote)) +\n  geom_abline(intercept = mod1[[1]][[1]], slope = mod1[[1]][[2]], color = \"red\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Data and linear fit\",\n       x = \"Average recent growth in personal income\",\n       y = \"Incumbent party's vote share\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nPredicted values with ggplot and geom_smooth.\n\nggplot(data = elections_data) +\n  geom_point(aes(x = growth, y = vote)) +\n  geom_smooth(method = \"lm\", aes(x = growth, y = vote), color = \"blue\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Data and linear fit\",\n       x = \"Average recent growth in personal income\",\n       y = \"Incumbent party's vote share\")\n\n\n\n\n\nSketching regression\n\nOriginal \\(y = 46.3 + 3.0 x\\). Explore the descriptive stats to get some parameters based on the observed data.\n\nelections_data |&gt; \n  summarise(min_growth = min(growth),\n            max_growth = max(growth),\n            mean_growth = mean(growth),\n            sd_growth = sd(growth),\n            min_vote = min(vote),\n            max_vote = max(vote),\n            mean_vote = mean(vote),\n            sd_vote = sd(vote))\n\n  min_growth max_growth mean_growth sd_growth min_vote max_vote mean_vote\n1      -0.39       4.21      1.8975  1.395538     44.6    61.79    52.055\n   sd_vote\n1 5.608951\n\n\nSimulating the data (technique often used in this book).\n\nset.seed(123)\nN &lt;- 16\nsimu_growth &lt;- runif(N, -0.39, 4)\nsimu_vote &lt;- rnorm(N, 46.2476  + 3.0605*simu_growth, 3.763)\nsimu_elections &lt;- data.frame(N,simu_growth, simu_vote)\n\nModel the simulated data.\n\nsimu_mod &lt;- lm(simu_vote ~ simu_growth, data = simu_elections)\n\nSummarize the model.\n\nsummary(simu_mod)\n\n\nCall:\nlm(formula = simu_vote ~ simu_growth, data = simu_elections)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.355 -1.513 -0.488  1.839  5.962 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  43.6769     1.7558  24.876 5.49e-13 ***\nsimu_growth   4.0052     0.6948   5.765 4.90e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.448 on 14 degrees of freedom\nMultiple R-squared:  0.7036,    Adjusted R-squared:  0.6824 \nF-statistic: 33.23 on 1 and 14 DF,  p-value: 4.896e-05\n\n\nPlot the simulated data using base graphics.\n\n# Base graphic\nplot(simu_elections$simu_growth, simu_elections$simu_vote, xlab = \"Simulated Economic Growth\", ylab = \"Simulated Vote Share\")\nabline(coef(simu_mod), col = \"blue\")\n\n\n\n\n\nSimulated Data and linear fit\n\n\n\nPlot the samen using ggplot version.\n\nggplot(data = simu_elections) +\n  geom_point(aes(x = simu_growth, y = simu_vote)) +\n  geom_smooth(method = \"lm\", aes(x = simu_growth, y = simu_vote), color = \"blue\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Simulated Data and linear fit\",\n       x = \"Simulated Average recent growth in personal income\",\n       y = \"Simulated Incumbent party's vote share\")\n\n\n\n\nExercise 1.2(a) from ROS for sketching a regression model and data.\n\n\\(y = 30 + 10x\\) (residual \\(sd 3.9\\)) & values of X ranging from \\(0-4\\)\n\nDefine the data.\n\nset.seed(123)\nN &lt;- 50\nx &lt;- runif(N, 0, 4)\ny &lt;- rnorm(N, 30 + 10*x, 3.9)\ndata &lt;- data.frame(N, x, y)\n\nModel the data.\n\nlm_a &lt;- lm(y ~ x, data)\n\nPlot the data.\n\nplot(data$x, data$y, xlab = \"X Value\", ylab = \"Y value\")\nabline(coef(lm_a), col = \"red\", size = 1)\n\nWarning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): \"size\" is\nnot a graphical parameter\n\n\n\n\n\n\nExercise 1.2 from ROS\n\n\n\n\n\\(y = 30 + 10x\\) (residual \\(sd 10\\)) & values of X ranging from \\(0-4\\).\n\nDefine the data.\n\nset.seed(123)\nN &lt;- 50\nx &lt;- runif(N, 0, 4)\ny &lt;- rnorm(N, 30 + 10*x, 10)\ndata &lt;- data.frame(N, x, y)\n\nModel it.\n\nlm_b &lt;- lm(y ~ x, data)\n\nPlot it.\n\nplot(data$x, data$y, xlab = \"X Value\", ylab = \"Y value\")\nabline(coef(lm_b), col = \"blue\")\n\n\n\n\n\nContinuous predictor\n\n\n\nNow simulate a binary predictor example from the Aki Vehtari GH\nSee Figure 1.5 (page 10).\n\nset.seed(1411)\nN &lt;- 50\nx &lt;- runif(N, 0, 4)\ny &lt;- rnorm(N, 30 + 10*x, 10)\nx_binary &lt;- ifelse(x &lt; 3, 0, 1)\ndata_simu &lt;- data.frame(N, x, y, x_binary)\n\nModel it.\n\nlm_binary &lt;- lm(y ~ x_binary, data = data_simu)\n\nSummarize the model.\n\nsummary(lm_binary)\n\n\nCall:\nlm(formula = y ~ x_binary, data = data_simu)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-27.2063  -8.5257   0.5297   9.3644  27.8011 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   45.812      2.296  19.953  &lt; 2e-16 ***\nx_binary      19.033      3.827   4.974 8.81e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.99 on 48 degrees of freedom\nMultiple R-squared:  0.3401,    Adjusted R-squared:  0.3264 \nF-statistic: 24.74 on 1 and 48 DF,  p-value: 8.813e-06\n\n\nPlot the relationship.\n\nggplot(data = data_simu) +\n  geom_point(aes(x = x_binary, y = y)) +\n  geom_abline(intercept = lm_binary[[1]][[1]], slope = lm_binary[[1]][[2]],\n              color = \"blue\", size = 1) +\n  labs(y = \"Crime reduction\", \n       x =  NULL) +\n  scale_x_continuous(breaks = c(0,1),\n                     labels = c(\"Control\", \"Treatment\")) +\n  annotate(geom = \"text\", x = 0.50, y = 40,\n           label = paste(\"Estimated treatment effect is\\nslope of fitted line: \",\n                         round(lm_binary[[1]][[2]], digits = 2)))\n\n\n\n\n\nBinary predictor\n\n\n\nNon-linear relationship\n\nset.seed(1411)\nx &lt;- runif(N, 1, 7)\ny &lt;- rnorm(N, 7 + 30*exp(-x), 2)\ndata_simu$y &lt;- y\n\nFit the model.\n\nlm_nonlinear &lt;- lm(y ~ x, data = data_simu)\n\nSummarize the model.\n\nsummary(lm_nonlinear)\n\n\nCall:\nlm(formula = y ~ x, data = data_simu)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0484 -1.4874 -0.0243  1.7868  4.4113 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13.1516     0.6188  21.253  &lt; 2e-16 ***\nx            -1.8761     0.2476  -7.579  9.6e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.21 on 48 degrees of freedom\nMultiple R-squared:  0.5447,    Adjusted R-squared:  0.5353 \nF-statistic: 57.43 on 1 and 48 DF,  p-value: 9.599e-10\n\n\nPlot the model outcome.\n\nggplot(data = data_simu) +\n  geom_point(aes(x = x, y = y)) +\n  geom_smooth(method = \"loess\", aes(x = x, y = y), color = \"blue\", size = 1, se = FALSE) +\n  labs(y = \"Theft counts per hour\", \n       x =  \"Hours of foot patrol\")  \n\n\n\n\n\nNon-linear relationship"
  },
  {
    "objectID": "03-chapter.html#presentation",
    "href": "03-chapter.html#presentation",
    "title": "3  Some basic methods in mathematics and probability",
    "section": "3.2 Presentation",
    "text": "3.2 Presentation"
  },
  {
    "objectID": "04-chapter.html#summary",
    "href": "04-chapter.html#summary",
    "title": "4  Statistical Inference",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nStatistical inference can be formulated as a set of operation on data that yield estimates and uncertainty statements about predictions and parameters of some underlying process of population. From a mathematical standpoint, these probabilistic uncertainty statements are derived based on some assumed probability model for observed data. In this chapter:\n- the basics of probability models are sketched (estimation, bias, and variance);\n- the interpretation of statistical inferences and statistical errors in applied work;\n- the theme of uncertainty in statistical inference is introduced;\n- a mistake to use hypothesis tests or statistical significance to attribute certainty from noisy data are discussed.\nStatistical inference is used to learn from incomplete or imperfect data.\n- In the sampling model we are for example interested in learning some characteristics of a population from a sample.\n- In the measurement model we are interested in learning about the underlying pattern or law.\n- Model error refers to the inevitable imperferction of the model.\n\nSome definitions are given. The sampling distribution is the set of possible datasets that could have been observed if the data collection process had been re-done, along with the probabilities of these possible values. It is said to be a generative model in that it represents a random process which, if known, could generate a new dataset. Parameters are the unknown numbers that determine a statistical model, e.g. \\(y_i=a+bx_i+\\epsilon_i\\) in which the errors \\(\\epsilon_I\\) are normally distributed with mean 0 and standard deviation \\(\\sigma\\). Thre parameters \\(a\\) and \\(b\\) are called coeffients and \\(\\sigma\\) is a scale or variance parameter.\nThe standard error (\\(\\sigma/ \\sqrt{n}\\)) is the estimated standard deviation of an estimate and can give us a sense of our uncertainty about the quantity of interest. The confidence interval represents a range of values of a parameter or quantity of that are roughly consistent with the data, given the assumed sampling distribution.\n\n\n\n\n\nBias and unmodeled uncertainty are also discussed. Roughly speaking, an estimate is unbiased if it is correct on average. Take into account that random samples and randomized experiments are imperfect in reality, and any approximations become even more tenuous when applied to observational data. Also, survey respondents are not balles drawn from an ure, and the probabilties in the “urn” are changing over time. So, improve data collection, expand the model, and increase stated uncertainty.\nPerforming data analysis is the possibility of mistakenly coming to strong conclusions that do not reflect real patterns in the underlying population. Statistical theories of hypothesis testing and error analysis have been developed to quantify these possibilities in the context of inference and decision making.\nA commonly used decision rule that we do not recommend is to consider a result as stable or real if it is “statistically significant” and to taken “non-statistically” results to be noisy and to be treated with skepticism. The concepts of hypothesis testing are reviewed with a simple hypothetical example. Estimate, standard error, degrees of freedom, null and alternative hypotheses and p-value, as well as the general formulation, confidence intervals to compare results, and Type 1 and Type 2-errors, important in conventional hypthesis testing, are presented.\nThey present the problems with the concept of statistical significance (some examples are given):\n\nStatistical significance is not the same as practical significance;\n\nNon-significance is not the same as zero;\n\nThe difference between “significant” and “non-significant” is not itself statistically significant;\n\nStatistical significance can be attained by multiple comparisons or multiple potential comparisons;\n\nThe statistical significant estimates tend to be overestimated;\n\n\nIn this book they try to move beyond hypothesis testing. The most important aspect of their statistical method is its ability to incorporate more information into the analysis. General rules are:\n- Analyse all your data;\n- Present all your comparisons;\n- Make your data public.\nBayesian methods can reduce now-common pattern of the researchers getting jerked around by noise patterns that happen to exceed the statistical significance threshold. We can move forward by accepting uncertainty and embracing variation."
  },
  {
    "objectID": "04-chapter.html#presentation",
    "href": "04-chapter.html#presentation",
    "title": "4  Statistical Inference",
    "section": "4.2 Presentation",
    "text": "4.2 Presentation"
  },
  {
    "objectID": "05-chapter.html#summary",
    "href": "05-chapter.html#summary",
    "title": "5  Simulation",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nIn this book Regression and Other Stories and in practice Gelman et all. use simulation for different reasons:\n- use probability models to mimic variation in the world (tools of simulation can help us better to understand how this variation plays out);\n- use simulation to approximate the sampling distribution of data and propagate this to the sampling distribution of statistical estimates and procedures;\n- regression models are not deterministic; they produce probabilisitc predictions. Simulation is the most convenient and general way to represent uncertainties in forecasts.\nBecause of this, chapter 5 introduces simulations (basic ideas and tools to perform it in R).\nDifferent examples are given of simulations of discrete, continuous, and mixed discrete/ continuous models are presented.\nHere is an discrete model example: Across the world the probability a baby will be born a girl is about \\(48.8\\%\\), with the probability of a boy then being about \\(51.2\\%\\). If you wanted to get a sense of how many girls you’d expect out of 400 births, you could simulate using the rbinom() function.\n\n\nset.seed(5)\n# use set seed here because we want to get the same results every time we run the code\n\nrbinom(n = 1, size = 400, prob = 0.488)\n\n[1] 188\n\nGraph results of 1000 simulations:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# this is from Solomon Kurz's book, he avoids working with loops\n\nset.seed(5)\n# set the global plotting theme\ntheme_set(theme_linedraw() +\n            theme(panel.grid = element_blank()))\n\n# set the seed\nset.seed(5)\n\n# simulate\ntibble(girls = rbinom(n = 1000, size = 400, prob = .488)) %&gt;% \n  \n  # plot\n  ggplot(aes(x = girls)) +\n  geom_histogram(binwidth = 5) +\n  scale_x_continuous(\"# girls out of 400\", breaks = 7:9 * 25) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05)))\n\n\n\n\nThere are many settings where it makes sense to use a set of simulation draws to summarize a distribution, which can represent a simulation from a probability model, a prediction for a future outcome from a fitted regression, or uncertainty about parameters in a fitted model.\nHere the tidyverse oriented flow, as Kurz defined it.\n\nset.seed(5)\n\ntibble(z = rnorm(1e4, mean = 5, sd = 2)) %&gt;% \n  summarise(mean   = mean(z),\n            median = median(z),\n            sd     = sd(z),\n            mad_sd = mad(z))\n\n# A tibble: 1 × 4\n   mean median    sd mad_sd\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1  5.00   4.99  2.02   2.03\n\n\nIn this book they use the Bayesian simulation approach for regression models. Bootstrap is also a simulation approach, but it is not as general as Bayesian simulation. It is very general, any estimate can be simulated, and it is easy to use with complex models. But it has limitations (for example leading to an answer with an inappropriately high level of certainty). This method is not used in the book."
  },
  {
    "objectID": "05-chapter.html#presentation",
    "href": "05-chapter.html#presentation",
    "title": "5  Simulation",
    "section": "5.2 Presentation",
    "text": "5.2 Presentation"
  }
]