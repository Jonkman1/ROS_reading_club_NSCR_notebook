{"title":"Some basic methods in mathematics and probability","markdown":{"yaml":{"title":"Some basic methods in mathematics and probability","author":" X Y (Chair)","format":{"html":{"grid":{"margin-width":"350px"}},"pdf":"default"},"reference-location":"margin","citation-location":"margin"},"headingText":"Summary","containsRefs":false,"markdown":"\n\n\nSimple methods from introductory mathematics and probability have three important roles in regression modelling. \\\n- Linear algebra and simple probability distributions are the building blocks for elaborate models. \\\n- It is useful to understand the basic ideas of inference separately from the details of particular class of model. \\\n- It is often useful in practice to construct quick estimates and comparisons for small parts of a problem - before fitting an elaborate model, or in understanding the output from such a model.\\\nThis chapter provides a quick review of some of these basic ideas.\n\nFirst some ideas from algebra are presented:\\\n- *Weighted averages* are used to adept to a target population (for eg. the average age of all North American as a weighted average). \\\n- *Vectors* are used to represent a collection of numbers and *matrices* are used to represent a collection of vectors. \\\n- To use linear regression effectively, you need to understand the algebra and geometry of straight *lines*, with the intercept and the slope. \\\n- To use *logarithmic* and *log-log relationships* for exponential and power-law growth and decline.\n\nHere an example of a regression line.\n\n```{r, message=FALSE, warning=FALSE}\n#| label: fig:regression-lines\n#| column: margin\n#\nlibrary(tidyverse)\nlibrary(patchwork)\n# Thanks Solomon Kurz \n# set the global plotting theme\ntheme_set(theme_linedraw() +\n            theme(panel.grid = element_blank()))\n\na <- 0\nb <- 1\n\n# left\np1 <-\n  tibble(x = 0:2) %>% \n  mutate(y = a + b * x) %>%\n  \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = 0:2) +\n  scale_y_continuous(breaks = 0:2, labels = c(\"a\", \"a+b\", \"a+2b\")) +\n  labs(subtitle = expression(y==a+bx~(with~b>0)))\n\nb <- -1\n\n# right\np2 <-\n  tibble(x = 0:2) %>% \n  mutate(y = a + b * x) %>%\n  \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = 0:2) +\n  scale_y_continuous(breaks = 0:-2, labels = c(\"a\", \"a+b\", \"a+2b\")) +\n  labs(subtitle = expression(y==a+bx~(with~b<0)))\n\n# combine with patchwork\nlibrary(patchwork)\n\np1 + p2\n```\n\n\n\n*Probabilistic distributions* are used in regression modeling to help to characterize the variation that remains *after* predicting the average. These distributions allow us to get a handle on how uncertain our predictions are and, additionally, our uncertainty in the estimated parameters of the model. *Mean* (expected value), *variance* (mean of squared difference from the mean), and *standard deviation* (square root of variance) are the basic concepts of probability distributions.\n\nNormal distribution, binomial distribution, and Poisson distribution and Unclassified probability distributions are types of probability distributions presented here. They will be worked out in detail in the following chapters.\n\nIn regression we typically model as much of the data variation as possible with a *deterministic* model, with a probability distribution included to capture the *error*, or unexplained variation. Distributions can be used to compare using such as the mean, but also to look at shifts in quantiles for example. Probability distributions can also be used for predicting new outcomes.\n\n## Presentation\nRemark: Following part has to be designed further (Harrie)\n\nWim Bernasco chaired the practice part of chapter 3 on  Tuesday 23rd of January 2024. His original script you can find [here](url: https://github.com/langtonhugh/regression_book_nscr/tree/main/scripts/chair)\n\nOpen two libraries first.\n\n```{r}\n# For tidy data processing\nlibrary(tidyverse)\n# Access data from \"Regression and other stories\"\nlibrary(rosdata)\n```\n\nWim looked at the excercises of chapter 3. He start with **excercise 3.1: Weighted averages**.\n\nYou often encounter weighted averages when you work with aggregated data, such as averages in subpopulations using\n\n*Groups*: the categories that are being weighted (here four age groups) \\\n*Shares*: the proportions of each group in the sample \\\n*Means* : the means values of 'something' in each group \\\n\nWe first calculate total sample size by summing the four categories:\n\n```{r}\n#| label: weighted-averages\nn_sample = 200 + 250 + 300 + 250\nn_sample\n```\n\nNext we multiply the means of the groups with their share in the sample.\n\n\n```{r}\n#| label: share-in-sample\n50 * 200/n_sample + \n60 * 250/n_sample +\n40 * 300/n_sample +\n30 * 250/n_sample\n```\n\nWe can also do this more systematically and in a tidy way. Set up a little table that holds the relevant input data.\n\n```{r}\n#| label: tax-support\ntax_support <- \n  tibble(age_class        = c(\"18-29\", \"30-44\", \"45-64\", \"65+\"),\n         tax_support      = c(    50 ,    60 ,     40,   30),\n         sample_frequency = c(   200 ,   250 ,    300,  250))\ntax_support\n```\n\n::: {.callout-note}\nNote. I stumbled on the tribble function from the tibble package. This\nallows inputting observations in rows and variables in columns. Nice for small inline dataframes.\n:::\n\n```{r}\n#| label: tax-support-alt\ntax_support_alt <-\n  tribble(\n    ~age_class, ~tax_support, ~sample_frequency,\n       \"18-29\",   50,   200,\n       \"30-44\",   60,   250,\n       \"45-64\",   40,   300,\n       \"65+\"  ,   30,   250\n)\ntax_support_alt\n\n```\n\nNext we calculate the weighted average. \n- We first calculate the share in the sample of each group \\\n- Next multiply shares with percentage tax support \\\n- And finally sum over the four groups \\\n\n```{r}\n#| label: tax-summarize\ntax_support |>\n  mutate(sample_share = sample_frequency / sum(sample_frequency),\n         sample_tax_support = tax_support * sample_share) |>\n  summarize(sample_tax_support = sum(sample_tax_support))\n```\n\nThen he looked at **excercise 3.2**. He was not sure whether he fully understood this exercise. He assumed the idea is to let us think about how other age distributions would affect the level of support. Thus, if the tax support in the four age groups is given, which age group distributions would yield an overall support percentage of 40?  An obvious but unrealistic distribution would consist of only people aged 30-44, because among this group the support is precisely $40\\%$. \n\nMathematically, the situation can be described with 2 equations with 4 unknowns.\n\n The first equation would just constrain the four weights to sum to 1.\n \n $$(eq 1) wght_18 + wght_30 + wght_45 + wght_65  = 1$$\n \n The second equation would constrain the weighted average to be 40\n \n $$(eq 2) wght_18 * 50 + wght_30 * 60 + wght_45 * 40 + wght_65 * 30 = 40$$\n \nTo find a deterministic solution, we need to assign values to two unknowns\n\nIf we fix the shares of the least extreme age classes \"18-29\" and \"45-64\"\nto their original values (.2 and .3), we should be able to get an overall tax support of $40\\%$ by finding a suitable mix of age group 30-44 (supportlevel $60\\%$) and age group 65+ (support level $30\\%$).\n\n\n$$(eq 1) wght_30 + wght_65  = .5$$\n$$(eq 2) wght_30 * 60 + wght_65 * 30 = 40 - 22 = 18$$\n\n$$(eq 1) wght_30 = .5 - wght_65$$\n$$(eq 2) (.5 - wght_65) * 60 + weight_65 * 30 = 18$$\n\n$$(eq 1) wght_30 = .5 - wght_65$$\n$$(eq 2) 30 - wght_65 * 60 + weight_65 * 30 = 18$$\n\n$$(eq 1) wght_30 = .5 - wght_65$$\n$$(eq 2) 30 - 30 * wght_65 = 18$$\n\n$$(eq 1) wght_30 = .5 - wght_65$$\n$$(eq 2) 30 * wght_65 = 12$$\n\n$$(eq 1) wght_30 = .5 - 0.4 = .1$$\n$$(eq 2) wght_65 = 12/30 = .4$$\n\nSo, we get\n\n```{r}\n#| label: simple-calc\n   50 * .2 + # this was fixed\n   60 * .1 + # this was calculated\n   40 * .3 + # this was fixed\n   30 * .4  # this was calculated\n```\n\nSo it worked.\n\nNow he looked at **excercise 3.3 Plotting a line**.    \n\n\n```\ncurve(expr = x,\n      from = 0,\n      to   = 20)\n```\nThis gave an Error in x(x): object 'x' not found. Wim thinks the curve function needs at least one existing function name (`log`, `exp`, `sqrt`, `sin`, `cos`, ...) or math symbol (`+` `-` `/` `*` `^` ...) beyond the 'x'\n\n\nHere are some lines\n\n```{r}\n#| label: curve1\n#| fig.cap: A line\n#| column: margin\ncurve(expr = x*1,\n      from = 0,\n      to   = 20)\n```\n\nA flat line\n```{r}\n#| label: curve2\n#| fig.cap: Another flat line\n#| column: margin\ncurve(expr = 1 + 0 * x,\n      from = 0,\n      to   = 20)\n```\n\nAnother flat line\n\n```{r}\n#| label: curve3\n#| fig.cap: Another flat line\n#| column: margin\ncurve(expr = 1 + 5 * x,\n      from = 0,\n      to   = 20)\n```\n\nA log line\n\n```{r}\n#| label: curve4\n#| fig.cap: A log line\n#| column: margin\ncurve(expr = log(1 + x),\n      from = 0,\n      to   = 20)\n```\n\nAn exponential line\n\n```{r}\n#| label: curve5\n#| fig.cap: An exponential line\n#| column: margin\ncurve(expr = exp( x),\n      from = 0,\n      to   = 20)\n```\n\nA square root line\n\n```{r}\n#| label: curve6\n#| fig.cap: A square root line\n#| column: margin\ncurve(expr = sin(sqrt(x)),\n      from = 0,\n      to   = 2000)\n\n```\n\n\nMile record data\n\nOpen dataset from rosdata.\n\n```{r}\n#| label: mile-data\ndata(\"mile\")\nglimpse(mile)\n```\n\nHe guesses that `year` is a time variable that include $monthnumber/12$ as decimals. Let us check the first two cases.\n\n```{r}\n#| label: year-plus\n1913 + 5/12\n\n1915 + 7/12\n```\n\nHe thinks `seconds` equals $min * 60 + seconds$. Let us check the first two cases.\n\n```{r}\n#| label: seconds-plus\n4 * 60 + 14.4\n\n4 * 60 + 12.6\n```\n\nCorrect.\n\nLet us plot data using base R.\n\n```{r}\n#| label: plot-mile\n#| fig.cap: Mile record times\n#| column: margin\nplot(x = mile$year,\n     y = mile$seconds,\n     xlab = \"Year\",\n     ylab = \"Seconds\",\n     main = \"Mile record times\")\n\n```\n\nLet us plot data using ggplot2.\n\n```{r}\n#| label: ggplot-mile-ggplot\n#| fig.cap: Mile record times\n#| column: margin\nggplot(data = mile,\n       mapping = aes(x = year,\n                     y = seconds)) +\n  geom_point() +\n  labs(x = \"Year\",\n       y = \"Seconds\",\n       title = \"Mile record times\")\n\n```\n\nEstimate the line \n\n```{r}\n#| label: ggplot-mile-lm\nfit <- lm(formula = seconds ~ year,\n          data = mile)\nsummary(fit)\n```\n\n```{r}\n#| label: ggplot-mile-int-slope\nfitted_intercept <- fit$coeff[\"(Intercept)\"]\nfitted_slope <- fit$coeff[\"year\"]\n```\n\nggplot (add the estimated regression line).\n\n```{r}\n#| label: ggplot-mile-ggplot2\n#| column: margin\nggplot(data = mile,\n       mapping = aes(x = year,\n                     y = seconds)) +\n  geom_point() +\n  labs(x = \"Year\",\n       y = \"Seconds\",\n       title = \"Mile record times\") +\n  geom_abline(intercept = fitted_intercept,\n              slope = fitted_slope,\n              color = \"red\")\n\n```\n\nNow **Exercise 3.3 Probability distributions**.\n\nMake sure we all get the same numbers and create just 10 random numbers.\n\n```{r}\n#| label: standard-normal-10\nset.seed(123456789)\nstandard_normal_10 <- rnorm(n = 10, mean = 0, sd = 1)\nstandard_normal_10\n```\n\nNow create 1000 random numbers.\n```{r}\n#| label: standard-normal-1000\nstandard_normal_1000 <- rnorm(n = 1000, mean = 0, sd = 1)\nstandard_normal_1000\n\n```\n\nhe most basic histogram (single variable distribution) is created with the `hist` function.\n\n```{r}\n#| label: standard-normal-hist\n#| column: margin\nhist(x = standard_normal_1000,\n     breaks = 20,\n     col = \"lightblue\",\n     main = \"Histogram of 1000 standard normal random numbers\",\n     xlab = \"Standard normal random numbers\")\n\n```\n\nDensity for the standard normal (mean = 0, sd = 1)\n\n\n\n```{r}\n#| label: standard-normal-density\nx_axis <-\n  seq(min(standard_normal_1000), \n      max(standard_normal_1000), \n      length = 40)\n\ndensity <- dnorm(x_axis, \n                 mean = mean(standard_normal_1000), \n                 sd = sd(standard_normal_1000))\n```\n\nBy assumption (theoretical distibution has mean 0.00000 and sd 0.00000).\n\n```{r}\n#| label: standard-normal-density-theoretical\n#| column: margin\ndensity_theoretical <- dnorm(x_axis, mean = 0, sd = 1)\n\nplot(x = x_axis, y = density, \n     type = \"l\",\n     col = \"red\",\n     lwd = 2,\n     ylab = \"Density\",\n     main = \"Density of standard normal distribution\")\n\n```\n\nCreating 1000 random numbers\n\n```{r}\n#| label: sd2-normal-1000\nsd2_normal_1000 <- rnorm(n = 1000, mean = 0, sd = 2)\nsd2_normal_1000\n\nx_axis <-\n  seq(min(sd2_normal_1000), \n      max(sd2_normal_1000), \n      length = 40)\n\ndensity <- dnorm(x_axis, mean = mean(sd2_normal_1000), \n                 sd = sd(sd2_normal_1000))\n\n```\n\n```{r}\n#| label: sd2-normal-density-theoretical\n#| column: margin\nplot(x = x_axis, y = density, type = \"l\",\n     col = \"red\", \n     lwd = 2,\n     ylab = \"Density\",\n     main = \"Density of standard normal distribution\")\n```\n   \n**Exercise 3.4**. \n\n```{r}\n#| label: poisson-35-1000\n#| fig.cap: Poisson distribution with lambda = 3.5\n#| column: margin\npoisson_35_1000 <- \n  rpois(n = 1000, lambda = 3.5)\n\npoisson_35_1000 |> hist(breaks = length(unique(poisson_35_1000)))\n```\n\n**Excercise 3.5 Binomial distribution**.\n\n```{r}\n#| label: binom-03-20-1000\n#| fig.cap: Binomial distribution with p = 0.3, n = 20\n#| column: margin\nbinom_03_20_1000 <- \n  rbinom(n = 1000, size = 20, prob = 0.3)\n\nbinom_03_20_1000 |> hist(breaks = length(unique(binom_03_20_1000)))\n```\n\n\n**Exercise 3.6 Linear transformations**.\n\nThe mean must be increased from 35 to 100 by adding 65, the standard deviation must be increased from 10 to 15 by multiplying with 1.5.\n\nTransformation $X' = aX + b$\n$mean(X') = a * mean(X) + b$\n$sd(X') = a * sd(X)$\n\nNow mean(X') = 100, mean(X) = 35, sd(X') = 15, sd(X) = 10\n\n Substituting this:\n   $100 = a * 35 + b$\n   $15 = a * 10$\n\n$100 = 15/10 * 35 + b$\n$  b = 47.5$\n$  a = (100 - b) / 35 = 1.5$\n\ntransformation: $X` = 1.5 * X + 47.5$\n\n```{r}\n#| label: linear-transformation\noriginal_scores = rnorm(n = 1000, mean = 35, sd = 10)\noriginal_scores |> hist(breaks = 10)\n\ntransformed_scores = 1.5 * original_scores + 47.5\ntransformed_scores |> hist(breaks = 10)\n```\n\nNew range:  \\\nLowest  (X=0) is $0 * 1.5 + 47.5 = 47.5$ \\\nHighest (X=50) is $50 * 1.5 + 47.5 = 122.5$\n\nSimple. First multiply to get the standard deviation right.\n\n```{r}\n#| label: linear-transformation-1\ntransformed_1 <- original_scores * 1.5\n# Check that is ~15 now\nsd(transformed_1)\n\n```\n\n\nWhat is the mean after the first transformation?\n\n```{r}\n#| label: linear-transformation-2\nmean(transformed_1)\n```\n\nAdd the difference between the target mean (100) and the current mean\n\n```{r}\n#| label: linear-transformation-3\ntransformed_2 <- transformed_1 + (100 - mean(transformed_1))\ntransformed_2 |> hist(breaks = 10)\n```\n\n```{r}\n#| label: linear-transformation-4\n#| fig.cap: Original scores vs transformed scores\n#| column: margin\nplot(original_scores, transformed_scores, type = \"l\")\n```\n\n  \n**Exercise 3.8 Correlated random variables**\n\n```{r}\n#| label: correlated-random-variables\ncorrelation_hw = .3\nmean_husbands = 69.1\nsd_husbands = 2.9\nmean_wives = 63.7\nsd_wives = 2.7\n```\n\nweighted sum:\n\n```{r}\n#| label: correlated-random-variables-1\n.5 * mean_husbands + .5 * mean_wives\n```\n\nStandard deviation of .5 * husband + .5 wife\n\n```{r}\n#| label: correlated-random-variables-2\nsqrt(.5^2 * sd_husbands + \n       .5^2 * sd_wives + \n       2 * .5 * .5 * correlation_hw)\n```","srcMarkdownNoYaml":"\n\n## Summary\n\nSimple methods from introductory mathematics and probability have three important roles in regression modelling. \\\n- Linear algebra and simple probability distributions are the building blocks for elaborate models. \\\n- It is useful to understand the basic ideas of inference separately from the details of particular class of model. \\\n- It is often useful in practice to construct quick estimates and comparisons for small parts of a problem - before fitting an elaborate model, or in understanding the output from such a model.\\\nThis chapter provides a quick review of some of these basic ideas.\n\nFirst some ideas from algebra are presented:\\\n- *Weighted averages* are used to adept to a target population (for eg. the average age of all North American as a weighted average). \\\n- *Vectors* are used to represent a collection of numbers and *matrices* are used to represent a collection of vectors. \\\n- To use linear regression effectively, you need to understand the algebra and geometry of straight *lines*, with the intercept and the slope. \\\n- To use *logarithmic* and *log-log relationships* for exponential and power-law growth and decline.\n\nHere an example of a regression line.\n\n```{r, message=FALSE, warning=FALSE}\n#| label: fig:regression-lines\n#| column: margin\n#\nlibrary(tidyverse)\nlibrary(patchwork)\n# Thanks Solomon Kurz \n# set the global plotting theme\ntheme_set(theme_linedraw() +\n            theme(panel.grid = element_blank()))\n\na <- 0\nb <- 1\n\n# left\np1 <-\n  tibble(x = 0:2) %>% \n  mutate(y = a + b * x) %>%\n  \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = 0:2) +\n  scale_y_continuous(breaks = 0:2, labels = c(\"a\", \"a+b\", \"a+2b\")) +\n  labs(subtitle = expression(y==a+bx~(with~b>0)))\n\nb <- -1\n\n# right\np2 <-\n  tibble(x = 0:2) %>% \n  mutate(y = a + b * x) %>%\n  \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = 0:2) +\n  scale_y_continuous(breaks = 0:-2, labels = c(\"a\", \"a+b\", \"a+2b\")) +\n  labs(subtitle = expression(y==a+bx~(with~b<0)))\n\n# combine with patchwork\nlibrary(patchwork)\n\np1 + p2\n```\n\n\n\n*Probabilistic distributions* are used in regression modeling to help to characterize the variation that remains *after* predicting the average. These distributions allow us to get a handle on how uncertain our predictions are and, additionally, our uncertainty in the estimated parameters of the model. *Mean* (expected value), *variance* (mean of squared difference from the mean), and *standard deviation* (square root of variance) are the basic concepts of probability distributions.\n\nNormal distribution, binomial distribution, and Poisson distribution and Unclassified probability distributions are types of probability distributions presented here. They will be worked out in detail in the following chapters.\n\nIn regression we typically model as much of the data variation as possible with a *deterministic* model, with a probability distribution included to capture the *error*, or unexplained variation. Distributions can be used to compare using such as the mean, but also to look at shifts in quantiles for example. Probability distributions can also be used for predicting new outcomes.\n\n## Presentation\nRemark: Following part has to be designed further (Harrie)\n\nWim Bernasco chaired the practice part of chapter 3 on  Tuesday 23rd of January 2024. His original script you can find [here](url: https://github.com/langtonhugh/regression_book_nscr/tree/main/scripts/chair)\n\nOpen two libraries first.\n\n```{r}\n# For tidy data processing\nlibrary(tidyverse)\n# Access data from \"Regression and other stories\"\nlibrary(rosdata)\n```\n\nWim looked at the excercises of chapter 3. He start with **excercise 3.1: Weighted averages**.\n\nYou often encounter weighted averages when you work with aggregated data, such as averages in subpopulations using\n\n*Groups*: the categories that are being weighted (here four age groups) \\\n*Shares*: the proportions of each group in the sample \\\n*Means* : the means values of 'something' in each group \\\n\nWe first calculate total sample size by summing the four categories:\n\n```{r}\n#| label: weighted-averages\nn_sample = 200 + 250 + 300 + 250\nn_sample\n```\n\nNext we multiply the means of the groups with their share in the sample.\n\n\n```{r}\n#| label: share-in-sample\n50 * 200/n_sample + \n60 * 250/n_sample +\n40 * 300/n_sample +\n30 * 250/n_sample\n```\n\nWe can also do this more systematically and in a tidy way. Set up a little table that holds the relevant input data.\n\n```{r}\n#| label: tax-support\ntax_support <- \n  tibble(age_class        = c(\"18-29\", \"30-44\", \"45-64\", \"65+\"),\n         tax_support      = c(    50 ,    60 ,     40,   30),\n         sample_frequency = c(   200 ,   250 ,    300,  250))\ntax_support\n```\n\n::: {.callout-note}\nNote. I stumbled on the tribble function from the tibble package. This\nallows inputting observations in rows and variables in columns. Nice for small inline dataframes.\n:::\n\n```{r}\n#| label: tax-support-alt\ntax_support_alt <-\n  tribble(\n    ~age_class, ~tax_support, ~sample_frequency,\n       \"18-29\",   50,   200,\n       \"30-44\",   60,   250,\n       \"45-64\",   40,   300,\n       \"65+\"  ,   30,   250\n)\ntax_support_alt\n\n```\n\nNext we calculate the weighted average. \n- We first calculate the share in the sample of each group \\\n- Next multiply shares with percentage tax support \\\n- And finally sum over the four groups \\\n\n```{r}\n#| label: tax-summarize\ntax_support |>\n  mutate(sample_share = sample_frequency / sum(sample_frequency),\n         sample_tax_support = tax_support * sample_share) |>\n  summarize(sample_tax_support = sum(sample_tax_support))\n```\n\nThen he looked at **excercise 3.2**. He was not sure whether he fully understood this exercise. He assumed the idea is to let us think about how other age distributions would affect the level of support. Thus, if the tax support in the four age groups is given, which age group distributions would yield an overall support percentage of 40?  An obvious but unrealistic distribution would consist of only people aged 30-44, because among this group the support is precisely $40\\%$. \n\nMathematically, the situation can be described with 2 equations with 4 unknowns.\n\n The first equation would just constrain the four weights to sum to 1.\n \n $$(eq 1) wght_18 + wght_30 + wght_45 + wght_65  = 1$$\n \n The second equation would constrain the weighted average to be 40\n \n $$(eq 2) wght_18 * 50 + wght_30 * 60 + wght_45 * 40 + wght_65 * 30 = 40$$\n \nTo find a deterministic solution, we need to assign values to two unknowns\n\nIf we fix the shares of the least extreme age classes \"18-29\" and \"45-64\"\nto their original values (.2 and .3), we should be able to get an overall tax support of $40\\%$ by finding a suitable mix of age group 30-44 (supportlevel $60\\%$) and age group 65+ (support level $30\\%$).\n\n\n$$(eq 1) wght_30 + wght_65  = .5$$\n$$(eq 2) wght_30 * 60 + wght_65 * 30 = 40 - 22 = 18$$\n\n$$(eq 1) wght_30 = .5 - wght_65$$\n$$(eq 2) (.5 - wght_65) * 60 + weight_65 * 30 = 18$$\n\n$$(eq 1) wght_30 = .5 - wght_65$$\n$$(eq 2) 30 - wght_65 * 60 + weight_65 * 30 = 18$$\n\n$$(eq 1) wght_30 = .5 - wght_65$$\n$$(eq 2) 30 - 30 * wght_65 = 18$$\n\n$$(eq 1) wght_30 = .5 - wght_65$$\n$$(eq 2) 30 * wght_65 = 12$$\n\n$$(eq 1) wght_30 = .5 - 0.4 = .1$$\n$$(eq 2) wght_65 = 12/30 = .4$$\n\nSo, we get\n\n```{r}\n#| label: simple-calc\n   50 * .2 + # this was fixed\n   60 * .1 + # this was calculated\n   40 * .3 + # this was fixed\n   30 * .4  # this was calculated\n```\n\nSo it worked.\n\nNow he looked at **excercise 3.3 Plotting a line**.    \n\n\n```\ncurve(expr = x,\n      from = 0,\n      to   = 20)\n```\nThis gave an Error in x(x): object 'x' not found. Wim thinks the curve function needs at least one existing function name (`log`, `exp`, `sqrt`, `sin`, `cos`, ...) or math symbol (`+` `-` `/` `*` `^` ...) beyond the 'x'\n\n\nHere are some lines\n\n```{r}\n#| label: curve1\n#| fig.cap: A line\n#| column: margin\ncurve(expr = x*1,\n      from = 0,\n      to   = 20)\n```\n\nA flat line\n```{r}\n#| label: curve2\n#| fig.cap: Another flat line\n#| column: margin\ncurve(expr = 1 + 0 * x,\n      from = 0,\n      to   = 20)\n```\n\nAnother flat line\n\n```{r}\n#| label: curve3\n#| fig.cap: Another flat line\n#| column: margin\ncurve(expr = 1 + 5 * x,\n      from = 0,\n      to   = 20)\n```\n\nA log line\n\n```{r}\n#| label: curve4\n#| fig.cap: A log line\n#| column: margin\ncurve(expr = log(1 + x),\n      from = 0,\n      to   = 20)\n```\n\nAn exponential line\n\n```{r}\n#| label: curve5\n#| fig.cap: An exponential line\n#| column: margin\ncurve(expr = exp( x),\n      from = 0,\n      to   = 20)\n```\n\nA square root line\n\n```{r}\n#| label: curve6\n#| fig.cap: A square root line\n#| column: margin\ncurve(expr = sin(sqrt(x)),\n      from = 0,\n      to   = 2000)\n\n```\n\n\nMile record data\n\nOpen dataset from rosdata.\n\n```{r}\n#| label: mile-data\ndata(\"mile\")\nglimpse(mile)\n```\n\nHe guesses that `year` is a time variable that include $monthnumber/12$ as decimals. Let us check the first two cases.\n\n```{r}\n#| label: year-plus\n1913 + 5/12\n\n1915 + 7/12\n```\n\nHe thinks `seconds` equals $min * 60 + seconds$. Let us check the first two cases.\n\n```{r}\n#| label: seconds-plus\n4 * 60 + 14.4\n\n4 * 60 + 12.6\n```\n\nCorrect.\n\nLet us plot data using base R.\n\n```{r}\n#| label: plot-mile\n#| fig.cap: Mile record times\n#| column: margin\nplot(x = mile$year,\n     y = mile$seconds,\n     xlab = \"Year\",\n     ylab = \"Seconds\",\n     main = \"Mile record times\")\n\n```\n\nLet us plot data using ggplot2.\n\n```{r}\n#| label: ggplot-mile-ggplot\n#| fig.cap: Mile record times\n#| column: margin\nggplot(data = mile,\n       mapping = aes(x = year,\n                     y = seconds)) +\n  geom_point() +\n  labs(x = \"Year\",\n       y = \"Seconds\",\n       title = \"Mile record times\")\n\n```\n\nEstimate the line \n\n```{r}\n#| label: ggplot-mile-lm\nfit <- lm(formula = seconds ~ year,\n          data = mile)\nsummary(fit)\n```\n\n```{r}\n#| label: ggplot-mile-int-slope\nfitted_intercept <- fit$coeff[\"(Intercept)\"]\nfitted_slope <- fit$coeff[\"year\"]\n```\n\nggplot (add the estimated regression line).\n\n```{r}\n#| label: ggplot-mile-ggplot2\n#| column: margin\nggplot(data = mile,\n       mapping = aes(x = year,\n                     y = seconds)) +\n  geom_point() +\n  labs(x = \"Year\",\n       y = \"Seconds\",\n       title = \"Mile record times\") +\n  geom_abline(intercept = fitted_intercept,\n              slope = fitted_slope,\n              color = \"red\")\n\n```\n\nNow **Exercise 3.3 Probability distributions**.\n\nMake sure we all get the same numbers and create just 10 random numbers.\n\n```{r}\n#| label: standard-normal-10\nset.seed(123456789)\nstandard_normal_10 <- rnorm(n = 10, mean = 0, sd = 1)\nstandard_normal_10\n```\n\nNow create 1000 random numbers.\n```{r}\n#| label: standard-normal-1000\nstandard_normal_1000 <- rnorm(n = 1000, mean = 0, sd = 1)\nstandard_normal_1000\n\n```\n\nhe most basic histogram (single variable distribution) is created with the `hist` function.\n\n```{r}\n#| label: standard-normal-hist\n#| column: margin\nhist(x = standard_normal_1000,\n     breaks = 20,\n     col = \"lightblue\",\n     main = \"Histogram of 1000 standard normal random numbers\",\n     xlab = \"Standard normal random numbers\")\n\n```\n\nDensity for the standard normal (mean = 0, sd = 1)\n\n\n\n```{r}\n#| label: standard-normal-density\nx_axis <-\n  seq(min(standard_normal_1000), \n      max(standard_normal_1000), \n      length = 40)\n\ndensity <- dnorm(x_axis, \n                 mean = mean(standard_normal_1000), \n                 sd = sd(standard_normal_1000))\n```\n\nBy assumption (theoretical distibution has mean 0.00000 and sd 0.00000).\n\n```{r}\n#| label: standard-normal-density-theoretical\n#| column: margin\ndensity_theoretical <- dnorm(x_axis, mean = 0, sd = 1)\n\nplot(x = x_axis, y = density, \n     type = \"l\",\n     col = \"red\",\n     lwd = 2,\n     ylab = \"Density\",\n     main = \"Density of standard normal distribution\")\n\n```\n\nCreating 1000 random numbers\n\n```{r}\n#| label: sd2-normal-1000\nsd2_normal_1000 <- rnorm(n = 1000, mean = 0, sd = 2)\nsd2_normal_1000\n\nx_axis <-\n  seq(min(sd2_normal_1000), \n      max(sd2_normal_1000), \n      length = 40)\n\ndensity <- dnorm(x_axis, mean = mean(sd2_normal_1000), \n                 sd = sd(sd2_normal_1000))\n\n```\n\n```{r}\n#| label: sd2-normal-density-theoretical\n#| column: margin\nplot(x = x_axis, y = density, type = \"l\",\n     col = \"red\", \n     lwd = 2,\n     ylab = \"Density\",\n     main = \"Density of standard normal distribution\")\n```\n   \n**Exercise 3.4**. \n\n```{r}\n#| label: poisson-35-1000\n#| fig.cap: Poisson distribution with lambda = 3.5\n#| column: margin\npoisson_35_1000 <- \n  rpois(n = 1000, lambda = 3.5)\n\npoisson_35_1000 |> hist(breaks = length(unique(poisson_35_1000)))\n```\n\n**Excercise 3.5 Binomial distribution**.\n\n```{r}\n#| label: binom-03-20-1000\n#| fig.cap: Binomial distribution with p = 0.3, n = 20\n#| column: margin\nbinom_03_20_1000 <- \n  rbinom(n = 1000, size = 20, prob = 0.3)\n\nbinom_03_20_1000 |> hist(breaks = length(unique(binom_03_20_1000)))\n```\n\n\n**Exercise 3.6 Linear transformations**.\n\nThe mean must be increased from 35 to 100 by adding 65, the standard deviation must be increased from 10 to 15 by multiplying with 1.5.\n\nTransformation $X' = aX + b$\n$mean(X') = a * mean(X) + b$\n$sd(X') = a * sd(X)$\n\nNow mean(X') = 100, mean(X) = 35, sd(X') = 15, sd(X) = 10\n\n Substituting this:\n   $100 = a * 35 + b$\n   $15 = a * 10$\n\n$100 = 15/10 * 35 + b$\n$  b = 47.5$\n$  a = (100 - b) / 35 = 1.5$\n\ntransformation: $X` = 1.5 * X + 47.5$\n\n```{r}\n#| label: linear-transformation\noriginal_scores = rnorm(n = 1000, mean = 35, sd = 10)\noriginal_scores |> hist(breaks = 10)\n\ntransformed_scores = 1.5 * original_scores + 47.5\ntransformed_scores |> hist(breaks = 10)\n```\n\nNew range:  \\\nLowest  (X=0) is $0 * 1.5 + 47.5 = 47.5$ \\\nHighest (X=50) is $50 * 1.5 + 47.5 = 122.5$\n\nSimple. First multiply to get the standard deviation right.\n\n```{r}\n#| label: linear-transformation-1\ntransformed_1 <- original_scores * 1.5\n# Check that is ~15 now\nsd(transformed_1)\n\n```\n\n\nWhat is the mean after the first transformation?\n\n```{r}\n#| label: linear-transformation-2\nmean(transformed_1)\n```\n\nAdd the difference between the target mean (100) and the current mean\n\n```{r}\n#| label: linear-transformation-3\ntransformed_2 <- transformed_1 + (100 - mean(transformed_1))\ntransformed_2 |> hist(breaks = 10)\n```\n\n```{r}\n#| label: linear-transformation-4\n#| fig.cap: Original scores vs transformed scores\n#| column: margin\nplot(original_scores, transformed_scores, type = \"l\")\n```\n\n  \n**Exercise 3.8 Correlated random variables**\n\n```{r}\n#| label: correlated-random-variables\ncorrelation_hw = .3\nmean_husbands = 69.1\nsd_husbands = 2.9\nmean_wives = 63.7\nsd_wives = 2.7\n```\n\nweighted sum:\n\n```{r}\n#| label: correlated-random-variables-1\n.5 * mean_husbands + .5 * mean_wives\n```\n\nStandard deviation of .5 * husband + .5 wife\n\n```{r}\n#| label: correlated-random-variables-2\nsqrt(.5^2 * sd_husbands + \n       .5^2 * sd_wives + \n       2 * .5 * .5 * correlation_hw)\n```"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","reference-location":"margin","output-file":"03-chapter.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["references.bib"],"theme":"cosmo","title":"Some basic methods in mathematics and probability","author":" X Y (Chair)","citation-location":"margin","grid":{"margin-width":"350px"}},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","reference-location":"margin","output-file":"03-chapter.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrreprt","title":"Some basic methods in mathematics and probability","author":" X Y (Chair)","citation-location":"margin"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}