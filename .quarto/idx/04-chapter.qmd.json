{"title":"Statistical Inference","markdown":{"yaml":{"title":"Statistical Inference","author":"Harrie Jonkman","format":{"html":{"grid":{"margin-width":"350px"}}},"reference-location":"margin","citation-location":"margin"},"headingText":"Summary","containsRefs":false,"markdown":"\n\n\nStatistical inference can be formulated as a set of operation on data that yield estimates and uncertainty statements about predictions and parameters of some underlying process of population. From a mathematical standpoint, these probabilistic uncertainty statements are derived based on some assumed probability model for observed data. \n\nIn this chapter:\\\n- the basics of probability models are sketched (estimation, bias, and variance); \\\n- the interpretation of statistical inferences and statistical errors in applied work; \\\n- the theme of uncertainty in statistical inference is introduced; \\\n- a mistake to use hypothesis tests or statistical significance to attribute certainty from noisy data are discussed.\n\nStatistical inference is used to learn from incomplete or imperfect data.    \n- In the *sampling model* we are for example interested in learning some characteristics of a population from a sample.\\\n- In the *measurement model* we are interested in learning about the underlying pattern or law.\\\n- *Model error* refers to the inevitable imperferction of the model.\\\n\nSome definitions are given. The *sampling distribution* is the set of possible datasets that could have been observed if the data collection process had been re-done, along with the probabilities of these possible values. It is said to be a *generative model* in that it represents a random process which, if known, could generate a new dataset. *Parameters* are the unknown numbers that determine a statistical model, e.g. $y_i=a+bx_i+\\epsilon_i$ in which the errors $\\epsilon_I$ are normally distributed with mean 0 and standard deviation $\\sigma$. Thre parameters $a$ and $b$ are called *coeffients* and $\\sigma$ is a *scale* or *variance parameter*.    \nThe *standard error* ($\\sigma/ \\sqrt{n}$) is the estimated standard deviation of an estimate and can give us a sense of our uncertainty about the quantity of interest. The *confidence interval* represents a range of values of a parameter or quantity of that are roughly consistent with the data, given the assumed sampling distribution. \n\n```{r, warning=FALSE, message=FALSE, echo=FALSE}\n\nlibrary(tidyverse)\n\n#| label: simulation\n#| fig.cap: \"Figure 4.2: Simulation\"\n#| column: margin \n\n# Thanks Solomon Kurz\n# how many simulations would you like?\nn <- 100\n\n# set the true data-generating parameters\nmu <- 6\nsigma <- 4\n\nset.seed(4)\n\n# simulate\nd <-\n  tibble(i = 1:n,\n         y = rnorm(n, mean = mu, sd = sigma)) %>% \n  mutate(ll95 = y - 2 * sigma,\n         ll50 = y - 0.67 * sigma,\n         ul50 = y + 0.67 * sigma,\n         ul95 = y + 2 * sigma) \n\n# plot\nd %>% \n  ggplot(aes(x = i, y = y)) +\n  geom_hline(yintercept = mu, color = \"grey75\", size = 1/4) +\n  geom_pointrange(aes(ymin = ll95, ymax = ul95),\n                  size = 1/4, fatten = 2/3) +\n  geom_linerange(aes(ymin = ll50, ymax = ul50),\n                 size = 1/2) +\n  labs(title = \"Simulation of coverage of confidence intervals\",\n       subtitle = \"The horizontal line shows the true parameter value, and dots and vertical lines show\\nestimates and confidence intervals obtained from 100 random simulations from the\\nsampling distribution.\",\n       x = \"Simulation index\",\n       y = \"Estimate, 50%, and 95%\\nconfidence interval\")\n\n\n```\n\nBias and unmodeled uncertainty are also discussed. Roughly speaking, an estimate is *unbiased* if it is correct on average. Take into account that random samples and randomized experiments are imperfect in reality, and any approximations become even more tenuous when applied to observational data. Also, survey respondents are not balles drawn from an ure, and the probabilties in the \"urn\" are changing over time. So, improve data collection, expand the model, and increase stated uncertainty.\n\nPerforming data analysis is the possibility of mistakenly coming to strong conclusions that do not reflect real patterns in the underlying population. Statistical theories of hypothesis testing and error analysis have been developed to quantify these possibilities in the context of inference and decision making.\n\nA commonly used decision rule that we do *not* recommend is to consider a result as stable or real if it is \"statistically significant\" and to taken \"non-statistically\" results to be noisy and to be treated with skepticism. The concepts of hypothesis testing are reviewed with a simple hypothetical example. Estimate, standard error, degrees of freedom, null and alternative hypotheses and p-value, as well as the general formulation, confidence intervals to compare results, and Type 1 and Type 2-errors, important in conventional hypthesis testing, are presented. \n\nThey present the problems with the concept of statistical significance (some examples are given):\n\n- Statistical significance is not the same as practical significance; \\\n- Non-significance is not the same as zero; \\\n- The difference between \"significant\" and \"non-significant\" is not itself statistically significant; \\\n- Statistical significance can be attained by multiple comparisons or multiple potential comparisons; \\\n- The statistical significant estimates tend to be overestimated; \\\n\nIn this book they try to move beyond hypothesis testing. The most important aspect of their statistical method is its ability to incorporate more information into the analysis. General rules are:    \n- Analyse *all* your data; \\\n- Present *all* your comparisons; \\\n- Make your data *public*.\n\nBayesian methods can reduce now-common pattern of the researchers getting jerked around by noise patterns that happen to exceed the statistical significance threshold. We can move forward by accepting uncertainty and embracing variation.\n\n## Presentation\n\n### 1. ESTIMATION\n\nSet the true data-generating parameters (e.g mean 175 cm and sd=6 cm)\n\n```{r load-packages, message=FALSE, warning=FALSE}\nlibrary(tidyverse)\n\nn<-1000\nmu<-175\nsigma<-6\n\nestimate<-mu/n\nse<-sqrt(estimate*(1-estimate)/n)\n\nestimate\nse\n```\n\nThis is figure 4.2 on page 52 of the ROS-book.\n\n```{r, warning=FALSE, message=FALSE, echo=FALSE}\nlibrary(tidyverse)\n\n#| label: simulation\n#| fig.cap: \"Simulation\"\n#| column: margin\n\n# how many simulations would you like?\nn <- 100\n\nmu <- 175\nsigma <- 6\n\nset.seed(4)\n\n# simulate\nd <-\n  tibble(i = 1:n,\n         y = rnorm(n, mean = mu, sd = sigma)) |>\n  mutate(ll95 = y - 2 * sigma,\n         ul95 = y + 2 * sigma)\n\n# plot\nd %>% \n  ggplot(aes(x = i, y = y)) +\n  geom_hline(yintercept = mu, color = \"grey75\", size = 1/4) +\n  geom_pointrange(aes(ymin = ll95, ymax = ul95),\n                  size = 1/4, fatten = 2/3) +\n  labs(title = \"Simulation of coverage of confidence intervals\",\n       subtitle = \"The horizontal line shows the true parameter value, and dots and vertical lines show\\nestimates and confidence intervals obtained from 100 random simulations from the\\nsampling distribution.\",\n       x = \"100 persons\",\n       y = \"Estimate, and 95% confidence interval\")\n```\n\nLet us look at different estimations using different distributions\n\n**Normal distribution**\n\nThe Normal distribution, frequently encountered in real-world scenarios like IQ scores or heights in a population, exemplifies a symmetric bell-shaped curve. It symbolizes situations where most observations cluster around a central mean, with fewer occurrences as we move away from the center.\n\nLet us generate a normal distribution in R\n\n```{r}\nset.seed(4)\n\nn<-1000\nmu<-175\nsd<-6\n\nsamplenorm <- rnorm(n=n, mean = mu, sd = sd)\nprint(samplenorm)\n```\n\nSummarize over sample.\n\n```{r}\nmean(samplenorm)\nsd(samplenorm)\nconfint(lm(samplenorm~1), level=0.95)\n```\n\nPlot it!\n\n```{r}\n#| label: normaldistribution\n#| fig.cap: \"Normal distribution\"\n#| column: margin\n\nhist(samplenorm, breaks=30, col = \"skyblue\", border = \"black\", main = \"Normal Distribution\", xlab = \"Value\", ylab = \"Frequency\")\n```\n\n**Binomial distributions**\n\nWhen N available things all have the same probability $p$ of being in a certain state (eg. being counted, male or dead)\n\nGenerating a binomial distribution in R\n\n```{r}\nset.seed(5)\n\nn<-1000.  # sample size\nN<- 16    # numbers of individuals\np<- 0.8   # probability of success (counted, male or)\n\nsamplebinom <- rbinom(n=n, size = N, prob = p)\nprint(samplebinom)\n```\n\nSummarize over sample\n\n```{r}\nmean(samplebinom)\nsd(samplebinom)\nconfint(lm(samplebinom~1), level=0.95)\n```\n\nPlot it!\n\n```{r}\n#| label: binomialdistribution\n#| fig.cap: \"Binomial distribution\"\n#| column: margin\n\nhist(samplebinom, breaks=30, col = \"skyblue\", border = \"black\", main = \"Binomial Distribution\", xlab = \"Value\", ylab = \"Frequency\")\n\n```\n\n## Poisson distribution\n\nThis is about frequency of rare events in a specific time or space (for example number of emails people receive on day basis, or number of cars passing a certain point in a given time)\n\nGenerating a Poisson distribution in R.\n\n```{r}\nset.seed(6)\n\nn<-1000 # sample size\nlambda<- 5 # average number of events in a given time or space\n\nsamplepois <- rpois(n=n, lambda = lambda)\nprint(samplepois)\n```\n\nSummarize over sample\n\n```{r}\nsummary(samplepois)\n```\n\nPlot it!\n\n```{r}\n#| label: poissondistribution\n#| fig.cap: \"Poisson distribution\"\n#| column: margin\n\nhist(samplepois, breaks=30, col = \"skyblue\", border = \"black\", main = \"Poisson Distribution\", xlab = \"Value\", ylab = \"Frequency\")\n```\n\n## 2. UNCERTAINTY\n\nUncertainty can often be compared visually, see for example figure 4.3 on page 53. It shows the proportion of American adults supporting the death penalty over the years (from a serie of Gallup polls)\n\nLet us load packages and the data.\n\n```{r}\npolls <- matrix(scan(\"ROS-Examples-master/Death/data/polls.dat\"), ncol=5, byrow=TRUE)\n\nView(polls)\n\n```\n\n```{r}\ndeath_penalty <- \n  polls |>\n  matrix(ncol = 5, byrow = TRUE) %>% \n  as_tibble(\n    .name_repair = ~ c(\"year\", \"month\", \"favor\", \"not_in_favor\", \"no_opinion\")\n  ) |> \n  transmute(\n    date = lubridate::make_date(year = year, month = month),\n    favor = favor / (favor + not_in_favor),\n    favor_sd = sqrt(favor * (1 - favor) / 1000)\n  )\n```\n\nLook at data set now\n\n```{r}\nView(death_penalty)\n```\n\nAre you in favor of the death penalty for a person convicted of murder?\n\n```{r}\n#| label: deathpenalty\n#| fig.cap: \"Death penalty\"\n#| column: margin\n#| \ndeath_penalty %>% \n  ggplot(aes(date, favor)) +\n  geom_pointrange(\n    aes(ymin = favor - favor_sd, ymax = favor + favor_sd),\n    size = 0.2\n  ) +\n  scale_x_date(\n    breaks = lubridate::make_date(year = seq(1940, 2000, 10)),\n    minor_breaks = lubridate::make_date(year = seq(1936, 2004, 2)),\n    date_labels = \"%Y\"\n  ) +\n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +\n  labs(\n    title = \n      \"Are you in favor of the death penalty for a person convicted of murder?\",\n    x = \"Year\",\n    y = \"Percentage in favor of those with an opinion\",\n    caption = \"Source: Gallup\"\n  )\n```\n\n## 3. SIGNIFICANCE TESTING\n\n> Statistical significance is conventionally defined as a $p$-value less than 0.05, relative to some *null hypothesis* or prespecified value that would indicate no effect present, as discussed below in the context of hypothesis testing. For fitted regressions, this roughly corresponds to coefficient estimates being labeled as statistically significant if they are at least two standard errors from zero, or not statistically significant otherwise. (p. 57, *emphasis* in the original\n\n```{r}\nn <- 20\ny <- 8\n\n# the estimated probability\n(p <- y / n)\n\n# the standard error\n(se <- sqrt(p * (1 - p) / n))\n\n# Not significant because .5 (the expected value, NH) is within the border. \np + c(-2 * se, 2 * se)\n```\n\nThe hypothesis test is based on a *test statistic* that summarizes the deviation of the data from what would be expected under the null hypothesis. The conventional test statistic in this sort of problem is the absolute value of the $t$-score. It is all summarized in $p$-value. The confidence interval is often more interesting than the $p$-value, because it gives a range of plausible values for the parameter of interest, rather than just a binary decision about whether the parameter is different from zero.\n\nOpen some libraries\n\n```{r }\nlibrary(broom)\nlibrary(rstanarm)\nlibrary(brms)\n```\n\nDataset\n\nI used this information [here](https://www.flutterbys.com.au/stats/tut/tut6.2b.html#MCMCgraphicalSummariestab-5)\n\n```{r}\nset.seed(1)\nnA <- 60                        #sample size from Population A\nnB <- 40                        #sample size from Population B\nmuA <- 105                      #population mean of Population A\nmuB <- 77.5                     #population mean of Population B\nsigma <- 3                      #standard deviation of both populations (equally varied)\nyA <- rnorm(nA, muA, sigma)     #Population A sample\nyB <- rnorm(nB, muB, sigma)     #Population B sample\ny <- c(yA, yB)                  #combined dataset\nx <- factor(rep(c(\"A\", \"B\"), c(nA, nB)))  #categorical listing of the populations\nxn <- as.numeric(x)  #numerical version of the population category for means parameterization. \n# Should not start at 0.\nmy_data <- data.frame(y, x, xn)  # dataset\nhead(my_data)\n```\n:::{.panel-tabset}\n## Traditional approach\n\nUse ttest\n\n```{r}\nt.test(y ~ x, data = my_data)\n```\n\nUse broom\n\n```{r}\nlm(y ~ x, data = my_data) |>\n  tidy()\n```\n\nPlot it!\n\n```{r}\n#| label: boxplot\n#| fig.cap: \"Boxplot\"\n#| column: margin\n#| \nggplot(my_data, aes(x = x, y = y)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, height = 0) +\n  labs(\n    title = \"Boxplot of the two groups\",\n    x = \"Group\",\n    y = \"y\"\n  )\n```\n\n## Bayesian approach: use `rstanarm`\n\n```{r}\ndata_rstanarm<-stan_glm(y ~ x, data = my_data, family = gaussian())\n\n```\n\n```{r}\nprint(data_rstanarm)\n```\n\n```{r}\nsummary(data_rstanarm)\n```\n\n```{r}\nposterior_interval(data_rstanarm)\n```\n\nProbability statements are possible\n\n```{r}\n#| label: histogram\n#| fig.cap: \"Histogram of ES\"\n#| column: margin\n#| \nmcmc = as.matrix(data_rstanarm)\n# Percentage change (relative to Group A)\nES = 100 * mcmc[, \"xB\"]/mcmc[, \"(Intercept)\"]\nhist(ES)\n```\n\nProbability that the effect is greater than 10% (a decline of \\>10%)\n\n```{r}\nsum(-1 * ES > 10)/length(ES)\n```\n\nProbability that the effect is greater than 25% (a decline of \\>25%)\n\n```{r}\nsum(-1 * ES > 25)/length(ES)\n```\n\n\n## Bayesian approach: use `brms`\n\n```{r}\ndata_brms <- brm(y ~ x, data = my_data, family = gaussian())\n\n```\n\n```{r}\nprint(data_brms)\n```\n\n```{r}\nsummary(data_brms)\n```\n\n```{r}\n#| label: histogram2\n#| fig.cap: \"Histogram of ES\"\n#| column: margin\n#| \nmcmc = as.matrix(data_brms)\n# Percentage change (relative to Group A)\nES = 100 * mcmc[, \"b_xB\"]/mcmc[, \"b_Intercept\"]\nhist(ES)\n```\n\nProbability that the effect is greater than 10% (a decline of \\>10%)\n\n```{r}\nsum(-1 * ES > 10)/length(ES)\n```\n\nProbability that the effect is greater than 25% (a decline of \\>25%)\n\n```{r}\nsum(-1 * ES > 25)/length(ES)\n```\n\n:::\n\nSteps of (Bayesian) regression analysis (will look at this in more detail later):   \n1. Identify and collect the data (also visualization);   \n2. Choose a statistical model;   \n3. Specify prior distributions;   \n4. Obtain posterior distributions;   \n5. Posterior predictive checks;   \n6. Interpret results (also visualization).\n\n\n\n","srcMarkdownNoYaml":"\n\n## Summary\n\nStatistical inference can be formulated as a set of operation on data that yield estimates and uncertainty statements about predictions and parameters of some underlying process of population. From a mathematical standpoint, these probabilistic uncertainty statements are derived based on some assumed probability model for observed data. \n\nIn this chapter:\\\n- the basics of probability models are sketched (estimation, bias, and variance); \\\n- the interpretation of statistical inferences and statistical errors in applied work; \\\n- the theme of uncertainty in statistical inference is introduced; \\\n- a mistake to use hypothesis tests or statistical significance to attribute certainty from noisy data are discussed.\n\nStatistical inference is used to learn from incomplete or imperfect data.    \n- In the *sampling model* we are for example interested in learning some characteristics of a population from a sample.\\\n- In the *measurement model* we are interested in learning about the underlying pattern or law.\\\n- *Model error* refers to the inevitable imperferction of the model.\\\n\nSome definitions are given. The *sampling distribution* is the set of possible datasets that could have been observed if the data collection process had been re-done, along with the probabilities of these possible values. It is said to be a *generative model* in that it represents a random process which, if known, could generate a new dataset. *Parameters* are the unknown numbers that determine a statistical model, e.g. $y_i=a+bx_i+\\epsilon_i$ in which the errors $\\epsilon_I$ are normally distributed with mean 0 and standard deviation $\\sigma$. Thre parameters $a$ and $b$ are called *coeffients* and $\\sigma$ is a *scale* or *variance parameter*.    \nThe *standard error* ($\\sigma/ \\sqrt{n}$) is the estimated standard deviation of an estimate and can give us a sense of our uncertainty about the quantity of interest. The *confidence interval* represents a range of values of a parameter or quantity of that are roughly consistent with the data, given the assumed sampling distribution. \n\n```{r, warning=FALSE, message=FALSE, echo=FALSE}\n\nlibrary(tidyverse)\n\n#| label: simulation\n#| fig.cap: \"Figure 4.2: Simulation\"\n#| column: margin \n\n# Thanks Solomon Kurz\n# how many simulations would you like?\nn <- 100\n\n# set the true data-generating parameters\nmu <- 6\nsigma <- 4\n\nset.seed(4)\n\n# simulate\nd <-\n  tibble(i = 1:n,\n         y = rnorm(n, mean = mu, sd = sigma)) %>% \n  mutate(ll95 = y - 2 * sigma,\n         ll50 = y - 0.67 * sigma,\n         ul50 = y + 0.67 * sigma,\n         ul95 = y + 2 * sigma) \n\n# plot\nd %>% \n  ggplot(aes(x = i, y = y)) +\n  geom_hline(yintercept = mu, color = \"grey75\", size = 1/4) +\n  geom_pointrange(aes(ymin = ll95, ymax = ul95),\n                  size = 1/4, fatten = 2/3) +\n  geom_linerange(aes(ymin = ll50, ymax = ul50),\n                 size = 1/2) +\n  labs(title = \"Simulation of coverage of confidence intervals\",\n       subtitle = \"The horizontal line shows the true parameter value, and dots and vertical lines show\\nestimates and confidence intervals obtained from 100 random simulations from the\\nsampling distribution.\",\n       x = \"Simulation index\",\n       y = \"Estimate, 50%, and 95%\\nconfidence interval\")\n\n\n```\n\nBias and unmodeled uncertainty are also discussed. Roughly speaking, an estimate is *unbiased* if it is correct on average. Take into account that random samples and randomized experiments are imperfect in reality, and any approximations become even more tenuous when applied to observational data. Also, survey respondents are not balles drawn from an ure, and the probabilties in the \"urn\" are changing over time. So, improve data collection, expand the model, and increase stated uncertainty.\n\nPerforming data analysis is the possibility of mistakenly coming to strong conclusions that do not reflect real patterns in the underlying population. Statistical theories of hypothesis testing and error analysis have been developed to quantify these possibilities in the context of inference and decision making.\n\nA commonly used decision rule that we do *not* recommend is to consider a result as stable or real if it is \"statistically significant\" and to taken \"non-statistically\" results to be noisy and to be treated with skepticism. The concepts of hypothesis testing are reviewed with a simple hypothetical example. Estimate, standard error, degrees of freedom, null and alternative hypotheses and p-value, as well as the general formulation, confidence intervals to compare results, and Type 1 and Type 2-errors, important in conventional hypthesis testing, are presented. \n\nThey present the problems with the concept of statistical significance (some examples are given):\n\n- Statistical significance is not the same as practical significance; \\\n- Non-significance is not the same as zero; \\\n- The difference between \"significant\" and \"non-significant\" is not itself statistically significant; \\\n- Statistical significance can be attained by multiple comparisons or multiple potential comparisons; \\\n- The statistical significant estimates tend to be overestimated; \\\n\nIn this book they try to move beyond hypothesis testing. The most important aspect of their statistical method is its ability to incorporate more information into the analysis. General rules are:    \n- Analyse *all* your data; \\\n- Present *all* your comparisons; \\\n- Make your data *public*.\n\nBayesian methods can reduce now-common pattern of the researchers getting jerked around by noise patterns that happen to exceed the statistical significance threshold. We can move forward by accepting uncertainty and embracing variation.\n\n## Presentation\n\n### 1. ESTIMATION\n\nSet the true data-generating parameters (e.g mean 175 cm and sd=6 cm)\n\n```{r load-packages, message=FALSE, warning=FALSE}\nlibrary(tidyverse)\n\nn<-1000\nmu<-175\nsigma<-6\n\nestimate<-mu/n\nse<-sqrt(estimate*(1-estimate)/n)\n\nestimate\nse\n```\n\nThis is figure 4.2 on page 52 of the ROS-book.\n\n```{r, warning=FALSE, message=FALSE, echo=FALSE}\nlibrary(tidyverse)\n\n#| label: simulation\n#| fig.cap: \"Simulation\"\n#| column: margin\n\n# how many simulations would you like?\nn <- 100\n\nmu <- 175\nsigma <- 6\n\nset.seed(4)\n\n# simulate\nd <-\n  tibble(i = 1:n,\n         y = rnorm(n, mean = mu, sd = sigma)) |>\n  mutate(ll95 = y - 2 * sigma,\n         ul95 = y + 2 * sigma)\n\n# plot\nd %>% \n  ggplot(aes(x = i, y = y)) +\n  geom_hline(yintercept = mu, color = \"grey75\", size = 1/4) +\n  geom_pointrange(aes(ymin = ll95, ymax = ul95),\n                  size = 1/4, fatten = 2/3) +\n  labs(title = \"Simulation of coverage of confidence intervals\",\n       subtitle = \"The horizontal line shows the true parameter value, and dots and vertical lines show\\nestimates and confidence intervals obtained from 100 random simulations from the\\nsampling distribution.\",\n       x = \"100 persons\",\n       y = \"Estimate, and 95% confidence interval\")\n```\n\nLet us look at different estimations using different distributions\n\n**Normal distribution**\n\nThe Normal distribution, frequently encountered in real-world scenarios like IQ scores or heights in a population, exemplifies a symmetric bell-shaped curve. It symbolizes situations where most observations cluster around a central mean, with fewer occurrences as we move away from the center.\n\nLet us generate a normal distribution in R\n\n```{r}\nset.seed(4)\n\nn<-1000\nmu<-175\nsd<-6\n\nsamplenorm <- rnorm(n=n, mean = mu, sd = sd)\nprint(samplenorm)\n```\n\nSummarize over sample.\n\n```{r}\nmean(samplenorm)\nsd(samplenorm)\nconfint(lm(samplenorm~1), level=0.95)\n```\n\nPlot it!\n\n```{r}\n#| label: normaldistribution\n#| fig.cap: \"Normal distribution\"\n#| column: margin\n\nhist(samplenorm, breaks=30, col = \"skyblue\", border = \"black\", main = \"Normal Distribution\", xlab = \"Value\", ylab = \"Frequency\")\n```\n\n**Binomial distributions**\n\nWhen N available things all have the same probability $p$ of being in a certain state (eg. being counted, male or dead)\n\nGenerating a binomial distribution in R\n\n```{r}\nset.seed(5)\n\nn<-1000.  # sample size\nN<- 16    # numbers of individuals\np<- 0.8   # probability of success (counted, male or)\n\nsamplebinom <- rbinom(n=n, size = N, prob = p)\nprint(samplebinom)\n```\n\nSummarize over sample\n\n```{r}\nmean(samplebinom)\nsd(samplebinom)\nconfint(lm(samplebinom~1), level=0.95)\n```\n\nPlot it!\n\n```{r}\n#| label: binomialdistribution\n#| fig.cap: \"Binomial distribution\"\n#| column: margin\n\nhist(samplebinom, breaks=30, col = \"skyblue\", border = \"black\", main = \"Binomial Distribution\", xlab = \"Value\", ylab = \"Frequency\")\n\n```\n\n## Poisson distribution\n\nThis is about frequency of rare events in a specific time or space (for example number of emails people receive on day basis, or number of cars passing a certain point in a given time)\n\nGenerating a Poisson distribution in R.\n\n```{r}\nset.seed(6)\n\nn<-1000 # sample size\nlambda<- 5 # average number of events in a given time or space\n\nsamplepois <- rpois(n=n, lambda = lambda)\nprint(samplepois)\n```\n\nSummarize over sample\n\n```{r}\nsummary(samplepois)\n```\n\nPlot it!\n\n```{r}\n#| label: poissondistribution\n#| fig.cap: \"Poisson distribution\"\n#| column: margin\n\nhist(samplepois, breaks=30, col = \"skyblue\", border = \"black\", main = \"Poisson Distribution\", xlab = \"Value\", ylab = \"Frequency\")\n```\n\n## 2. UNCERTAINTY\n\nUncertainty can often be compared visually, see for example figure 4.3 on page 53. It shows the proportion of American adults supporting the death penalty over the years (from a serie of Gallup polls)\n\nLet us load packages and the data.\n\n```{r}\npolls <- matrix(scan(\"ROS-Examples-master/Death/data/polls.dat\"), ncol=5, byrow=TRUE)\n\nView(polls)\n\n```\n\n```{r}\ndeath_penalty <- \n  polls |>\n  matrix(ncol = 5, byrow = TRUE) %>% \n  as_tibble(\n    .name_repair = ~ c(\"year\", \"month\", \"favor\", \"not_in_favor\", \"no_opinion\")\n  ) |> \n  transmute(\n    date = lubridate::make_date(year = year, month = month),\n    favor = favor / (favor + not_in_favor),\n    favor_sd = sqrt(favor * (1 - favor) / 1000)\n  )\n```\n\nLook at data set now\n\n```{r}\nView(death_penalty)\n```\n\nAre you in favor of the death penalty for a person convicted of murder?\n\n```{r}\n#| label: deathpenalty\n#| fig.cap: \"Death penalty\"\n#| column: margin\n#| \ndeath_penalty %>% \n  ggplot(aes(date, favor)) +\n  geom_pointrange(\n    aes(ymin = favor - favor_sd, ymax = favor + favor_sd),\n    size = 0.2\n  ) +\n  scale_x_date(\n    breaks = lubridate::make_date(year = seq(1940, 2000, 10)),\n    minor_breaks = lubridate::make_date(year = seq(1936, 2004, 2)),\n    date_labels = \"%Y\"\n  ) +\n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +\n  labs(\n    title = \n      \"Are you in favor of the death penalty for a person convicted of murder?\",\n    x = \"Year\",\n    y = \"Percentage in favor of those with an opinion\",\n    caption = \"Source: Gallup\"\n  )\n```\n\n## 3. SIGNIFICANCE TESTING\n\n> Statistical significance is conventionally defined as a $p$-value less than 0.05, relative to some *null hypothesis* or prespecified value that would indicate no effect present, as discussed below in the context of hypothesis testing. For fitted regressions, this roughly corresponds to coefficient estimates being labeled as statistically significant if they are at least two standard errors from zero, or not statistically significant otherwise. (p. 57, *emphasis* in the original\n\n```{r}\nn <- 20\ny <- 8\n\n# the estimated probability\n(p <- y / n)\n\n# the standard error\n(se <- sqrt(p * (1 - p) / n))\n\n# Not significant because .5 (the expected value, NH) is within the border. \np + c(-2 * se, 2 * se)\n```\n\nThe hypothesis test is based on a *test statistic* that summarizes the deviation of the data from what would be expected under the null hypothesis. The conventional test statistic in this sort of problem is the absolute value of the $t$-score. It is all summarized in $p$-value. The confidence interval is often more interesting than the $p$-value, because it gives a range of plausible values for the parameter of interest, rather than just a binary decision about whether the parameter is different from zero.\n\nOpen some libraries\n\n```{r }\nlibrary(broom)\nlibrary(rstanarm)\nlibrary(brms)\n```\n\nDataset\n\nI used this information [here](https://www.flutterbys.com.au/stats/tut/tut6.2b.html#MCMCgraphicalSummariestab-5)\n\n```{r}\nset.seed(1)\nnA <- 60                        #sample size from Population A\nnB <- 40                        #sample size from Population B\nmuA <- 105                      #population mean of Population A\nmuB <- 77.5                     #population mean of Population B\nsigma <- 3                      #standard deviation of both populations (equally varied)\nyA <- rnorm(nA, muA, sigma)     #Population A sample\nyB <- rnorm(nB, muB, sigma)     #Population B sample\ny <- c(yA, yB)                  #combined dataset\nx <- factor(rep(c(\"A\", \"B\"), c(nA, nB)))  #categorical listing of the populations\nxn <- as.numeric(x)  #numerical version of the population category for means parameterization. \n# Should not start at 0.\nmy_data <- data.frame(y, x, xn)  # dataset\nhead(my_data)\n```\n:::{.panel-tabset}\n## Traditional approach\n\nUse ttest\n\n```{r}\nt.test(y ~ x, data = my_data)\n```\n\nUse broom\n\n```{r}\nlm(y ~ x, data = my_data) |>\n  tidy()\n```\n\nPlot it!\n\n```{r}\n#| label: boxplot\n#| fig.cap: \"Boxplot\"\n#| column: margin\n#| \nggplot(my_data, aes(x = x, y = y)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, height = 0) +\n  labs(\n    title = \"Boxplot of the two groups\",\n    x = \"Group\",\n    y = \"y\"\n  )\n```\n\n## Bayesian approach: use `rstanarm`\n\n```{r}\ndata_rstanarm<-stan_glm(y ~ x, data = my_data, family = gaussian())\n\n```\n\n```{r}\nprint(data_rstanarm)\n```\n\n```{r}\nsummary(data_rstanarm)\n```\n\n```{r}\nposterior_interval(data_rstanarm)\n```\n\nProbability statements are possible\n\n```{r}\n#| label: histogram\n#| fig.cap: \"Histogram of ES\"\n#| column: margin\n#| \nmcmc = as.matrix(data_rstanarm)\n# Percentage change (relative to Group A)\nES = 100 * mcmc[, \"xB\"]/mcmc[, \"(Intercept)\"]\nhist(ES)\n```\n\nProbability that the effect is greater than 10% (a decline of \\>10%)\n\n```{r}\nsum(-1 * ES > 10)/length(ES)\n```\n\nProbability that the effect is greater than 25% (a decline of \\>25%)\n\n```{r}\nsum(-1 * ES > 25)/length(ES)\n```\n\n\n## Bayesian approach: use `brms`\n\n```{r}\ndata_brms <- brm(y ~ x, data = my_data, family = gaussian())\n\n```\n\n```{r}\nprint(data_brms)\n```\n\n```{r}\nsummary(data_brms)\n```\n\n```{r}\n#| label: histogram2\n#| fig.cap: \"Histogram of ES\"\n#| column: margin\n#| \nmcmc = as.matrix(data_brms)\n# Percentage change (relative to Group A)\nES = 100 * mcmc[, \"b_xB\"]/mcmc[, \"b_Intercept\"]\nhist(ES)\n```\n\nProbability that the effect is greater than 10% (a decline of \\>10%)\n\n```{r}\nsum(-1 * ES > 10)/length(ES)\n```\n\nProbability that the effect is greater than 25% (a decline of \\>25%)\n\n```{r}\nsum(-1 * ES > 25)/length(ES)\n```\n\n:::\n\nSteps of (Bayesian) regression analysis (will look at this in more detail later):   \n1. Identify and collect the data (also visualization);   \n2. Choose a statistical model;   \n3. Specify prior distributions;   \n4. Obtain posterior distributions;   \n5. Posterior predictive checks;   \n6. Interpret results (also visualization).\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","reference-location":"margin","output-file":"04-chapter.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["references.bib"],"theme":"cosmo","title":"Statistical Inference","author":"Harrie Jonkman","citation-location":"margin","grid":{"margin-width":"350px"}},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","reference-location":"margin","output-file":"04-chapter.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrreprt","title":"Statistical Inference","author":"Harrie Jonkman","citation-location":"margin"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}