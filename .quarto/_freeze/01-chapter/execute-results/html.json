{
  "hash": "7f06e772e4e66e8f0bbb5bc21969f260",
  "result": {
    "markdown": "---\ntitle: \"Overview\"\nauthor: \"Alex Trinidad (Chair)\"\nformat:\n  html: \n    grid: \n      margin-width: 350px\n  pdf: default\nreference-location: margin\ncitation-location: margin\n---\n\n\n## Summary\n\nThis firstnchapter lays out the key challenges of statistical inference in general and regression modeling in particular.\n\n::: column-margin\nInference defined as using mathematical models to make general claims from particular data\n:::\n\nThere are three challenges to statistics, which all can be framed as problems of prediction:\\\n- Generalizing from sample to population;\\\n- Generalizing from treatment to control group;\\\n- Generalizing from observed measurements to the underlying construct of interest.\n\nThe key skills you learn in this book are: \\\n- Understanding regression models;\\\n- Constructing regression models;\\\n- Fitting regression models to data;\\\n- Displaying and interpreting the results of regression models;\n\nRegression is a method that allows researchers to summarize how predictions or average values of an *outcome* vary across individuals defined by a set of *predictors*. It is used for example to predict, to explore associations, to extrapolate and for causal inference. Exmaples are given.\n\nThere are four steps in statistical analysis: \\\n- Model building (starting);\\\n- Model fitting;\\\n- Understanding model fits;\\\n- Criticism.\n\nFitting models and making predictions can be down different frameworks. Three concerns are important everytime: \n- Information used;\\\n- Assumptions used;\\\n- Estimating and interpreting (classical or Bayesian framework).\n\nGelman et all. recommend to use the Bayesian framework. If information available you can use it, if not you can use weakly informative default priors. On this way you stable estimates and with the simulations you can express uncertainty.\n\nThe overall Bayesian regression in R is:\n\n:::{.panel-tabset}\n\n## rstanarm\n\n\n```         \nfit<-stan_glm(y~x,data=mydata)\n```\n\n::: column-margin\nBayes can take longer time. Here you can use\n\n```         \nfit<-stan_glm(y~x,data=mydata,algorithm=\"optimizing\")\n```\n:::\n\n## brms\n```\nfit<-brm(y~x,data=mydata)\n```\n\n:::\n\n\n\nWhere y is the outcome, x is the predictor and mydata is the data frame. But you can do it also in classical framework:\n\n```         \nfit<-lm(y~x,data=mydata)\n```\n\nUsing Bayesian and simulation approaches can be more important when fitting multilevel or regularized regression models. This will be handled in their next book.\n\n## Presentation\n\nFirst some libraries are loaded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rosdata) # for the ROSdata\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(rstanarm) # for the stan_glm function\nlibrary(brms) # for the brm function\n```\n:::\n\n\n\n\n## Presentation\nOn 14-11-2023 Alex Trinidad (University of Cologne and Netherlands Institute for the Study of Crime and Law Enforcement) presented the first chapter of the book *Regression and Other Stories* by Andrew Gelman, Jennifer Hill, and Aki Vehtari: **Overview**. The session was held online via Zoom.\n[Here]() you can find Alex' script Trinidad.\n\nFirst he loaded this package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n1. Regression to predict \n\nHow can we predict presidential vote share using economy growth? For this he loaded the ROS-data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nelections_data <- read.csv(url(\"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectionsEconomy/data/hibbs.dat\"), sep = \"\")\n```\n:::\n\n\nThis another way to load these data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremotes::install_github(\"avehtari/ROS-Examples\", subdir = \"rpackage\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSkipping install of 'rosdata' from a github remote, the SHA1 (a049a104) has not changed since last install.\n  Use `force = TRUE` to force installation\n```\n:::\n\n```{.r .cell-code}\nelections_data <- rosdata::hibbs\n```\n:::\n\n\n\nLet us first explore economy growth.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(elections_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 16\nColumns: 5\n$ year                <int> 1952, 1956, 1960, 1964, 1968, 1972, 1976, 1980, 19…\n$ growth              <dbl> 2.40, 2.89, 0.85, 4.21, 3.02, 3.62, 1.08, -0.39, 3…\n$ vote                <dbl> 44.60, 57.76, 49.91, 61.34, 49.60, 61.79, 48.95, 4…\n$ inc_party_candidate <chr> \"Stevenson\", \"Eisenhower\", \"Nixon\", \"Johnson\", \"Hu…\n$ other_candidate     <chr> \"Eisenhower\", \"Stevenson\", \"Kennedy\", \"Goldwater\",…\n```\n:::\n:::\n\nTry the view-function yourself.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# View(elections_data)\n```\n:::\n\n\nUse visualization to understand the data.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth))\n```\n\n::: {.cell-output-display}\n![Predicting elections from the economy 1952-2016](01-chapter_files/figure-html/plot-data1-1.png){width=672}\n:::\n:::\n\n\nAdd a line to the plot.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth)) +\n  geom_smooth(aes(x = year, y = growth), se = FALSE)\n```\n\n::: {.cell-output-display}\n![Predicting elections from the economy 1952-2016 with line](01-chapter_files/figure-html/plot-data2-1.png){width=672}\n:::\n:::\n\n\nAdd the CI around the line.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nggplot(data = elections_data) +\n  geom_point(aes(x = year, y = growth)) +\n  geom_smooth(aes(x = year, y = growth), se = TRUE)\n```\n\n::: {.cell-output-display}\n![Predicting elections from the economy 1952-2016 with line and confidence interval](01-chapter_files/figure-html/plot-data3-1.png){width=672}\n:::\n:::\n\n\nFit ols-regression to obtain the predicted values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(vote ~ growth, data = elections_data)\n```\n:::\n\n\nSummarize the regression results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = vote ~ growth, data = elections_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9929 -0.6674  0.2556  2.3225  5.3094 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  46.2476     1.6219  28.514 8.41e-14 ***\ngrowth        3.0605     0.6963   4.396  0.00061 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.763 on 14 degrees of freedom\nMultiple R-squared:  0.5798,\tAdjusted R-squared:  0.5498 \nF-statistic: 19.32 on 1 and 14 DF,  p-value: 0.00061\n```\n:::\n:::\n\n\nPlot the predicted values.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nplot(elections_data$growth, elections_data$vote, xlab = \"Economic Growth\", ylab = \"Vote Share\")\nabline(coef(mod1), col = \"red\")\n```\n\n::: {.cell-output-display}\n![Predicting elections from the economy 1952-2016 with line](01-chapter_files/figure-html/plot-predicted-1.png){width=672}\n:::\n:::\n\nPredicted values with ggplot.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nggplot(data = elections_data) +\n  geom_point(aes(x = growth, y = vote)) +\n  geom_abline(intercept = mod1[[1]][[1]], slope = mod1[[1]][[2]], color = \"red\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Data and linear fit\",\n       x = \"Average recent growth in personal income\",\n       y = \"Incumbent party's vote share\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](01-chapter_files/figure-html/plot-predicted2-1.png){width=672}\n:::\n:::\n\n\nPredicted values with ggplot and geom_smooth.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nggplot(data = elections_data) +\n  geom_point(aes(x = growth, y = vote)) +\n  geom_smooth(method = \"lm\", aes(x = growth, y = vote), color = \"blue\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Data and linear fit\",\n       x = \"Average recent growth in personal income\",\n       y = \"Incumbent party's vote share\")\n```\n\n::: {.cell-output-display}\n![](01-chapter_files/figure-html/plot-predicted3-1.png){width=672}\n:::\n:::\n\n\n2. Sketching regression \n\nOriginal $y = 46.3 + 3.0 x$. Explore the descriptive stats to get some parameters based on the observed data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nelections_data |> \n  summarise(min_growth = min(growth),\n            max_growth = max(growth),\n            mean_growth = mean(growth),\n            sd_growth = sd(growth),\n            min_vote = min(vote),\n            max_vote = max(vote),\n            mean_vote = mean(vote),\n            sd_vote = sd(vote))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  min_growth max_growth mean_growth sd_growth min_vote max_vote mean_vote\n1      -0.39       4.21      1.8975  1.395538     44.6    61.79    52.055\n   sd_vote\n1 5.608951\n```\n:::\n:::\n\n\nSimulating the data (technique often used in this book).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nN <- 16\nsimu_growth <- runif(N, -0.39, 4)\nsimu_vote <- rnorm(N, 46.2476  + 3.0605*simu_growth, 3.763)\nsimu_elections <- data.frame(N,simu_growth, simu_vote)\n```\n:::\n\n\nModel the simulated data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimu_mod <- lm(simu_vote ~ simu_growth, data = simu_elections)\n```\n:::\n\n\nSummarize the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(simu_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = simu_vote ~ simu_growth, data = simu_elections)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.355 -1.513 -0.488  1.839  5.962 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  43.6769     1.7558  24.876 5.49e-13 ***\nsimu_growth   4.0052     0.6948   5.765 4.90e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.448 on 14 degrees of freedom\nMultiple R-squared:  0.7036,\tAdjusted R-squared:  0.6824 \nF-statistic: 33.23 on 1 and 14 DF,  p-value: 4.896e-05\n```\n:::\n:::\n\n\nPlot the simulated data using base graphics.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\n# Base graphic\nplot(simu_elections$simu_growth, simu_elections$simu_vote, xlab = \"Simulated Economic Growth\", ylab = \"Simulated Vote Share\")\nabline(coef(simu_mod), col = \"blue\")\n```\n\n::: {.cell-output-display}\n![Simulated Data and linear fit](01-chapter_files/figure-html/plot-simulated-1.png){width=672}\n:::\n:::\n\n\nPlot the samen using ggplot version.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nggplot(data = simu_elections) +\n  geom_point(aes(x = simu_growth, y = simu_vote)) +\n  geom_smooth(method = \"lm\", aes(x = simu_growth, y = simu_vote), color = \"blue\", size = 1) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  scale_y_continuous(labels = scales::label_percent(accuracy = 1, scale = 1)) + \n  geom_hline(yintercept = 50) +\n  labs(title = \"Simulated Data and linear fit\",\n       x = \"Simulated Average recent growth in personal income\",\n       y = \"Simulated Incumbent party's vote share\")\n```\n\n::: {.cell-output-display}\n![](01-chapter_files/figure-html/plot-simulated2-1.png){width=672}\n:::\n:::\n\n\nExercise 1.2(a) from ROS for sketching a regression model and data. \n\na) $y = 30 + 10x$  (residual $sd 3.9$) & values of X ranging from $0-4$ \n\nDefine the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nN <- 50\nx <- runif(N, 0, 4)\ny <- rnorm(N, 30 + 10*x, 3.9)\ndata <- data.frame(N, x, y)\n```\n:::\n\n\nModel the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_a <- lm(y ~ x, data)\n```\n:::\n\n\nPlot the data.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nplot(data$x, data$y, xlab = \"X Value\", ylab = \"Y value\")\nabline(coef(lm_a), col = \"red\", size = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): \"size\" is\nnot a graphical parameter\n```\n:::\n\n::: {.cell-output-display}\n![Exercise 1.2 from ROS](01-chapter_files/figure-html/exercise-1.2-plot-1.png){width=672}\n:::\n:::\n\n\nb) $y = 30 + 10x$  (residual $sd 10$) & values of X ranging from $0-4$. \n\nDefine the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nN <- 50\nx <- runif(N, 0, 4)\ny <- rnorm(N, 30 + 10*x, 10)\ndata <- data.frame(N, x, y)\n```\n:::\n\n\nModel it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_b <- lm(y ~ x, data)\n```\n:::\n\n\nPlot it.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nplot(data$x, data$y, xlab = \"X Value\", ylab = \"Y value\")\nabline(coef(lm_b), col = \"blue\")\n```\n\n::: {.cell-output-display}\n![Continuous predictor](01-chapter_files/figure-html/exercise-1.2b-plot-1.png){width=672}\n:::\n:::\n\n\nNow simulate a binary predictor [example from the Aki Vehtari GH](https://avehtari.github.io/ROS-Examples/SimpleCausal/causal.html)\n\nSee Figure 1.5 (page 10).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1411)\nN <- 50\nx <- runif(N, 0, 4)\ny <- rnorm(N, 30 + 10*x, 10)\nx_binary <- ifelse(x < 3, 0, 1)\ndata_simu <- data.frame(N, x, y, x_binary)\n```\n:::\n\n\nModel it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_binary <- lm(y ~ x_binary, data = data_simu)\n```\n:::\n\n\nSummarize the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm_binary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x_binary, data = data_simu)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-27.2063  -8.5257   0.5297   9.3644  27.8011 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   45.812      2.296  19.953  < 2e-16 ***\nx_binary      19.033      3.827   4.974 8.81e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.99 on 48 degrees of freedom\nMultiple R-squared:  0.3401,\tAdjusted R-squared:  0.3264 \nF-statistic: 24.74 on 1 and 48 DF,  p-value: 8.813e-06\n```\n:::\n:::\n\n\nPlot the relationship.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nggplot(data = data_simu) +\n  geom_point(aes(x = x_binary, y = y)) +\n  geom_abline(intercept = lm_binary[[1]][[1]], slope = lm_binary[[1]][[2]],\n              color = \"blue\", size = 1) +\n  labs(y = \"Crime reduction\", \n       x =  NULL) +\n  scale_x_continuous(breaks = c(0,1),\n                     labels = c(\"Control\", \"Treatment\")) +\n  annotate(geom = \"text\", x = 0.50, y = 40,\n           label = paste(\"Estimated treatment effect is\\nslope of fitted line: \",\n                         round(lm_binary[[1]][[2]], digits = 2)))\n```\n\n::: {.cell-output-display}\n![Binary predictor](01-chapter_files/figure-html/binary-predictor-plot-1.png){width=672}\n:::\n:::\n\n\nNon-linear relationship \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1411)\nx <- runif(N, 1, 7)\ny <- rnorm(N, 7 + 30*exp(-x), 2)\ndata_simu$y <- y\n```\n:::\n\n\nFit the model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_nonlinear <- lm(y ~ x, data = data_simu)\n```\n:::\n\n\nSummarize the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm_nonlinear)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x, data = data_simu)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0484 -1.4874 -0.0243  1.7868  4.4113 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  13.1516     0.6188  21.253  < 2e-16 ***\nx            -1.8761     0.2476  -7.579  9.6e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.21 on 48 degrees of freedom\nMultiple R-squared:  0.5447,\tAdjusted R-squared:  0.5353 \nF-statistic: 57.43 on 1 and 48 DF,  p-value: 9.599e-10\n```\n:::\n:::\n\n\nPlot the model outcome.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nggplot(data = data_simu) +\n  geom_point(aes(x = x, y = y)) +\n  geom_smooth(method = \"loess\", aes(x = x, y = y), color = \"blue\", size = 1, se = FALSE) +\n  labs(y = \"Theft counts per hour\", \n       x =  \"Hours of foot patrol\")  \n```\n\n::: {.cell-output-display}\n![Non-linear relationship](01-chapter_files/figure-html/non-linear-plot-1.png){width=672}\n:::\n:::\n\n\n## More examples\nFirst look at dataset to predict US-elections (1952-2021) from the economy and explore data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"hibbs\")\nglimpse(hibbs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 16\nColumns: 5\n$ year                <int> 1952, 1956, 1960, 1964, 1968, 1972, 1976, 1980, 19…\n$ growth              <dbl> 2.40, 2.89, 0.85, 4.21, 3.02, 3.62, 1.08, -0.39, 3…\n$ vote                <dbl> 44.60, 57.76, 49.91, 61.34, 49.60, 61.79, 48.95, 4…\n$ inc_party_candidate <chr> \"Stevenson\", \"Eisenhower\", \"Nixon\", \"Johnson\", \"Hu…\n$ other_candidate     <chr> \"Eisenhower\", \"Stevenson\", \"Kennedy\", \"Goldwater\",…\n```\n:::\n:::\n\n\nReplicate the plot of Figure 1.1.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nggplot(data = hibbs,\n       mapping = aes(x = growth, y = vote)) +\n  # geom_label(mapping = aes(label = year), nudge_x = 0.3, fill = NA, size = 3) +\n  geom_point() \n```\n\n::: {.cell-output-display}\n![Predicting elections from the economy 1952-2016](01-chapter_files/figure-html/fig-hibbs-1.png){width=672}\n:::\n:::\n\n\nNow run the first regression model using `stanarm` or `brms`. This simulation works with four chains and 2000 iterations per chain. \n\n:::{.panel-tabset}\n\n## rstanarm\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM1 <- stan_glm(vote ~ growth, data=hibbs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.83 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.075 seconds (Warm-up)\nChain 1:                0.057 seconds (Sampling)\nChain 1:                0.132 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.5e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.06 seconds (Warm-up)\nChain 2:                0.046 seconds (Sampling)\nChain 2:                0.106 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.4e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.052 seconds (Warm-up)\nChain 3:                0.049 seconds (Sampling)\nChain 3:                0.101 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.074 seconds (Warm-up)\nChain 4:                0.054 seconds (Sampling)\nChain 4:                0.128 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\nM1 is set on your computer and you can give a summary of this regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nstan_glm\n family:       gaussian [identity]\n formula:      vote ~ growth\n observations: 16\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 46.3    1.6  \ngrowth       3.0    0.7  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 3.9    0.7   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n```\n:::\n:::\n\n\nOr print the intercept (46.26) and the slope (3.05) of this model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(M1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)      growth \n  46.300290    3.037234 \n```\n:::\n:::\n\n\n\n## brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM2 <- brm(vote ~ growth, data=hibbs)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCompiling Stan program...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nStart sampling\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.027 seconds (Warm-up)\nChain 1:                0.022 seconds (Sampling)\nChain 1:                0.049 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.027 seconds (Warm-up)\nChain 2:                0.021 seconds (Sampling)\nChain 2:                0.048 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 3:                0.023 seconds (Sampling)\nChain 3:                0.051 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 5e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 4:                0.023 seconds (Sampling)\nChain 4:                0.051 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\nM2 is set on your computer and you can give a summary of this regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM2 <-\n  brm(data = hibbs,\n      vote ~ growth,\n      cores = 4, chains = 4, iter = 2000,\n      seed = 123)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCompiling Stan program...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nStart sampling\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nM2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: vote ~ growth \n   Data: hibbs (Number of observations: 16) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    46.15      1.85    42.36    49.74 1.00     2993     2088\ngrowth        3.07      0.80     1.49     4.73 1.00     2757     1903\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.07      0.83     2.84     6.06 1.00     2683     1955\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\n\n\n:::\n\nNow add line to plot.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nggplot(data = hibbs,\n       mapping = aes(x = growth, y = vote)) +\n  geom_point() +\n  geom_abline(slope     = coef(M1)[[\"growth\"]],\n              intercept = coef(M1)[[\"(Intercept)\"]]) \n```\n\n::: {.cell-output-display}\n![Predicting elections from the economy 1952-2016](01-chapter_files/figure-html/fig-hibbs2-1.png){width=672}\n:::\n:::\n\n\n\nWe also looked at the peacekeeping data (1.3). First open the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeace_df <- read_csv(\"~/Desktop/WERK/Gelman/reading_club_GIT/Reading_club_Git/ROS-Examples-master/Peacekeeping/data/minidata.csv\")\n```\n:::\n\n\nExplore this dataset now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(peace_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 2,031\nColumns: 7\n$ ...1            <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ cfdate          <dbl> 140, 150, 210, 125, 126, 200, 110, 165, 190, 125, 200,…\n$ faildate        <dbl> 66, 64, 74, 66, 64, 65, 63, 68, 63, 64, 62, 73, 72, 72…\n$ `peacekeepers?` <dbl> 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ badness         <chr> \"white\", \"black\", \"white\", \"white\", \"white\", \"white\", …\n$ delay           <dbl> 1, 1, 3, 5, 1, 1, 6, 1, 1, 4, 2, 1, 1, 1, 2, 1, 4, 4, …\n$ `censored?`     <dbl> 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, …\n```\n:::\n:::\n\n\nCreate date measure. It's actually the same as delay.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeace_df <- peace_df |>\n  mutate(time_diff = (faildate-cfdate)/365)\n```\n:::\n\n\nLet us plot it ...\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\n# Harrie: not working\n# peace_df |>\n#  ggplot(data = .) +\n#  geom_histogram(mapping = aes(x = delay), bins = 10) +\n#  facet_wrap(~`peacekeepers?`) \n```\n:::\n\n\n... or put it in a scatterplot.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nggplot(data = peace_df) +\n  geom_point(mapping = aes(y = delay,\n                           colour = as.factor(`censored?`),\n                           x = badness,\n                           )) +\n  facet_wrap(~`peacekeepers?`) \n```\n\n::: {.cell-output-display}\n![Outcomes after civil war in countries with and without UN-peacekeepers](01-chapter_files/figure-html/fig-peace2-1.png){width=672}\n:::\n:::\n\n\nMeans.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeace_df |> \n  group_by(`peacekeepers?`, `censored?`) |> \n  summarise(mean_badness = mean(badness, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 3\n# Groups:   peacekeepers? [2]\n  `peacekeepers?` `censored?` mean_badness\n            <dbl>       <dbl>        <dbl>\n1               0           1           NA\n2               0           2           NA\n3               1           1           NA\n4               1           2           NA\n5               1          NA           NA\n```\n:::\n:::\n\n\nSimple causal graph for reproducibility of simulated data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSEED <- 1151\nset.seed(SEED)\nN <- 50\nx <- runif(N, 1, 5)\ny <- rnorm(N, 10 + 3*x, 3)\nx_binary <- ifelse(x<3, 0, 1)\ncausal_df <- data.frame(N, x, y, x_binary)\n```\n:::\n\n\nPlot this.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nggplot(data = causal_df) +\n  geom_point(mapping = aes(y = y, x = x)) \n```\n\n::: {.cell-output-display}\n![Causal graph of simulated data](01-chapter_files/figure-html/fig-causal-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "01-chapter_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}